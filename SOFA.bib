% This file was created with JabRef 2.6.
% Encoding: Cp1252

@ARTICLE{Ambadar05,
  author = {Zara Ambadar and J. Schooler and Jeffrey Cohn},
  title = {{D}eciphering the {E}nigmatic {F}ace: the {I}mportance of {F}acial
	{D}ynamics to {I}nterpreting {S}ubtle {F}acial {E}xpressions},
  journal = {Psychological Science},
  year = {2005}
}

@INPROCEEDINGS{Baker_ICCV07,
  author = {Baker, S. and Scharstein, D. and Lewis, J.P. and Roth, S. and Black,
	M.J. and Szeliski, R.},
  title = {{A} {D}atabase and {E}valuation {M}ethodology for {O}ptical {F}low},
  booktitle = {Proc. ICCV},
  year = {2007},
  issn = {1550-5499},
  keywords = {absolute flow endpoint error;average angular error;frame interpolation
	error;hidden fluorescent texture;high frame-rate video;image database;nonrigid
	motion;optical flow;optical tracking;statistics;stereo sequence;error
	analysis;image sequences;image texture;interpolation;optical tracking;statistical
	analysis;stereo image processing;visual databases;}
}

@INPROCEEDINGS{Baltrusaitis_FERA11,
  author = {Tadas Baltrusaitis and Daniel McDuff and Ntombikayise Banda and Marwa
	Mahmoud and Rana el Kaliouby and Peter Robinson and Rosalind Picard},
  title = {{R}eal-time {I}nference of {M}ental {S}tates from {F}acial {E}xpressions
	and {U}pper {B}ody {G}estures},
  booktitle = {Proc. FG Workshop on FERA Challenge},
  year = {2011},
  owner = {Songfan},
  timestamp = {2011.05.01}
}

@INPROCEEDINGS{Bartlett_FG06,
  author = {Bartlett, M.S. and Littlewort, G. and Frank, M. and Lainscsek, C.
	and Fasel, I. and Movellan, J.},
  title = {{F}ully {A}utomatic {F}acial {A}ction {R}ecognition in {S}pontaneous
	{B}ehavior},
  booktitle = {Proc. FG},
  year = {2006},
  keywords = {AdaBoost classifiers;facial action coding system;fully automatic facial
	action recognition;spontaneous behavior;support vector machines;emotion
	recognition;face recognition;support vector machines;}
}

@ARTICLE{Bassili79,
  author = {Bassili, John N.},
  title = {{E}motion {R}ecognition: {T}he {R}ole of {F}acial {M}ovement and
	the {R}elative {I}mportance of {U}pper and {L}ower {A}reas of the
	{F}ace},
  journal = {Personality and Social Psychology},
  year = {1979},
  owner = {songfan},
  timestamp = {2011.02.28}
}

@ARTICLE{Boker09,
  author = {Steven M. Boker and Jeffrey F. Cohn and Iain Matthews and Timothy
	R. Brick},
  title = {{E}ffects of {D}amping {H}ead {M}ovement and {F}acial {E}xpression
	in {D}yadic {C}onversation {U}sing {R}eal-time {F}acial {E}xpression
	{T}racking and {S}ynthesized {A}vatars},
  journal = {Philosophical Transactions B of the Royal Society},
  year = {2009}
}

@ARTICLE{opencv,
  author = {Bradski, G.},
  title = {{The OpenCV Library}},
  journal = {Dr. Dobb's Journal of Software Tools},
  year = {2000},
  citeulike-article-id = {2236121},
  keywords = {bibtex-import},
  posted-at = {2008-01-15 19:21:54},
  priority = {4}
}

@ARTICLE{Candes_JACM11,
  author = {E. Cand\`{e}s and X. Li and Y. Ma and J. Wright},
  title = {{R}obust {P}rincipal {C}omponent {A}nalysis?},
  journal = {Journal of the {ACM}},
  year = {2011}
}

@ARTICLE{Caspi_PAMI02,
  author = {Caspi, Y. and Irani, M.},
  title = {Spatio-temporal alignment of sequences},
  journal = {IEEE Trans. PAMI},
  year = {2002}
}

@MANUAL{SVMlib,
  title = {{LIBSVM}: {A} {L}ibrary for {S}upport {V}ector {M}achines},
  author = {Chih-Chung Chang and Chih-Jen Lin},
  year = {2001}
}

@INPROCEEDINGS{Chew_FERA11,
  author = {Sien Wei Chew and Patrick J. Lucey and Simon Lucey and Jason Saragih
	and Jeffrey Cohn and Sridha Sridharan},
  title = {{P}erson-independent {F}acial {E}xpression {D}etection {U}sing {C}onstrained
	{L}ocal {M}odels},
  booktitle = {Proc. FG Workshop on FERA Challenge},
  year = {2011},
  abstract = {{I}n automatic facial expression detection, very accurate registration
	is desired which can be achieved via a deformable model approach
	where a dense mesh of 60-70 points on the face is used, such as an
	active appearance model ({AAM}). {H}owever, for applications where
	manually labeling frames is prohibitive, {AAM}s do not work well
	as they do not generalize well to unseen subjects. {A}s such, a more
	coarse approach is taken for person-independent facial expression
	detection, where just a couple of key features (such as face and
	eyes) are tracked using a {V}iola-{J}ones type approach. {T}he tracked
	image is normally post-processed to encode for shift and illumination
	invariance using a linear bank of filters. {R}ecently, it was shown
	that this preprocessing step is of no benefit when close to ideal
	registration has been obtained. {I}n this paper, we present a system
	based on the {C}onstrained {L}ocal {M}odel ({CLM}) which is a generic
	or person-independent face alignment algorithm which gains high accuracy.
	{W}e show these results against the {LBP} feature extraction on the
	{CK}+ and {GEMEP} datasets.},
  keywords = {facial expression recognition, constrained local models, local binary
	patterns, fera2011 challenge}
}

@ARTICLE{Cootes_PAMI01,
  author = {Cootes, T.F. and Edwards, G.J. and Taylor, C.J.},
  title = {{A}ctive {A}ppearance {M}odels},
  journal = {IEEE Trans. PAMI},
  year = {2001},
  abstract = {{W}e describe a new method of matching statistical models of appearance
	to images. {A} set of model parameters control modes of shape and
	gray-level variation learned from a training set. {W}e construct
	an efficient iterative matching algorithm by learning the relationship
	between perturbations in the model parameters and the induced image
	errors},
  issn = {0162-8828},
  keywords = {active appearance models;deformable template;gray-level variation;iterative
	method;learning;model matching;optimisation;shape matching;statistical
	models;texture matching;image matching;image texture;iterative methods;learning
	(artificial intelligence);optimisation;statistical analysis;}
}

@INPROCEEDINGS{Cox_CVPR08,
  author = {Cox, M. and Sridharan, S. and Lucey, S. and Cohn, J.},
  title = {{L}east {S}quares {C}ongealing for {U}nsupervised {A}lignment of
	{I}mages},
  booktitle = {Proc. CVPR},
  year = {2008},
  issn = {1063-6919},
  keywords = {image alignment;least squares congealing;unsupervised learning;warp
	parameter update estimation;image processing;least squares approximations;unsupervised
	learning;}
}

@INPROCEEDINGS{Cruz_AVEC11,
  author = {Albert Cruz and Bir Bhanu and Songfan Yang},
  title = {{A} {P}sychologically-{I}nspired {M}atch-{S}core {F}usion {M}odel
	for {V}ideo-{B}ased {F}acial {E}xpression {R}ecognition},
  booktitle = {Proc. ACII workshop on AVEC},
  year = {2011},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  ee = {http://dx.doi.org/10.1007/978-3-642-24571-8_45}
}

@INPROCEEDINGS{Dahmane_FERA11,
  author = {Mohamed Dahmane and Jean Meunier},
  title = {{E}motion {R}ecognition using {D}ynamic {G}rid-based {H}o{G} {F}eatures},
  booktitle = {Proc. FG Workshop on FERA Challenge},
  year = {2011},
  owner = {Songfan},
  timestamp = {2011.05.01}
}

@INPROCEEDINGS{Dalal_CVPR05,
  author = {Dalal, N. and Triggs, B.},
  title = {{H}istograms of {O}riented {G}radients for {H}uman {D}etection},
  booktitle = {Proc. CVPR},
  year = {2005}
}

@INPROCEEDINGS{Dhall_FERA11,
  author = {Abhinav Dhall and Akshay Asthana and Roland Goecke and Tom Gedeon},
  title = {{E}motion {R}ecognition {U}sing \uppercase{PHOG} and \uppercase{LPQ}
	features},
  booktitle = {Proc. FG Workshop on FERA Challenge},
  year = {2011},
  owner = {Songfan},
  timestamp = {2011.05.01}
}

@ARTICLE{Donato_PAMI99,
  author = {Donato, G. and Bartlett, M.S. and Hager, J.C. and Ekman, P. and Sejnowski,
	T.J.},
  title = {{C}lassifying {F}acial {A}ctions},
  journal = {IEEE Trans. PAMI},
  year = {1999},
  abstract = {{T}he facial action coding system ({FAGS}) is an objective method
	for quantifying facial movement in terms of component actions. {T}his
	paper explores and compares techniques for automatically recognizing
	facial actions in sequences of images. {T}hese techniques include:
	analysis of facial motion through estimation of optical flow; holistic
	spatial analysis, such as principal component analysis, independent
	component analysis, local feature analysis, and linear discriminant
	analysis; and methods based on the outputs of local filters, such
	as {G}abor wavelet representations and local principal components.
	{P}erformance of these systems is compared to naive and expert human
	subjects. {B}est performances were obtained using the {G}abor wavelet
	representation and the independent component representation, both
	of which achieved 96 percent accuracy for classifying 12 facial actions
	of the upper and lower face. {T}he results provide converging evidence
	for the importance of using local filters, high spatial frequencies,
	and statistical independence for classifying facial actions},
  issn = {0162-8828},
  keywords = {Gabor wavelet;computer vision;facial action coding system;facial expression
	recognition;image sequences;independent component analysis;linear
	discriminant analysis;local feature analysis;motion estimation;optical
	flow;principal component analysis;computer vision;face recognition;image
	sequences;motion estimation;principal component analysis;wavelet
	transforms;}
}

@ARTICLE{Beltfast,
  author = {Ellen Douglas-Cowie and Nick Campbell and Roddy Cowie and Peter Roach},
  title = {{E}motional {S}peech: {T}owards {A} {N}ew {G}eneration of {D}atabases},
  journal = {Speech Communication},
  year = {2003}
}

@INPROCEEDINGS{Douglas-Cowie00,
  author = {Ellen Douglas-Cowie and Roddy Cowie and Marc Schröder},
  title = {{A} {N}ew {E}motion {D}atabase: {C}onsiderations, {S}ources and {S}cope},
  booktitle = {Proc. the ISCA Workshop on Speech and Emotion},
  year = {2000}
}

@BOOK{Ekman78,
  title = {{F}acial {A}ction {C}oding {S}ystem: {A} {T}echnique for the {M}easurement
	of {F}acial {M}ovement},
  publisher = {Consulting Psychologists Press},
  year = {1978},
  author = {Ekman, P. and Friesen, W.},
  journal = {Consulting Psychologists Press},
  owner = {Songfan},
  timestamp = {2012.02.20}
}

@BOOK{Ekman2005,
  title = {What the Face Reveals: Basic and Applied Studies of Spontaneous Expression
	Using the Facial Action Coding System (FACS)},
  publisher = {Oxford University Press},
  year = {2005},
  author = {Paul Ekman and Erika Rosenberg},
  owner = {Songfan},
  timestamp = {2014.04.04}
}

@INPROCEEDINGS{Kaliouby_SMC04,
  author = {El Kaliouby, R. and Robinson, P.},
  title = {{M}ind {R}eading {M}achines: {A}utomated {I}nference of {C}ognitive
	{M}ental {S}tates from {V}ideo},
  booktitle = {Proc. SMC},
  year = {2004},
  abstract = {{M}ind reading encompasses our ability to attribute mental states
	to others, and is essential for operating in a complex social environment.
	{T}he goal in building mind reading machines is to enable computer
	technologies to understand and react to people's emotions and mental
	states. {T}his paper describes a system, for the automated inference
	of cognitive mental states from observed facial expressions and head
	gestures in video. {T}he system is based on a multilevel dynamic
	{B}ayesian network classifier which models cognitive mental states
	as a number of interacting facial and head displays. {E}xperimental
	results yield an average recognition rate of 87.4% for 6 mental states
	groups: agreement, concentrating, disagreement, interested, thinking
	and unsure. {R}eal time performance, unobtrusiveness and lack of
	preprocessing make our system particularly suitable for user-independent
	human computer interaction},
  issn = {1062-922X},
  keywords = {automated inference;cognitive mental states;complex social environment;head
	gestures;mind reading machines;multilevel dynamic Bayesian network
	classifier;observed facial expressions;user-independent human computer
	interaction;Bayes methods;belief networks;cognition;emotion recognition;human
	computer interaction;inference mechanisms;}
}

@ARTICLE{Essa_PAMI97,
  author = {Essa, I.A. and Pentland, A.P.},
  title = {{C}oding, {A}nalysis, {I}nterpretation, and {R}ecognition of {F}acial
	{E}xpressions},
  journal = {IEEE Trans. PAMI},
  year = {1997},
  abstract = {{W}e describe a computer vision system for observing facial motion
	by using an optimal estimation optical flow method coupled with geometric,
	physical and motion-based dynamic models describing the facial structure.
	{O}ur method produces a reliable parametric representation of the
	face's independent muscle action groups, as well as an accurate estimate
	of facial motion. {P}revious efforts at analysis of facial expression
	have been based on the facial action coding system ({FACS}), a representation
	developed in order to allow human psychologists to code expression
	from static pictures. {T}o avoid use of this heuristic coding scheme,
	we have used our computer vision system to probabilistically characterize
	facial motion and muscle activation in an experimental population,
	thus deriving a new, more accurate, representation of human facial
	expressions that we call {FACS}+. {F}inally, we show how this method
	can be used for coding, analysis, interpretation, and recognition
	of facial expressions},
  issn = {0162-8828},
  keywords = {FACS;FACS+;computer vision system;facial action coding system;facial
	expression recognition;facial motion;facial structure;image analysis;image
	coding;image interpretation;independent muscle action groups;muscle
	activation;optimal estimation optical flow method;probabilistic characterization;face
	recognition;image coding;image sequences;motion estimation;}
}

@ARTICLE{LIBLINEAR,
  author = {Rong-En Fan and Kai-Wei Chang and Cho-Jui Hsieh and Xiang-Rui Wang
	and Chih-Jen Lin},
  title = {{LIBLINEAR}: {A} {L}ibrary for {L}arge {L}inear {C}lassification},
  journal = {JMLR},
  year = {2008}
}

@ARTICLE{Freeman_IJCV00,
  author = {Freeman, William T. and Pasztor, Egon C. and Carmichael, Owen T.},
  title = {{L}earning {L}ow-{L}evel {V}ision},
  journal = {IJCV},
  year = {2000},
  issue = {1}
}

@INPROCEEDINGS{Gehrig_CVPRW11,
  author = {Gehrig, T. and Ekenel, H.K.},
  title = {{A} {C}ommon {F}ramework for {R}eal-time {E}motion {R}ecognition
	and {F}acial {A}ction {U}nit {D}etection},
  booktitle = {Proc. CVPR Workshops},
  year = {2011},
  issn = {2160-7508},
  keywords = {FG 2011 facial expression analysis;FG 2011 facial expression recognition;discrete
	cosine transform;facial action unit detection;local appearance-based
	face representation approach;real-time emotion recognition;support
	vector machine classifiers;discrete cosine transforms;emotion recognition;face
	recognition;image classification;support vector machines;}
}

@ARTICLE{Han_PAMI06,
  author = {Han, J. and Bhanu, B.},
  title = {{I}ndividual {R}ecognition {U}sing {G}ait {E}nergy {I}mage},
  journal = {IEEE Trans. PAMI},
  year = {2006},
  abstract = {{I}n this paper, we propose a new spatio-temporal gait representation,
	called {G}ait {E}nergy {I}mage ({GEI}), to characterize human walking
	properties for individual recognition by gait. {T}o address the problem
	of the lack of training templates, we also propose a novel approach
	for human recognition by combining statistical gait features from
	real and synthetic templates. {W}e directly compute the real templates
	from training silhouette sequences, while we generate the synthetic
	templates from training sequences by simulating silhouette distortion.
	{W}e use a statistical approach for learning effective features from
	real and synthetic templates. {W}e compare the proposed {GEI}-based
	gait recognition approach with other gait recognition approaches
	on {USF} {H}uman{ID} {D}atabase. {E}xperimental results show that
	the proposed {GEI} is an effective and efficient gait representation
	for individual recognition, and the proposed approach achieves highly
	competitive performance with respect to the published gait recognition
	approaches},
  doi = {10.1109/TPAMI.2006.38},
  issn = {0162-8828},
  keywords = {distortion analysis;feature fusion;gait energy image;human recognition;human
	walking properties;individual recognition;spatio-temporal gait representation;statistical
	gait features;gait analysis;image recognition;}
}

@INPROCEEDINGS{Hu_IVC06,
  author = {Changbo Hu and Ya Chang and Feris, R. and Turk, M.},
  title = {{M}anifold {B}ased {A}nalysis of {F}acial {E}xpression},
  booktitle = {Image and Vision Computing},
  year = {2006}
}

@INPROCEEDINGS{Huang_ICCV07,
  author = {Huang, G.B. and Jain, V. and Learned-Miller, E.},
  title = {{U}nsupervised {J}oint {A}lignment of {C}omplex {I}mages},
  booktitle = {Proc. ICCV},
  year = {2007},
  issn = {1550-5499},
  keywords = {canonical pose recognition;class-specialized learning algorithm;face
	detector;face recognition;image recognition algorithm;unsupervised
	joint alignment;face recognition;pose estimation;unsupervised learning;}
}

@BOOK{Huber81,
  title = {{R}obust {S}tatistics},
  publisher = {John Wiley \& Sons, Inc.},
  year = {1981},
  author = {Huber, P. J.},
  address = {Hoboken, NJ},
  owner = {Songfan},
  timestamp = {2012.02.22}
}

@ARTICLE{Irani91,
  author = {Irani, Michal and Peleg, Shmuel},
  title = {{I}mproving {R}esolution by {I}mage {R}egistration},
  journal = {Graph. Models Image Process.},
  year = {1991},
  issue = {3}
}

@ARTICLE{Josephine2003,
  author = {Josephine L.C.M. Woltman Elpers, Michel Wedel, Rik G.M. Pieters},
  title = {Why Do Consumers Stop Viewing Television Commercials? Two Experiments
	on the Influence of Moment-to-Moment Entertainment and Information
	Value},
  journal = {Journal of Marketing Research},
  year = {2003},
  owner = {Songfan},
  timestamp = {2014.04.14}
}

@CONFERENCE{close_loop_icra_05,
  author = {Michael Kaess and Frank Dellaert},
  title = {A Markov Chain Monte Carlo Approach to Closing the Loop in {SLAM}},
  booktitle = {Proc. ICRA},
  year = {2005},
  owner = {Songfan},
  timestamp = {2014.09.05}
}

@INPROCEEDINGS{Kanade_FG00,
  author = {Kanade, T. and Cohn, J.F. and Yingli Tian},
  title = {{C}omprehensive {D}atabase for {F}acial {E}xpression {A}nalysis},
  booktitle = {Proc. FG},
  year = {2000},
  abstract = {{W}ithin the past decade, significant effort has occurred in developing
	methods of facial expression analysis. {B}ecause most investigators
	have used relatively limited data sets, the generalizability of these
	various methods remains unknown. {W}e describe the problem space
	for facial expression analysis, which includes level of description,
	transitions among expressions, eliciting conditions, reliability
	and validity of training and test data, individual differences in
	subjects, head orientation and scene complexity image characteristics,
	and relation to non-verbal behavior. {W}e then present the {CMU}-{P}ittsburgh
	{AU}-{C}oded {F}ace {E}xpression {I}mage {D}atabase, which currently
	includes 2105 digitized image sequences from 182 adult subjects of
	varying ethnicity, performing multiple tokens of most primary {FACS}
	action units. {T}his database is the most comprehensive testbed to
	date for comparative studies of facial expression analysis},
  keywords = {CMU-Pittsburgh AU-Coded Face Expression Image Database;FACS action
	units;description level;digitized image sequences;eliciting conditions;expression
	transitions;facial expression analysis;head orientation;image characteristics;non-verbal
	behavior;reliability;scene complexity;subject differences;validity;face
	recognition;image sequences;reliability;visual databases;}
}

@INPROCEEDINGS{Keren_CVPR88,
  author = {Keren, D. and Peleg, S. and Brada, R.},
  title = {{I}mage {S}equence {E}nhancement {U}sing {S}ub-pixel {D}isplacements},
  booktitle = {Proc. CVPR},
  year = {1988}
}

@ARTICLE{Kotsia_IP07,
  author = {Kotsia, I. and Pitas, I.},
  title = {{F}acial {E}xpression {R}ecognition in {I}mage {S}equences {U}sing
	{G}eometric {D}eformation {F}eatures and {S}upport {V}ector {M}achines},
  journal = {IEEE Trans. IP},
  year = {2007},
  issn = {1057-7149},
  keywords = {Candide grid nodes;SVM;facial action units;facial expression;facial
	expression recognition;geometric deformation features;grid-tracking;image
	sequences;support vector machines;video frames;face recognition;image
	sequences;support vector machines;Algorithms;Artificial Intelligence;Face;Facial
	Expression;Humans;Image Enhancement;Image Interpretation, Computer-Assisted;Information
	Storage and Retrieval;Pattern Recognition, Automated;Subtraction
	Technique;Video Recording;}
}

@ARTICLE{LearnedMiller_PAMI06,
  author = {Learned-Miller, E.G.},
  title = {{D}ata {D}riven {I}mage {M}odels {T}hrough {C}ontinuous {J}oint {A}lignment},
  journal = {IEEE Trans. PAMI},
  year = {2006},
  issn = {0162-8828},
  keywords = {bias removal;congealing;continuous joint alignment;data driven image
	models;handwritten character recognition;handwritten digit classifier;magnetic
	resonance images;nuisance variables;computer vision;handwritten character
	recognition;image resolution;magnetic resonance imaging;nonparametric
	statistics;Algorithms;Artificial Intelligence;Automatic Data Processing;Computer
	Simulation;Documentation;Handwriting;Image Enhancement;Image Interpretation,
	Computer-Assisted;Information Storage and Retrieval;Models, Theoretical;Pattern
	Recognition, Automated;Subtraction Technique;}
}

@INPROCEEDINGS{Littlewort_IVC06,
  author = {Littlewort, G. and Bartlett, M.S. and Fasel, I. and Susskind, J.
	and Movellan, J.},
  title = {{D}ynamics of {F}acial {E}xpression {E}xtracted {A}utomatically from
	{V}ideo},
  booktitle = {Image and Vision Computing},
  year = {2006},
  abstract = { {W}e present a systematic comparison of machine learning methods
	applied to the problem of fully automatic recognition of facial expressions,
	including {A}da{B}oost, support vector machines, and linear discriminant
	analysis. {E}ach video-frame is first scanned in real-time to detect
	approximately upright-frontal faces. {T}he faces found are scaled
	into image patches of equal size, convolved with a bank of {G}abor
	energy filters, and then passed to a recognition engine that codes
	facial expressions into 7 dimensions in real time: neutral, anger,
	disgust, fear, joy, sadness, surprise. {W}e report results on a series
	of experiments comparing spatial frequency ranges, feature selection
	techniques, and recognition engines. {B}est results were obtained
	by selecting a subset of {G}abor filters using {A}da{B}oost and then
	training {S}upport {V}ector {M}achines on the outputs of the filters
	selected by {A}da{B}oost. {T}he generalization performance to new
	subjects for a 7-way forced choice was 93% or more correct on two
	publicly available datasets, the best performance reported so far
	on these datasets. {S}urprisingly, registration of internal facial
	features was not necessary, even though the face detector does not
	provide precisely registered images. {T}he outputs of the classifier
	change smoothly as a function of time and thus can be used for unobtrusive
	motion capture. {W}e developed an end-to-end system that provides
	facial expression codes at 24 frames per second and animates a computer
	generated character. {I}n real-time this expression mirror operates
	down to resolutions of 16 pixels from eye to eye. {W}e also applied
	the system to fully automated facial action coding.},
  doi = {10.1109/CVPR.2004.53}
}

@INPROCEEDINGS{Bartlett_FG11,
  author = {Littlewort, G. and Whitehill, J and Wu, T. and Fasel, I. and Frank,M.
	and Movellan, J. and Bartlett, M.},
  title = {{C}omputer {E}xpression {R}ecognition {T}oolbox},
  booktitle = {Proc. FG},
  year = {2011},
  abstract = {{W}e present a live demo of the {C}omputer {E}xpression {R}ecognition
	{T}oolbox ({CERT}) developed at {U}niversity of {C}alifornia, {S}an
	{D}iego. {CERT} measures facial expressions in real-time, and codes
	them with respect to expressions of basic emotion, as well as over
	20 facial actions from the {F}acial {A}ction {C}oding {S}ystem ({E}kman
	amp; {F}riesen, 1978). {H}ead pose (yaw, pitch, and roll) is also
	detected using an algorithm presented at this conference ({W}hitehill
	amp; {M}ovellan, 2008). {A} sample output is shown in {F}igure 1.},
  doi = {10.1109/AFGR.2008.4813406},
  keywords = {computer expression recognition toolbox;emotion recognition;facial
	action coding system;facial expression;pose estimation;emotion recognition;face
	recognition;image coding;pose estimation;}
}

@INPROCEEDINGS{Littlewort_CERT_FG2011,
  author = {Littlewort, G. and Whitehill, J. and Tingfan Wu and Fasel, I. and
	Frank, M. and Movellan, J. and Bartlett, M.},
  title = {{T}he {C}omputer {E}xpression {R}ecognition {T}oolbox ({CERT})},
  booktitle = {Proc. FG},
  year = {2011},
  keywords = {3D orientation;CERT;FACS;automatic real-time facial expression recognition;computer
	expression recognition toolbox;dual core laptop;extended Cohn-Kanade;facial
	action unit coding system;facial expression dataset;software tool;two-alternative
	forced choice task;emotion recognition;face recognition;image coding;software
	tools;}
}

@INPROCEEDINGS{Littlewort_FERA11,
  author = {Gwen Littlewort and Jacob Whitehill and Ting-Fan Wu and Nicholas
	Butko and Paul Ruvolo and Javier Movellan and Marian Bartlett},
  title = {{T}he {M}otion in {E}motion – {A} {CERT} {B}ased {A}pproach to the
	{FERA} {E}motion {C}hallenge},
  booktitle = {Proc. FG Workshop on FERA Challenge},
  year = {2011},
  owner = {Songfan},
  timestamp = {2011.04.29}
}

@ARTICLE{Liu_PAMI11,
  author = {Liu, C. and Yuen, J. and Torralba, A.},
  title = {\uppercase{SIFT} \uppercase{F}low: {D}ense {C}orrespondence across
	{S}cenes and its {A}pplications},
  journal = {IEEE Trans. PAMI},
  year = {2011},
  doi = {http://doi.ieeecomputersociety.org/10.1109/TPAMI.2010.147},
  issn = {0162-8828}
}

@ARTICLE{SIFT,
  author = {Lowe, David G.},
  title = {Distinctive Image Features from Scale-Invariant Keypoints},
  journal = {IJCV}
}

@INPROCEEDINGS{Lucey_FG06,
  author = {Lucey, S. and Matthews, I. and Changbo Hu and Ambadar, Z. and de
	la Torre, F. and Cohn, J.},
  title = {\uppercase{AAM} {D}erived {F}ace {R}epresentations for {R}obust {F}acial
	{A}ction {R}ecognition},
  booktitle = {Proc. FG},
  year = {2006},
  doi = {10.1109/FGR.2006.17},
  keywords = {active appearance model;face representations;normalization methods;robust
	facial action recognition;face recognition;image representation;}
}

@ARTICLE{Martinez_PAMI13,
  author = {Brais Martinez and Michel Valstar and Xavier Binefa and Maja Pantic},
  title = {Local Evidence Aggregation for Regression Based Facial Point Detection},
  journal = {IEEE Trans. PAMI},
  year = {2013},
  owner = {Songfan},
  timestamp = {2014.04.04}
}

@ARTICLE{Matthews_IJCV03,
  author = {Iain Matthews and Simon Baker},
  title = {{A}ctive {A}ppearance {M}odels {R}evisited},
  journal = {IJCV},
  year = {2003}
}

@INPROCEEDINGS{Meng_FERA11,
  author = {Hongying Meng and Bernardino Romera-Paredes and Nadia Bianchi-Berthouze},
  title = {{E}motion {R}ecognition by {T}wo {V}iew \uppercase{SVM} 2\uppercase{K}
	{C}lassifier on {D}ynamic {F}acial {E}xpression {F}eatures},
  booktitle = {Proc. FG Workshop on FERA Challenge},
  year = {2011},
  owner = {Songfan},
  timestamp = {2011.05.01}
}

@INCOLLECTION{Meng05,
  author = {Meng, Hongying and Shawe-Taylor, John and Szedmak, Sandor and Farquhar,
	Jason},
  title = {{S}upport {V}ector {M}achine to {S}ynthesise {K}ernels},
  booktitle = {Deterministic and Statistical Methods in Machine Learning},
  publisher = {Springer Berlin / Heidelberg},
  year = {2005},
  editor = {Winkler, Joab and Niranjan, Mahesan and Lawrence, Neil},
  affiliation = {School of Electronics and Computer Science, University of Southampton,
	Southampton, SO17 1BJ UK}
}

@INPROCEEDINGS{Negri06,
  author = {Negri, P. and Clady, X. and Milgram, M. and Poulenard, R.},
  title = {An Oriented-Contour Point Based Voting Algorithm for Vehicle Type
	Classification},
  booktitle = {Proc. ICPR},
  year = {2006}
}

@ARTICLE{Ojala_PAMI02,
  author = {Ojala, T. and Pietik\"ainen, M. and M\"aenp\"a\"a, T.},
  title = {{M}ultiresolution {G}ray-scale and {R}otation {I}nvariant {T}exture
	{C}lassification with {L}ocal {B}inary {P}atterns},
  journal = {IEEE Trans. PAMI},
  year = {2002},
  abstract = {{P}resents a theoretically very simple, yet efficient, multiresolution
	approach to gray-scale and rotation invariant texture classification
	based on local binary patterns and nonparametric discrimination of
	sample and prototype distributions. {T}he method is based on recognizing
	that certain local binary patterns, termed "uniform," are fundamental
	properties of local image texture and their occurrence histogram
	is proven to be a very powerful texture feature. {W}e derive a generalized
	gray-scale and rotation invariant operator presentation that allows
	for detecting the "uniform" patterns for any quantization of the
	angular space and for any spatial resolution and presents a method
	for combining multiple operators for multiresolution analysis. {T}he
	proposed approach is very robust in terms of gray-scale variations
	since the operator is, by definition, invariant against any monotonic
	transformation of the gray scale. {A}nother advantage is computational
	simplicity as the operator can be realized with a few operations
	in a small neighborhood and a lookup table. {E}xperimental results
	demonstrate that good discrimination can be achieved with the occurrence
	statistics of simple rotation invariant local binary patterns },
  doi = {10.1109/TPAMI.2002.1017623},
  issn = {0162-8828},
  keywords = {angular space;computational simplicity;gray-scale variations;local
	binary patterns;local image texture;multiresolution analysis;multiresolution
	gray-scale texture classification;nonparametric discrimination;occurrence
	histogram;prototype distributions;rotation invariant texture classification;sample
	distributions;spatial resolution;uniform patterns;image classification;image
	texture;invariance;nonparametric statistics;}
}

@INPROCEEDINGS{Ojansivu_ICISP08,
  author = {Ojansivu, V. and Heikkil\"a, J.},
  title = {{B}lur {I}nsensitive {T}exture {C}lassification {U}sing {L}ocal {P}hase
	{Q}uantization},
  booktitle = {Proc. ICISP},
  year = {2008},
  acmid = {1426636},
  doi = {http://dx.doi.org/10.1007/978-3-540-69905-7_27},
  isbn = {978-3-540-69904-0},
  location = {Cherbourg-Octeville, France},
  numpages = {8}
}

@ARTICLE{Pantic_SMCB06,
  author = {Pantic, M. and Patras, I.},
  title = {{D}ynamics of {F}acial {E}xpression: {R}ecognition of {F}acial {A}ctions
	and {T}heir {T}emporal {S}egments from {F}ace {P}rofile {I}mage {S}equences},
  journal = {IEEE Trans. SMC-B},
  year = {2006},
  abstract = {{A}utomatic analysis of human facial expression is a challenging problem
	with many applications. {M}ost of the existing automated systems
	for facial expression analysis attempt to recognize a few prototypic
	emotional expressions, such as anger and happiness. {I}nstead of
	representing another approach to machine analysis of prototypic facial
	expressions of emotion, the method presented in this paper attempts
	to handle a large range of human facial behavior by recognizing facial
	muscle actions that produce expressions. {V}irtually all of the existing
	vision systems for facial muscle action detection deal only with
	frontal-view face images and cannot handle temporal dynamics of facial
	actions. {I}n this paper, we present a system for automatic recognition
	of facial action units ({AU}s) and their temporal models from long,
	profile-view face image sequences. {W}e exploit particle filtering
	to track 15 facial points in an input face-profile sequence, and
	we introduce facial-action-dynamics recognition from continuous video
	input using temporal rules. {T}he algorithm performs both automatic
	segmentation of an input video into facial expressions pictured and
	recognition of temporal segments (i.e., onset, apex, offset) of 27
	{AU}s occurring alone or in a combination in the input face-profile
	video. {A} recognition rate of 87% is achieved.},
  issn = {1083-4419},
  keywords = {automatic facial action unit recognition;automatic human facial expression
	analysis;automatic video segmentation;computer vision system;emotional
	expression recognition;face profile image sequences;facial muscle
	action recognition;facial-action-dynamics recognition;human facial
	behavior;particle filtering;temporal model;temporal segment recognition;computer
	vision;emotion recognition;face recognition;image segmentation;image
	sequences;particle filtering (numerical methods);Algorithms;Artificial
	Intelligence;Cluster Analysis;Face;Facial Expression;Humans;Image
	Enhancement;Image Interpretation, Computer-Assisted;Information Storage
	and Retrieval;Movement;Pattern Recognition, Automated;Photography;Reproducibility
	of Results;Sensitivity and Specificity;Subtraction Technique;Time
	Factors;Video Recording;}
}

@ARTICLE{Pantic_SMCB04,
  author = {Pantic, M. and Rothkrantz, L.J.M.},
  title = {{F}acial {A}ction {R}ecognition for {F}acial {E}xpression {A}nalysis
	from {S}tatic {F}ace {I}mages},
  journal = {IEEE Trans. SMC-B},
  year = {2004},
  abstract = {{A}utomatic recognition of facial gestures (i.e., facial muscle activity)
	is rapidly becoming an area of intense interest in the research field
	of machine vision. {I}n this paper, we present an automated system
	that we developed to recognize facial gestures in static, frontal-
	and/or profile-view color face images. {A} multidetector approach
	to facial feature localization is utilized to spatially sample the
	profile contour and the contours of the facial components such as
	the eyes and the mouth. {F}rom the extracted contours of the facial
	features, we extract ten profile-contour fiducial points and 19 fiducial
	points of the contours of the facial components. {B}ased on these,
	32 individual facial muscle actions ({AU}s) occurring alone or in
	combination are recognized using rule-based reasoning. {W}ith each
	scored {AU}, the utilized algorithm associates a factor denoting
	the certainty with which the pertinent {AU} has been scored. {A}
	recognition rate of 86% is achieved.},
  issn = {1083-4419},
  keywords = {facial action recognition;facial component contour sampling;facial
	expression analysis;facial feature localization;facial gesture recognition;facial
	muscle action units;image processing;profile contour sampling;profile-contour
	fiducial points;rule-based reasoning;spatial reasoning;static face
	images;face recognition;feature extraction;gesture recognition;image
	colour analysis;knowledge based systems;spatial reasoning;uncertainty
	handling;Algorithms;Artificial Intelligence;Face;Facial Expression;Humans;Image
	Interpretation, Computer-Assisted;Pattern Recognition, Automated;Photography;Posture;Reproducibility
	of Results;Sensitivity and Specificity;}
}

@ARTICLE{Pantic_PAMI00,
  author = {Pantic, M. and Rothkrantz, L.J.M.},
  title = {{A}utomatic {A}nalysis of {F}acial {E}xpressions: {T}he {S}tate of
	the {A}rt},
  journal = {IEEE Trans. PAMI},
  year = {2000},
  doi = {10.1109/34.895976},
  issn = {0162-8828},
  keywords = {automatic analysis;automatic facial expression analyzer;emotion categories;facial
	expressions;human visual system;human-like interaction;image segment
	detection;face recognition;feature extraction;image classification;image
	sequences;neural nets;}
}

@INPROCEEDINGS{Pantic_ICME05,
  author = {Pantic, M. and Valstar, M. and Rademaker, R. and Maat, L.},
  title = {{W}eb-based {D}atabase for {F}acial {E}xpression {A}nalysis},
  booktitle = {IEEE Int. Conf. on Multimedia and Expo.},
  year = {2005},
  abstract = { {I}n the last decade, the research topic of automatic analysis of
	facial expressions has become a central topic in machine vision research.
	{N}onetheless, there is a glaring lack of a comprehensive, readily
	accessible reference set of face images that could be used as a basis
	for benchmarks for efforts in the field. {T}his lack of easily accessible,
	suitable, common testing resource forms the major impediment to comparing
	and extending the issues concerned with automatic facial expression
	analysis. {I}n this paper, we discuss a number of issues that make
	the problem of creating a benchmark facial expression database difficult.
	{W}e then present the {MMI} facial expression database, which includes
	more than 1500 samples of both static images and image sequences
	of faces in frontal and in profile view displaying various expressions
	of emotion, single and multiple facial muscle activation. {I}t has
	been built as a {W}eb-based direct-manipulation application, allowing
	easy access and easy search of the available images. {T}his database
	represents the most comprehensive reference set of images for studies
	on facial expression analysis to date.},
  doi = {10.1109/ICME.2005.1521424},
  keywords = { MMI facial expression database; Web-based direct-manipulation application;
	image representation; image sequence; man-machine interaction; multiple
	facial muscle activation; static image; Internet; distributed databases;
	emotion recognition; face recognition; image representation; image
	sequences; visual databases;}
}

@INPROCEEDINGS{Pearce11,
  author = {Pearce, G. and Pears, N.},
  title = {Automatic make and model recognition from frontal images of cars},
  booktitle = {IEEE AVSS},
  year = {2011},
  pages = {373-378},
  month = {Aug},
  doi = {10.1109/AVSS.2011.6027353},
  keywords = {automobiles;image classification;video surveillance;Harris corner
	strengths;Naive Bayes classifier;automatic make and model recognition;cars;classification
	approaches;feature detection;frontal images;hierarchical fashion;k
	nearest neighbour classifier;Detectors;Feature extraction;Image edge
	detection;Licenses;Testing;Training;Vehicles}
}

@ARTICLE{sklearn,
  author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and
	Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and
	Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau,
	D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  title = {Scikit-learn: Machine Learning in {P}ython},
  journal = {Journal of Machine Learning Research},
  year = {2011}
}

@INPROCEEDINGS{Peng_CVPR10,
  author = {Yigang Peng and Ganesh, A. and Wright, J. and Wenli Xu and Yi Ma},
  title = {{RASL}: {R}obust {A}lignment by {S}parse and {L}ow-rank {D}ecomposition
	for {L}inearly {C}orrelated {I}mages},
  booktitle = {Proc. CVPR},
  year = {2010},
  issn = {1063-6919},
  keywords = {RASL;convex programs;gross corruption;image domain transformations;linearly
	correlated images;low-rank decomposition;low-rank matrix;occlusion;optimization
	problem;realistic misalignments;recovered aligned images;robust alignment
	algorithm;scalable convex optimization techniques;sparse decomposition;sparse
	matrix;transformed image matrix;computer graphics;convex programming;correlation
	methods;image registration;sparse matrices;}
}

@INPROCEEDINGS{Petrovic04,
  author = {V.S. Petrovic and T. F. Cootes},
  title = {Analysis of Features for Rigid Structure Vehicle Type Recognition},
  booktitle = {Proc. BMVC},
  year = {2004}
}

@ARTICLE{Pham_06,
  author = {Pham, Tuan Q. and van Vliet, Lucas J. and Schutte, Klamer},
  title = {{R}obust {F}usion of {I}rregularly {S}ampled {D}ata {U}sing {A}daptive
	{N}ormalized {C}onvolution},
  journal = {EURASIP JASP},
  year = {2006}
}

@BOOK{Russell97,
  title = {{T}he {P}sychology of {F}acial {E}xpression},
  publisher = {Cambridge University Press},
  year = {1997},
  author = {James A. Russell and Jos\'e Miguel Fern\'andez-Dols},
  owner = {songfan},
  timestamp = {2011.04.28}
}

@INPROCEEDINGS{Saragih_ICCV09,
  author = {Saragih, J.M. and Lucey, S. and Cohn, J.F.},
  title = {{F}ace {A}lignment through {S}ubspace {C}onstrained {M}ean-{S}hifts},
  booktitle = {Proc. ICCV},
  year = {2009},
  doi = {10.1109/ICCV.2009.5459377},
  issn = {1550-5499},
  keywords = {computer vision community;deformable model fitting;distribution replacement;face
	alignment;local detector;model landmark distribution;principled optimization
	strategy;smoothed estimate hierarchy;subspace constrained mean shifts;face
	recognition;optimisation;}
}

@ARTICLE{Scharstein_IJCV02,
  author = {Daniel Scharstein and Richard Szeliski},
  title = {{A} {T}axonomy and {E}valuation of {D}ense {T}wo-frame {S}tereo {C}orrespondence
	{A}lgorithms},
  journal = {IJCV},
  year = {2002}
}

@INPROCEEDINGS{Senechal_FERA11,
  author = {Ruchir Srivastava and Sujoy Roy and Shuicheng Yan and Terence Sim},
  title = {{A}ccumulated {M}otion {I}mages for {F}acial {E}xpression {R}ecognition
	in {V}ideos},
  booktitle = {Proc. FG Workshop on FERA Challenge},
  year = {2011},
  owner = {Songfan},
  timestamp = {2011.05.01}
}

@INPROCEEDINGS{Sun_CVPR08,
  author = {Jian Sun and Zongben Xu and Heung-Yeung Shum},
  title = {{I}mage {S}uper-resolution {U}sing {G}radient {P}rofile {P}rior},
  booktitle = {Proc. CVPR},
  year = {2008}
}

@ARTICLE{Szeliski06,
  author = {Richard Szeliski},
  title = {Image Alignment and Stitching: A Tutorial},
  journal = {Found. Trends. Comput. Graph. Vis.},
  year = {2006}
}

@INPROCEEDINGS{Tariq_FERA11,
  author = {Usman Tariq and Kai-Hsiang Lin and Zhen Li and Xi Zhou and Zhaowen
	Wang and Vuong Le and Thomas S. Huang and Xutao Lv and Tony X. Han},
  title = {{E}motion {R}ecognition from an {E}nsemble of {F}eatures},
  booktitle = {Proc. FG Workshop on FERA Challenge},
  year = {2011},
  owner = {Songfan},
  timestamp = {2011.04.29}
}

@ARTICLE{Thakoor13,
  author = {Thakoor, N.S. and Bhanu, B.},
  title = {Structural Signatures for Passenger Vehicle Classification in Video},
  journal = {IEEE Trans. Intelligent Transportation Systems},
  year = {2013}
}

@INPROCEEDINGS{Thakoor05,
  author = {Thakoor, N. and Jean Gao},
  title = {Automatic video object shape extraction and its classification with
	camera in motion},
  booktitle = {Proc. ICIP},
  year = {2005}
}

@ARTICLE{Tian_PAMI01,
  author = {Tian, Y.-I. and Kanade, T. and Cohn, J.F.},
  title = {{R}ecognizing {A}ction {U}nits for {F}acial {E}xpression {A}nalysis},
  journal = {IEEE Trans. PAMI},
  year = {2001},
  abstract = {{M}ost automatic expression analysis systems attempt to recognize
	a small set of prototypic expressions, such as happiness, anger,
	surprise, and fear. {S}uch prototypic expressions, however, occur
	rather infrequently. {H}uman emotions and intentions are more often
	communicated by changes in one or a few discrete facial features.
	{I}n this paper, we develop an automatic face analysis ({AFA}) system
	to analyze facial expressions based on both permanent facial features
	(brows, eyes, mouth) and transient facial features (deepening of
	facial furrows) in a nearly frontal-view face image sequence. {T}he
	{AFA} system recognizes fine-grained changes in facial expression
	into action units ({AU}) of the {F}acial {A}ction {C}oding {S}ystem
	({FACS}), instead of a few prototypic expressions. {M}ultistate face
	and facial component models are proposed for tracking and modeling
	the various facial features, including lips, eyes, brows, cheeks,
	and furrows. {D}uring tracking, detailed parametric descriptions
	of the facial features are extracted. {W}ith these parameters as
	the inputs, a group of action units (neutral expression, six upper
	face {AU} and 10 lower face {AU}) are recognized whether they occur
	alone or in combinations. {T}he system has achieved average recognition
	rates of 96.4 percent (95.4 percent if neutral expressions are excluded)
	for upper face {AU} and 96.7 percent (95.6 percent with neutral expressions
	excluded) for lower face {AU}. {T}he generalizability of the system
	has been tested by using independent image databases collected and
	{FACS}-coded for ground-truth by different research teams},
  issn = {0162-8828},
  keywords = {AU;FACS;Facial Action Coding System;action unit recognition;anger;automatic
	facial expression analysis;cheeks;discrete facial features;eyebrows;eyes;facial
	furrows;fear;frontal-view face image sequence;happiness;image databases;lips;mouth;multistate
	face models;neutral expression;parametric descriptions;surprise;tracking;face
	recognition;feature extraction;image sequences;tracking;}
}

@ARTICLE{Tong_PAMI10,
  author = {Yan Tong and Jixu Chen and Qiang Ji},
  title = {{A} {U}nified {P}robabilistic {F}ramework for {S}pontaneous {F}acial
	{A}ction {M}odeling and {U}nderstanding},
  journal = {IEEE Trans. PAMI},
  year = {2010},
  abstract = {{F}acial expression is a natural and powerful means of human communication.
	{R}ecognizing spontaneous facial actions, however, is very challenging
	due to subtle facial deformation, frequent head movements, and ambiguous
	and uncertain facial motion measurements. {B}ecause of these challenges,
	current research in facial expression recognition is limited to posed
	expressions and often in frontal view. {A} spontaneous facial expression
	is characterized by rigid head movements and nonrigid facial muscular
	movements. {M}ore importantly, it is the coherent and consistent
	spatiotemporal interactions among rigid and nonrigid facial motions
	that produce a meaningful facial expression. {R}ecognizing this fact,
	we introduce a unified probabilistic facial action model based on
	the dynamic {B}ayesian network ({DBN}) to simultaneously and coherently
	represent rigid and nonrigid facial motions, their spatiotemporal
	dependencies, and their image measurements. {A}dvanced machine learning
	methods are introduced to learn the model based on both training
	data and subjective prior knowledge. {G}iven the model and the measurements
	of facial motions, facial action recognition is accomplished through
	probabilistic inference by systematically integrating visual measurements
	with the facial action model. {E}xperiments show that compared to
	the state-of-the-art techniques, the proposed system yields significant
	improvements in recognizing both rigid and nonrigid facial motions,
	especially for spontaneous facial expressions.},
  issn = {0162-8828},
  keywords = {dynamic Bayesian network;facial action recognition;facial deformation;facial
	expression;facial motion measurement;facial muscular movement;head
	movement;human communication;image measurement;machine learning;probabilistic
	facial action model;probabilistic inference;rigid facial motion;spatiotemporal
	interaction;spontaneous facial action modeling;spontaneous facial
	action understanding;visual measurement;belief networks;face recognition;image
	motion analysis;inference mechanisms;learning (artificial intelligence);Algorithms;Artificial
	Intelligence;Bayes Theorem;Biometric Identification;Databases, Factual;Face;Humans;Models,
	Statistical;}
}

@INPROCEEDINGS{Uenohara95,
  author = {Michihiro Uenohara and Takeo Kanade},
  title = {Real-Time Vision Based Object Registration for Image Overlay},
  booktitle = {1995 Conference on Computer Vision,Virtual Reality and Robotics in
	Medicine},
  year = {1995},
  month = {April}
}

@INPROCEEDINGS{Valstar_FERA11,
  author = {Valstar, M.F. and Jiang, B. and M\'{e}hu, M. and Pantic, M. and Scherer,
	K},
  title = {{T}he {F}irst {F}acial {E}xpression {R}ecognition and {A}nalysis
	{C}hallenge},
  booktitle = {Proc. FG Workshop on FERA Challenge},
  year = {2011},
  owner = {songfan},
  timestamp = {2011.03.01}
}

@ARTICLE{Valstar12,
  author = {Valstar, M.F. and Mehu, M. and Bihan Jiang and Pantic, M. and Scherer,
	K.},
  title = {Meta-Analysis of the First Facial Expression Recognition Challenge},
  journal = {IEEE Trans. SMC-B},
  year = {2012}
}

@ARTICLE{Valstar_SMCB12,
  author = {Valstar, M.F. and Pantic, M.},
  title = {{F}ully {A}utomatic {R}ecognition of the {T}emporal {P}hases of {F}acial
	{A}ctions},
  journal = {IEEE Trans. SMC-B},
  year = {2012},
  issn = {1083-4419},
  keywords = {Gabor-feature-based boosted classifiers;GentleBoost;action units;decision
	making;facial behavior;facial fiducial point localization;facial
	muscle action temporal phase automatic recognition;facial point detector;factorized
	likelihoods;hidden Markov models;image sequence;particle filtering;support
	vector machines;temporal activation models;face recognition;hidden
	Markov models;image classification;image sequences;particle filtering
	(numerical methods);support vector machines;}
}

@INPROCEEDINGS{Valstar_HCI07,
  author = {M. F. Valstar and M. Pantic},
  title = {{C}ombined {S}upport {V}ector {M}achines and {H}idden {M}arkov {M}odels
	for {M}odeling {F}acial {A}ction {T}emporal {D}ynamics},
  booktitle = {Proc. CVPR workshop on Human Computer Interaction},
  year = {2007}
}

@ARTICLE{Vandewalle06,
  author = {Patrick Vandewalle and Sabine S\"usstrunk and Martin Vetterli},
  title = {{A} {F}requency {D}omain {A}pproach to {R}egistration of {A}liased
	{I}mages with {A}pplication to {S}uper-{R}esolution},
  journal = {EURASIP JASP},
  year = {2006}
}

@MISC{vlfeat,
  author = {A. Vedaldi and B. Fulkerson},
  title = {{VLFeat}: An Open and Portable Library of Computer Vision Algorithms},
  howpublished = {\url{http://www.vlfeat.org/}},
  year = {2008},
  owner = {Songfan},
  timestamp = {2014.05.17}
}

@ARTICLE{Viola_IJCV04,
  author = {Paul Viola and Michael Jones},
  title = {{R}obust {R}eal-time {F}ace {D}etection},
  journal = {IJCV},
  year = {2004}
}

@INPROCEEDINGS{Wagner2009,
  author = {Wagner, A. and Wright, J. and Ganesh, A. and Zihan Zhou and Yi Ma},
  title = {Towards a practical face recognition system: Robust registration
	and illumination by sparse representation},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
  year = {2009},
  pages = {597-604},
  month = {June},
  issn = {1063-6919}
}

@ARTICLE{Wang_TIP94,
  author = {Wang, J.Y.A. and Adelson, E.H.},
  title = {{R}epresenting {M}oving {I}mages with {L}ayers},
  journal = {IEEE Trans. IP},
  year = {1994},
  doi = {10.1109/83.334981},
  issn = {1057-7149},
  keywords = {alpha map;image coding;image representation;image sequences decomposition;intensity
	map;layered representation;motion analysis;motion estimation;motion
	segmentation;moving images;overlapping layers;transparency;velocity
	maps;video sequences;image coding;image segmentation;image sequences;motion
	estimation;video signals;}
}

@INPROCEEDINGS{Wang_FG98,
  author = {Mei Wang and Iwai, Y. and Yachida, M.},
  title = {{E}xpression {R}ecognition from {T}ime-sequential {F}acial {I}mages
	by {U}se of {E}xpression {C}hange {M}odel},
  booktitle = {Proc. FG},
  year = {1998},
  abstract = {{I}n order to construct a better human interface, recognition of facial
	expressions by means of computer is an important technology. {A}n
	approach is proposed to recognize the degree of facial expression
	change from time-sequential images. {T}he facial features in an input
	image sequence are tracked by using labeled graph matching with weighted
	links. {T}o represent the relationship between the motion of features
	and change of expression, we construct expression change models by
	using {B}-spline curves. {B}y making a comparison between the trajectory
	of features and the expression change models, the facial expression
	in the input image sequence can be recognized. {N}ot only the category,
	but also the degree of facial expression change can be determined.
	{F}urthermore, the obtained expressional information is then fed
	back to guide the tracking in the next frame},
  keywords = {B-spline curves;expression change model;expression recognition;expressional
	information;facial expression change;facial features;feature trajectory;human
	interface;input image sequence;labeled graph matching;time-sequential
	facial images;tracking;weighted links;face recognition;image sequences;splines
	(mathematics);user interfaces;}
}

@INPROCEEDINGS{Wu_CVPRW10,
  author = {Tingfan Wu and Bartlett, M.S. and Movellan, J.R.},
  title = {{F}acial {E}xpression {R}ecognition {U}sing {G}abor {M}otion {E}nergy
	{F}ilters},
  booktitle = {Proc. CVPRW},
  year = {2010},
  doi = {10.1109/CVPRW.2010.5543267},
  keywords = {Cohn-Kanade expression dataset;GME;Gabor motion energy filters;biologically
	inspired representation;computer vision;dynamic facial expressions;face
	expression analysis;face recognition;facial expression recognition;primary
	visual cortex;spatial Gabor energy filters;spatio-temporal Gabor
	filters;video sequences;visual signal;Gabor filters;computer vision;face
	recognition;image motion analysis;image representation;image sequences;spatial
	filters;video signal processing;}
}

@INPROCEEDINGS{Wu_FERA11,
  author = {Tingfan Wu and Butko, N.J. and Ruvolo, P. and Whitehill, J. and Bartlett,
	M.S. and Movellan, J.R.},
  title = {{A}ction {U}nit {R}ecognition {T}ransfer across {D}atasets},
  booktitle = {Proc. FG Workshop on FERA Challenge},
  year = {2011},
  doi = {10.1109/FG.2011.5771369},
  keywords = {CERT;FERA dataset;action unit recognition;baseline method;computer
	expression recognition toolbox;idiosyncratic;spontaneous facial expression;transfer
	across dataset;emotion recognition;face recognition;}
}

@ARTICLE{Wu_TIP06,
  author = {Xiaolin Wu and Zhang, D.},
  title = {Improvement of Color Video Demosaicking in Temporal Domain},
  journal = {IEEE Trans. Image Process.},
  year = {2006},
  volume = {15},
  pages = {3138-3151},
  number = {10},
  month = {Oct},
  issn = {1057-7149}
}

@INPROCEEDINGS{Xiong13,
  author = {Xuehan Xiong and de la Torre, F.},
  title = {Supervised Descent Method and Its Applications to Face Alignment},
  booktitle = {Proc. CVPR},
  year = {2013}
}

@INPROCEEDINGS{Xue_CVPR13,
  author = {Wufeng Xue and Lei Zhang and Xuanqin Mou},
  title = {Learning without Human Scores for Blind Image Quality Assessment},
  booktitle = {Proc. CVPR},
  year = {2013}
}

@ARTICLE{Yacoob_PAMI96,
  author = {Yacoob, Y. and Davis, L.S.},
  title = {{R}ecognizing {H}uman {F}acial {E}xpressions from {L}ong {I}mage
	{S}equences {U}sing {O}ptical {F}low},
  journal = {IEEE Trans. PAMI},
  year = {1996},
  doi = {10.1109/34.506414},
  issn = {0162-8828},
  keywords = {facial dynamics;human facial expression recognition;image sequences;motion
	analysis;optical flow;symbolic representation;face recognition;image
	recognition;image representation;image sequences;motion estimation;}
}

@INPROCEEDINGS{Yang_CVPR07,
  author = {Peng Yang and Qingshan Liu and Metaxas, D.N.},
  title = {{B}oosting {C}oded {D}ynamic {F}eatures for {F}acial {A}ction {U}nits
	and {F}acial {E}xpression {R}ecognition},
  booktitle = {Proc. CVPR},
  year = {2007},
  abstract = {{I}t is well known that how to extract dynamical features is a key
	issue for video based face analysis. {I}n this paper, we present
	a novel approach of facial action units ({AU}) and expression recognition
	based on coded dynamical features. {I}n order to capture the dynamical
	characteristics of facial events, we design the dynamical haar-like
	features to represent the temporal variations of facial events. {I}nspired
	by the binary pattern coding, we further encode the dynamic haar-like
	features into binary pattern features, which are useful to construct
	weak classifiers for boosting learning. {F}inally the {A}daboost
	is performed to learn a set of discriminating coded dynamic features
	for facial active units and expression recognition. {E}xperiments
	on the {CMU} expression database and our own facial {AU} database
	show its encouraging performance.},
  doi = {10.1109/CVPR.2007.383059},
  keywords = {binary pattern coding;coded dynamic features;facial action units;facial
	expression recognition;video based face analysis;binary codes;emotion
	recognition;face recognition;video signal processing;}
}

@CONFERENCE{Yang_FG13,
  author = {Songfan Yang and Le An and Bir Bhanu and Ninad Thakoor},
  title = {Improving action units recognition using dense flow-based face registration
	in video},
  booktitle = {Proc. FG},
  year = {2013},
  owner = {Songfan},
  timestamp = {2014.04.04}
}

@ARTICLE{Yang_SMCB12,
  author = {Yang, S. and Bhanu, B.},
  title = {{U}nderstanding {D}iscrete {F}acial {E}xpressions in {V}ideo {U}sing
	an {E}motion {A}vatar {I}mage},
  journal = {IEEE Trans. SMC-B},
  year = {2012},
  keywords = {Gabor-feature-based boosted classifiers;GentleBoost;action units;decision
	making;facial behavior;facial fiducial point localization;facial
	muscle action temporal phase automatic recognition;facial point detector;factorized
	likelihoods;hidden Markov models;image sequence;particle filtering;support
	vector machines;temporal activation models;face recognition;hidden
	Markov models;image classification;image sequences;particle filtering
	(numerical methods);support vector machines;}
}

@INPROCEEDINGS{Yang_FERA11,
  author = {Songfan Yang and Bir Bhanu},
  title = {{F}acial {E}xpression {R}ecognition {U}sing {E}motion {A}vatar {I}mage},
  booktitle = {Proc. FG Workshop on FERA Challenge},
  year = {2011},
  owner = {songfan},
  timestamp = {2011.05.23}
}

@ARTICLE{Yu_PRL06,
  author = {Jiangang Yu and Bir Bhanu},
  title = {{E}volutionary {F}eature {S}ynthesis for {F}acial {E}xpression {R}ecognition},
  journal = {Pattern Recognition Letters},
  year = {2006},
  keywords = {genetic algorithms, genetic programming, Feature learning, Gabor filters}
}

@INPROCEEDINGS{Zafar09,
  author = {Iffat Zafar and Eran A. Edirisinghe and B. Serpil Acar},
  title = {Localised contourlet features in vehicle make and model recognition},
  booktitle = {Image Processing: Machine Vision Applications II, Proc. of SPIE-IS\&T
	Electronic Imaging},
  year = {2009}
}

@INPROCEEDINGS{Zhang_ACCV10,
  author = {Z. Zhang and X. Liang and A. Ganesh and Y. Ma},
  title = {{TILT}: {T}ransform {I}nvariant {L}ow-rank {T}extures},
  booktitle = {Proc. ACCV},
  year = {2010}
}

@ARTICLE{Zhao_PAMI07,
  author = {Guoying Zhao and Pietik\"ainen, M.},
  title = {{D}ynamic {T}exture {R}ecognition {U}sing {L}ocal {B}inary {P}atterns
	with an {A}pplication to {F}acial {E}xpressions},
  journal = {IEEE Trans. PAMI},
  year = {2007},
  doi = {10.1109/TPAMI.2007.1110},
  issn = {0162-8828},
  keywords = {dynamic texture recognition;facial expressions;facial image analysis;volume
	local binary patterns;face recognition;image texture;Algorithms;Artificial
	Intelligence;Biometry;Computer Security;Face;Facial Expression;Humans;Image
	Enhancement;Image Interpretation, Computer-Assisted;Pattern Recognition,
	Automated;Reproducibility of Results;Sensitivity and Specificity;Signal
	Processing, Computer-Assisted;Video Recording;}
}

@INPROCEEDINGS{Zhou_CVPR10,
  author = {Feng Zhou and De la Torre, F. and Cohn, J.F.},
  title = {{U}nsupervised {D}iscovery of {F}acial {E}vents},
  booktitle = {Proc. CVPR},
  year = {2010},
  abstract = {{A}utomatic facial image analysis has been a long standing research
	problem in computer vision. {A} key component in facial image analysis,
	largely conditioning the success of subsequent algorithms (e.g. facial
	expression recognition), is to define a vocabulary of possible dynamic
	facial events. {T}o date, that vocabulary has come from the anatomically-based
	{F}acial {A}ction {C}oding {S}ystem ({FACS}) or more subjective approaches
	(i.e. emotion-specified expressions). {T}he aim of this paper is
	to discover facial events directly from video of naturally occurring
	facial behavior, without recourse to {FACS} or other labeling schemes.
	{T}o discover facial events, we propose a temporal clustering algorithm,
	{A}ligned {C}luster {A}nalysis ({ACA}), and a multi-subject correspondence
	algorithm for matching expressions. {W}e use a variety of video sources:
	posed facial behavior ({C}ohn-{K}anade database), unscripted facial
	behavior ({RU}-{FACS} database) and some video in infants. {A}ccuracy
	of (unsupervised) {ACA} approached that of a supervised version,
	achieved moderate intersystem agreement with {FACS}, and proved informative
	as a visualization/summarization tool.},
  doi = {10.1109/CVPR.2010.5539966},
  issn = {1063-6919},
  keywords = {aligned cluster analysis;automatic facial image analysis;computer
	vision;facial action coding system;facial events;facial expression
	recognition;labeling schemes;multi-subject correspondence algorithm;unsupervised
	discovery;computer vision;emotion recognition;face recognition;image
	matching;video coding;}
}

@INPROCEEDINGS{Zhu_ICCV09,
  author = {Jianke Zhu and Luc Van Gool and Hoi, S.C.H.},
  title = {{U}nsupervised {F}ace {A}lignment by {R}obust {N}onrigid {M}apping},
  booktitle = {Proc. ICCV},
  year = {2009},
  issn = {1550-5499},
  keywords = {Lucas-Kanade image registration;affine transformations;robust nonrigid
	mapping;robust optimization scheme;unsupervised facial image alignment;face
	recognition;feature extraction;image registration;transforms;}
}

@ARTICLE{Zhu_PAMI09,
  author = {Zhu, J. and Lyu, M.R. and Huang, T.S.},
  title = {{A} {F}ast 2{D} {S}hape {R}ecovery {A}pproach by {F}using {F}eatures
	and {A}ppearance},
  journal = {IEEE Trans. PAMI},
  year = {2009},
  issn = {0162-8828},
  keywords = {2D shape recovery;Lucas-Kanade algorithm;appearance fusion;computer
	vision;feature fusion;feature-based nonrigid surface detection;finite
	Newton optimization scheme;least squares problem;unconstrained quadratic
	optimization problem;computer graphics;image fusion;least squares
	approximations;object detection;quadratic programming;Algorithms;Artificial
	Intelligence;Image Enhancement;Image Interpretation, Computer-Assisted;Information
	Storage and Retrieval;Models, Biological;Pattern Recognition, Automated;Reproducibility
	of Results;Sensitivity and Specificity;Subtraction Technique;}
}

@INPROCEEDINGS{Zhu_CVPR12,
  author = {Xiangxin Zhu and Ramanan, D.},
  title = {Face detection, pose estimation, and landmark localization in the
	wild},
  booktitle = {Proc. CVPR},
  year = {2012}
}

@ARTICLE{Zitova_IVC03,
  author = {Barbara Zitov\'a and Jan Flusser},
  title = {{I}mage {R}egistration {M}ethods: {A} {S}urvey},
  journal = {Image and Vision Computing},
  year = {2003}
}

@INPROCEEDINGS{Zomet_CVPR01,
  author = {Zomet, A. and Rav-Acha, A. and Peleg, S.},
  title = {{R}obust {S}uper-resolution},
  booktitle = {Proc. CVPR},
  year = {2001}
}

@MANUAL{FERA11,
  title = {{FERA}2011: {F}acial {E}xpression {R}ecognition and {A}nalysis {C}hallenge},
  note = {http://sspnet.eu/fera2011/},
  owner = {songfan},
  timestamp = {2011.02.28}
}


% This file was created with JabRef 2.10.
% Encoding: Cp1252


@Article{Ambadar05,
  Title                    = {{D}eciphering the {E}nigmatic {F}ace: the {I}mportance of {F}acial {D}ynamics to {I}nterpreting {S}ubtle {F}acial {E}xpressions},
  Author                   = {Zara Ambadar and J. Schooler and Jeffrey Cohn},
  Journal                  = {Psychological Science},
  Year                     = {2005}
}

@InProceedings{Baker_ICCV07,
  Title                    = {A Database and Evaluation Methodology for Optical Flow},
  Author                   = {Baker, S. and Roth, S. and Scharstein, D. and Black, M.J. and Lewis, J.P. and Szeliski, R.},
  Booktitle                = {IEEE International Conference on Computer Vision},
  Year                     = {2007},
  Month                    = {Oct},
  Pages                    = {1-8},

  Doi                      = {10.1109/ICCV.2007.4408903},
  ISSN                     = {1550-5499},
  Keywords                 = {error analysis;image sequences;image texture;interpolation;optical tracking;statistical analysis;stereo image processing;visual databases;absolute flow endpoint error;average angular error;frame interpolation error;hidden fluorescent texture;high frame-rate video;image database;nonrigid motion;optical flow;optical tracking;statistics;stereo sequence;Benchmark testing;Databases;Fluid flow measurement;Fluorescence;Image motion analysis;Interpolation;Layout;Optical noise;Optical sensors;Tracking}
}

@InProceedings{Tadas_FERA15,
  Title                    = {Cross-dataset learning and person-specific normalisation for automatic Action Unit detection},
  Author                   = {Baltrusaitis, T. and Mahmoud, M. and Robinson, P.},
  Booktitle                = {IEEE International Conference and Workshops on Automatic Face and Gesture Recognition},
  Year                     = {2015}
}

@InProceedings{Tadas_FERA11,
  Title                    = {{R}eal-time {I}nference of {M}ental {S}tates from {F}acial {E}xpressions and {U}pper {B}ody {G}estures},
  Author                   = {Tadas Baltrusaitis and Daniel McDuff and Ntombikayise Banda and Marwa Mahmoud and Rana el Kaliouby and Peter Robinson and Rosalind Picard},
  Booktitle                = {Proc. FG Workshop on FERA Challenge},
  Year                     = {2011},

  Owner                    = {Songfan},
  Timestamp                = {2011.05.01}
}

@InCollection{Tadas_ECCV14,
  Title                    = {Continuous Conditional Neural Fields for Structured Regression},
  Author                   = {Baltrusaitis, Tadas and Robinson, Peter and Morency, Louis-Philippe},
  Booktitle                = {European Conference on Computer Vision},
  Year                     = {2014}
}

@InProceedings{Bartlett_FG06,
  Title                    = {{F}ully {A}utomatic {F}acial {A}ction {R}ecognition in {S}pontaneous {B}ehavior},
  Author                   = {Bartlett, M.S. and Littlewort, G. and Frank, M. and Lainscsek, C. and Fasel, I. and Movellan, J.},
  Booktitle                = {Proc. FG},
  Year                     = {2006},

  Keywords                 = {AdaBoost classifiers;facial action coding system;fully automatic facial action recognition;spontaneous behavior;support vector machines;emotion recognition;face recognition;support vector machines;}
}

@Article{Bassili79,
  Title                    = {{E}motion {R}ecognition: {T}he {R}ole of {F}acial {M}ovement and the {R}elative {I}mportance of {U}pper and {L}ower {A}reas of the {F}ace},
  Author                   = {Bassili, John N.},
  Journal                  = {Personality and Social Psychology},
  Year                     = {1979},

  Owner                    = {songfan},
  Timestamp                = {2011.02.28}
}

@Article{Boker09,
  Title                    = {{E}ffects of {D}amping {H}ead {M}ovement and {F}acial {E}xpression in {D}yadic {C}onversation {U}sing {R}eal-time {F}acial {E}xpression {T}racking and {S}ynthesized {A}vatars},
  Author                   = {Steven M. Boker and Jeffrey F. Cohn and Iain Matthews and Timothy R. Brick},
  Journal                  = {Philosophical Transactions B of the Royal Society},
  Year                     = {2009}
}

@Article{opencv,
  Title                    = {{The OpenCV Library}},
  Author                   = {Bradski, G.},
  Journal                  = {Dr. Dobb's Journal of Software Tools},
  Year                     = {2000},

  Citeulike-article-id     = {2236121},
  Keywords                 = {bibtex-import},
  Posted-at                = {2008-01-15 19:21:54},
  Priority                 = {4}
}

@Article{Candes_JACM11,
  Title                    = {{R}obust {P}rincipal {C}omponent {A}nalysis?},
  Author                   = {E. Cand\`{e}s and X. Li and Y. Ma and J. Wright},
  Journal                  = {Journal of the {ACM}},
  Year                     = {2011}
}

@Article{Caspi_PAMI02,
  Title                    = {Spatio-temporal alignment of sequences},
  Author                   = {Caspi, Y. and Irani, M.},
  Journal                  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  Year                     = {2002},

  Month                    = {Nov},
  Number                   = {11},
  Pages                    = {1409-1424},
  Volume                   = {24},

  Doi                      = {10.1109/TPAMI.2002.1046148},
  ISSN                     = {0162-8828},
  Keywords                 = {image matching;image motion analysis;image sequences;video cameras;dynamic scene;image alignment;image frames;image motion analysis;intercamera external parameters;scene illumination;sequence-to-sequence alignment;spatial cues;spatio-temporal sequence alignment;temporal cues;video cameras;video sequences;Cameras;Helium;Image analysis;Image resolution;Infrared sensors;Layout;Lighting;Pixel;Spatial resolution;Video sequences}
}

@Article{SVMlib,
  Title                    = {{LIBSVM}: A library for support vector machines},
  Author                   = {Chang, Chih-Chung and Lin, Chih-Jen},
  Journal                  = {ACM Transactions on Intelligent Systems and Technology},
  Year                     = {2011},
  Pages                    = {1-27},
  Volume                   = {2},

  Issue                    = {3}
}

@InProceedings{Chew_FERA11,
  Title                    = {{P}erson-independent {F}acial {E}xpression {D}etection {U}sing {C}onstrained {L}ocal {M}odels},
  Author                   = {Sien Wei Chew and Patrick J. Lucey and Simon Lucey and Jason Saragih and Jeffrey Cohn and Sridha Sridharan},
  Booktitle                = {Proc. FG Workshop on FERA Challenge},
  Year                     = {2011},

  Abstract                 = {{I}n automatic facial expression detection, very accurate registration is desired which can be achieved via a deformable model approach where a dense mesh of 60-70 points on the face is used, such as an active appearance model ({AAM}). {H}owever, for applications where manually labeling frames is prohibitive, {AAM}s do not work well as they do not generalize well to unseen subjects. {A}s such, a more coarse approach is taken for person-independent facial expression detection, where just a couple of key features (such as face and eyes) are tracked using a {V}iola-{J}ones type approach. {T}he tracked image is normally post-processed to encode for shift and illumination invariance using a linear bank of filters. {R}ecently, it was shown that this preprocessing step is of no benefit when close to ideal registration has been obtained. {I}n this paper, we present a system based on the {C}onstrained {L}ocal {M}odel ({CLM}) which is a generic or person-independent face alignment algorithm which gains high accuracy. {W}e show these results against the {LBP} feature extraction on the {CK}+ and {GEMEP} datasets.},
  Keywords                 = {facial expression recognition, constrained local models, local binary patterns, fera2011 challenge}
}

@Article{Cootes_PAMI01,
  Title                    = {{A}ctive {A}ppearance {M}odels},
  Author                   = {Cootes, T.F. and Edwards, G.J. and Taylor, C.J.},
  Journal                  = {IEEE Trans. PAMI},
  Year                     = {2001},

  Abstract                 = {{W}e describe a new method of matching statistical models of appearance to images. {A} set of model parameters control modes of shape and gray-level variation learned from a training set. {W}e construct an efficient iterative matching algorithm by learning the relationship between perturbations in the model parameters and the induced image errors},
  ISSN                     = {0162-8828},
  Keywords                 = {active appearance models;deformable template;gray-level variation;iterative method;learning;model matching;optimisation;shape matching;statistical models;texture matching;image matching;image texture;iterative methods;learning (artificial intelligence);optimisation;statistical analysis;}
}

@InProceedings{Cox_CVPR08,
  Title                    = {{L}east {S}quares {C}ongealing for {U}nsupervised {A}lignment of {I}mages},
  Author                   = {Cox, M. and Sridharan, S. and Lucey, S. and Cohn, J.},
  Booktitle                = {Proc. CVPR},
  Year                     = {2008},

  ISSN                     = {1063-6919},
  Keywords                 = {image alignment;least squares congealing;unsupervised learning;warp parameter update estimation;image processing;least squares approximations;unsupervised learning;}
}

@InProceedings{Cruz_AVEC11,
  Title                    = {{A} {P}sychologically-{I}nspired {M}atch-{S}core {F}usion {M}odel for {V}ideo-{B}ased {F}acial {E}xpression {R}ecognition},
  Author                   = {Albert Cruz and Bir Bhanu and Songfan Yang},
  Booktitle                = {Proc. ACII workshop on AVEC},
  Year                     = {2011},

  Bibsource                = {DBLP, http://dblp.uni-trier.de},
  Ee                       = {http://dx.doi.org/10.1007/978-3-642-24571-8_45}
}

@InProceedings{Dahmane_FERA11,
  Title                    = {{E}motion {R}ecognition using {D}ynamic {G}rid-based {H}o{G} {F}eatures},
  Author                   = {Mohamed Dahmane and Jean Meunier},
  Booktitle                = {Proc. FG Workshop on FERA Challenge},
  Year                     = {2011},

  Owner                    = {Songfan},
  Timestamp                = {2011.05.01}
}

@InProceedings{Dalal_CVPR05,
  Title                    = {{H}istograms of {O}riented {G}radients for {H}uman {D}etection},
  Author                   = {Dalal, N. and Triggs, B.},
  Booktitle                = {Proc. CVPR},
  Year                     = {2005}
}

@InProceedings{intraface,
  Title                    = {{I}ntraFace},
  Author                   = {De la Torre, F. and Wen-Sheng Chu and Xuehan Xiong and Vicente, F. and Xiaoyu Ding and Cohn, J.},
  Booktitle                = {IEEE International Conference and Workshops on Automatic Face and Gesture Recognition},
  Year                     = {2015}
}

@InProceedings{Dhall_FERA11,
  Title                    = {{E}motion {R}ecognition {U}sing \uppercase{PHOG} and \uppercase{LPQ} features},
  Author                   = {Abhinav Dhall and Akshay Asthana and Roland Goecke and Tom Gedeon},
  Booktitle                = {Proc. FG Workshop on FERA Challenge},
  Year                     = {2011},

  Owner                    = {Songfan},
  Timestamp                = {2011.05.01}
}

@Article{Donato_PAMI99,
  Title                    = {{C}lassifying {F}acial {A}ctions},
  Author                   = {Donato, G. and Bartlett, M.S. and Hager, J.C. and Ekman, P. and Sejnowski, T.J.},
  Journal                  = {IEEE Trans. PAMI},
  Year                     = {1999},

  Abstract                 = {{T}he facial action coding system ({FAGS}) is an objective method for quantifying facial movement in terms of component actions. {T}his paper explores and compares techniques for automatically recognizing facial actions in sequences of images. {T}hese techniques include: analysis of facial motion through estimation of optical flow; holistic spatial analysis, such as principal component analysis, independent component analysis, local feature analysis, and linear discriminant analysis; and methods based on the outputs of local filters, such as {G}abor wavelet representations and local principal components. {P}erformance of these systems is compared to naive and expert human subjects. {B}est performances were obtained using the {G}abor wavelet representation and the independent component representation, both of which achieved 96 percent accuracy for classifying 12 facial actions of the upper and lower face. {T}he results provide converging evidence for the importance of using local filters, high spatial frequencies, and statistical independence for classifying facial actions},
  ISSN                     = {0162-8828},
  Keywords                 = {Gabor wavelet;computer vision;facial action coding system;facial expression recognition;image sequences;independent component analysis;linear discriminant analysis;local feature analysis;motion estimation;optical flow;principal component analysis;computer vision;face recognition;image sequences;motion estimation;principal component analysis;wavelet transforms;}
}

@Article{Beltfast,
  Title                    = {{E}motional {S}peech: {T}owards {A} {N}ew {G}eneration of {D}atabases},
  Author                   = {Ellen Douglas-Cowie and Nick Campbell and Roddy Cowie and Peter Roach},
  Journal                  = {Speech Communication},
  Year                     = {2003}
}

@InProceedings{Douglas-Cowie00,
  Title                    = {{A} {N}ew {E}motion {D}atabase: {C}onsiderations, {S}ources and {S}cope},
  Author                   = {Ellen Douglas-Cowie and Roddy Cowie and Marc Schröder},
  Booktitle                = {Proc. the ISCA Workshop on Speech and Emotion},
  Year                     = {2000}
}

@Book{Ekman78,
  Title                    = {{F}acial {A}ction {C}oding {S}ystem: {A} {T}echnique for the {M}easurement of {F}acial {M}ovement},
  Author                   = {Ekman, P. and Friesen, W.},
  Publisher                = {Consulting Psychologists Press},
  Year                     = {1978},

  Journal                  = {Consulting Psychologists Press},
  Owner                    = {Songfan},
  Timestamp                = {2012.02.20}
}

@Book{Ekman2005,
  Title                    = {What the Face Reveals: Basic and Applied Studies of Spontaneous Expression Using the Facial Action Coding System (FACS)},
  Author                   = {Paul Ekman and Erika Rosenberg},
  Publisher                = {Oxford University Press},
  Year                     = {2005},

  Owner                    = {Songfan},
  Timestamp                = {2014.04.04}
}

@InProceedings{Kaliouby_SMC04,
  Title                    = {{M}ind {R}eading {M}achines: {A}utomated {I}nference of {C}ognitive {M}ental {S}tates from {V}ideo},
  Author                   = {El Kaliouby, R. and Robinson, P.},
  Booktitle                = {Proc. SMC},
  Year                     = {2004},

  Abstract                 = {{M}ind reading encompasses our ability to attribute mental states to others, and is essential for operating in a complex social environment. {T}he goal in building mind reading machines is to enable computer technologies to understand and react to people's emotions and mental states. {T}his paper describes a system, for the automated inference of cognitive mental states from observed facial expressions and head gestures in video. {T}he system is based on a multilevel dynamic {B}ayesian network classifier which models cognitive mental states as a number of interacting facial and head displays. {E}xperimental results yield an average recognition rate of 87.4% for 6 mental states groups: agreement, concentrating, disagreement, interested, thinking and unsure. {R}eal time performance, unobtrusiveness and lack of preprocessing make our system particularly suitable for user-independent human computer interaction},
  ISSN                     = {1062-922X},
  Keywords                 = {automated inference;cognitive mental states;complex social environment;head gestures;mind reading machines;multilevel dynamic Bayesian network classifier;observed facial expressions;user-independent human computer interaction;Bayes methods;belief networks;cognition;emotion recognition;human computer interaction;inference mechanisms;}
}

@Article{Essa_PAMI97,
  Title                    = {{C}oding, {A}nalysis, {I}nterpretation, and {R}ecognition of {F}acial {E}xpressions},
  Author                   = {Essa, I.A. and Pentland, A.P.},
  Journal                  = {IEEE Trans. PAMI},
  Year                     = {1997},

  Abstract                 = {{W}e describe a computer vision system for observing facial motion by using an optimal estimation optical flow method coupled with geometric, physical and motion-based dynamic models describing the facial structure. {O}ur method produces a reliable parametric representation of the face's independent muscle action groups, as well as an accurate estimate of facial motion. {P}revious efforts at analysis of facial expression have been based on the facial action coding system ({FACS}), a representation developed in order to allow human psychologists to code expression from static pictures. {T}o avoid use of this heuristic coding scheme, we have used our computer vision system to probabilistically characterize facial motion and muscle activation in an experimental population, thus deriving a new, more accurate, representation of human facial expressions that we call {FACS}+. {F}inally, we show how this method can be used for coding, analysis, interpretation, and recognition of facial expressions},
  ISSN                     = {0162-8828},
  Keywords                 = {FACS;FACS+;computer vision system;facial action coding system;facial expression recognition;facial motion;facial structure;image analysis;image coding;image interpretation;independent muscle action groups;muscle activation;optimal estimation optical flow method;probabilistic characterization;face recognition;image coding;image sequences;motion estimation;}
}

@Article{LIBLINEAR,
  Title                    = {{LIBLINEAR}: {A} {L}ibrary for {L}arge {L}inear {C}lassification},
  Author                   = {Rong-En Fan and Kai-Wei Chang and Cho-Jui Hsieh and Xiang-Rui Wang and Chih-Jen Lin},
  Journal                  = {JMLR},
  Year                     = {2008}
}

@Article{Freeman_IJCV00,
  Title                    = {{L}earning {L}ow-{L}evel {V}ision},
  Author                   = {Freeman, William T. and Pasztor, Egon C. and Carmichael, Owen T.},
  Journal                  = {IJCV},
  Year                     = {2000},

  Issue                    = {1}
}

@InProceedings{Gehrig_CVPRW11,
  Title                    = {{A} {C}ommon {F}ramework for {R}eal-time {E}motion {R}ecognition and {F}acial {A}ction {U}nit {D}etection},
  Author                   = {Gehrig, T. and Ekenel, H.K.},
  Booktitle                = {Proc. CVPR Workshops},
  Year                     = {2011},

  ISSN                     = {2160-7508},
  Keywords                 = {FG 2011 facial expression analysis;FG 2011 facial expression recognition;discrete cosine transform;facial action unit detection;local appearance-based face representation approach;real-time emotion recognition;support vector machine classifiers;discrete cosine transforms;emotion recognition;face recognition;image classification;support vector machines;}
}

@Article{Han_PAMI06,
  Title                    = {{I}ndividual {R}ecognition {U}sing {G}ait {E}nergy {I}mage},
  Author                   = {Han, J. and Bhanu, B.},
  Journal                  = {IEEE Trans. PAMI},
  Year                     = {2006},

  Abstract                 = {{I}n this paper, we propose a new spatio-temporal gait representation, called {G}ait {E}nergy {I}mage ({GEI}), to characterize human walking properties for individual recognition by gait. {T}o address the problem of the lack of training templates, we also propose a novel approach for human recognition by combining statistical gait features from real and synthetic templates. {W}e directly compute the real templates from training silhouette sequences, while we generate the synthetic templates from training sequences by simulating silhouette distortion. {W}e use a statistical approach for learning effective features from real and synthetic templates. {W}e compare the proposed {GEI}-based gait recognition approach with other gait recognition approaches on {USF} {H}uman{ID} {D}atabase. {E}xperimental results show that the proposed {GEI} is an effective and efficient gait representation for individual recognition, and the proposed approach achieves highly competitive performance with respect to the published gait recognition approaches},
  Doi                      = {10.1109/TPAMI.2006.38},
  ISSN                     = {0162-8828},
  Keywords                 = {distortion analysis;feature fusion;gait energy image;human recognition;human walking properties;individual recognition;spatio-temporal gait representation;statistical gait features;gait analysis;image recognition;}
}

@InProceedings{Hu_IVC06,
  Title                    = {{M}anifold {B}ased {A}nalysis of {F}acial {E}xpression},
  Author                   = {Changbo Hu and Ya Chang and Feris, R. and Turk, M.},
  Booktitle                = {Image and Vision Computing},
  Year                     = {2006}
}

@InProceedings{Huang_ICCV07,
  Title                    = {{U}nsupervised {J}oint {A}lignment of {C}omplex {I}mages},
  Author                   = {Huang, G.B. and Jain, V. and Learned-Miller, E.},
  Booktitle                = {Proc. ICCV},
  Year                     = {2007},

  ISSN                     = {1550-5499},
  Keywords                 = {canonical pose recognition;class-specialized learning algorithm;face detector;face recognition;image recognition algorithm;unsupervised joint alignment;face recognition;pose estimation;unsupervised learning;}
}

@Book{Huber81,
  Title                    = {{R}obust {S}tatistics},
  Author                   = {Huber, P. J.},
  Publisher                = {John Wiley \& Sons, Inc.},
  Year                     = {1981},

  Address                  = {Hoboken, NJ},

  Owner                    = {Songfan},
  Timestamp                = {2012.02.22}
}

@Article{Irani91,
  Title                    = {Improving resolution by image registration },
  Author                   = {Michal Irani and Shmuel Peleg},
  Journal                  = {Graphical Models and Image Processing },
  Year                     = {1991},
  Number                   = {3},
  Pages                    = {231 - 239},
  Volume                   = {53},

  Abstract                 = {Image resolution can be improved when the relative displacements in image sequences are known accurately, and some knowledge of the imaging process is available. The proposed approach is similar to back-projection used in tomography. Examples of improved image resolution are given for gray-level and color images, when the unknown image displacements are computed from the image sequence. },
  Doi                      = {http://dx.doi.org/10.1016/1049-9652(91)90045-L},
  ISSN                     = {1049-9652}
}

@Article{Josephine2003,
  Title                    = {Why Do Consumers Stop Viewing Television Commercials? Two Experiments on the Influence of Moment-to-Moment Entertainment and Information Value},
  Author                   = {Josephine L.C.M. Woltman Elpers, Michel Wedel, Rik G.M. Pieters},
  Journal                  = {Journal of Marketing Research},
  Year                     = {2003},

  Owner                    = {Songfan},
  Timestamp                = {2014.04.14}
}

@InProceedings{close_loop_icra_05,
  Title                    = {A Markov Chain Monte Carlo Approach to Closing the Loop in {SLAM}},
  Author                   = {Kaess, M. and Dellaert, F.},
  Booktitle                = {IEEE International Conference on Robotics and Automation},
  Year                     = {2005},
  Month                    = {April},
  Pages                    = {643-648},

  Doi                      = {10.1109/ROBOT.2005.1570190},
  Keywords                 = {SLAM;localization;loop closing;mapping;Educational institutions;Iterative algorithms;Kalman filters;Large-scale systems;Monte Carlo methods;Motion estimation;Partitioning algorithms;Probability distribution;Robots;Simultaneous localization and mapping;SLAM;localization;loop closing;mapping}
}

@InProceedings{Kanade_FG00,
  Title                    = {Comprehensive database for facial expression analysis},
  Author                   = {Kanade, T. and Cohn, J.F. and YingLi Tian},
  Booktitle                = {IEEE International Conference on Automatic Face and Gesture Recognition},
  Year                     = {2000},
  Pages                    = {46-53},

  Doi                      = {10.1109/AFGR.2000.840611},
  Keywords                 = {face recognition;image sequences;reliability;visual databases;CMU-Pittsburgh AU-Coded Face Expression Image Database;FACS action units;description level;digitized image sequences;eliciting conditions;expression transitions;facial expression analysis;head orientation;image characteristics;non-verbal behavior;reliability;scene complexity;subject differences;validity;Face recognition;Facial features;Gold;Head;Image analysis;Image databases;Layout;Prototypes;Robots;Testing}
}

@InProceedings{Keren_CVPR88,
  Title                    = {Image sequence enhancement using sub-pixel displacements},
  Author                   = {Keren, D. and Peleg, S. and Brada, R.},
  Booktitle                = {Computer Society Conference on Computer Vision and Pattern Recognition},
  Year                     = {1988},
  Month                    = {Jun},
  Pages                    = {742-746},

  Doi                      = {10.1109/CVPR.1988.196317},
  ISSN                     = {1063-6919},
  Keywords                 = {computerised pattern recognition;computerised picture processing;computerised picture processing;image sequence enhancement;rotation;subpixel displacement;translation;Cameras;Computer science;Fourier transforms;Frequency domain analysis;Image registration;Image resolution;Image sampling;Image sequences;Spatial resolution;Taylor series}
}

@Article{Kotsia_IP07,
  Title                    = {{F}acial {E}xpression {R}ecognition in {I}mage {S}equences {U}sing {G}eometric {D}eformation {F}eatures and {S}upport {V}ector {M}achines},
  Author                   = {Kotsia, I. and Pitas, I.},
  Journal                  = {IEEE Trans. IP},
  Year                     = {2007},

  ISSN                     = {1057-7149},
  Keywords                 = {Candide grid nodes;SVM;facial action units;facial expression;facial expression recognition;geometric deformation features;grid-tracking;image sequences;support vector machines;video frames;face recognition;image sequences;support vector machines;Algorithms;Artificial Intelligence;Face;Facial Expression;Humans;Image Enhancement;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Pattern Recognition, Automated;Subtraction Technique;Video Recording;}
}

@Article{LearnedMiller_PAMI06,
  Title                    = {Data driven image models through continuous joint alignment},
  Author                   = {Learned-Miller, E.G.},
  Journal                  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  Year                     = {2006},

  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {236-250},
  Volume                   = {28},

  Doi                      = {10.1109/TPAMI.2006.34},
  ISSN                     = {0162-8828},
  Keywords                 = {computer vision;handwritten character recognition;image resolution;magnetic resonance imaging;nonparametric statistics;bias removal;congealing;continuous joint alignment;data driven image models;handwritten character recognition;handwritten digit classifier;magnetic resonance images;nuisance variables;Biomedical imaging;Character recognition;Computer vision;Entropy;Handwriting recognition;Image generation;Magnetic resonance;Maximum likelihood estimation;Robustness;Statistics;Index Terms- Alignment;artifact removal;bias removal;clustering;congealing;correspondence;density estimation;entropy;magnetic resonance imaging;maximum likelihood;medical imaging;nonparametric statistics;registration;unsupervised learning.;Algorithms;Artificial Intelligence;Automatic Data Processing;Computer Simulation;Documentation;Handwriting;Image Enhancement;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Models, Theoretical;Pattern Recognition, Automated;Subtraction Technique}
}

@InProceedings{Littlewort_IVC06,
  Title                    = {{D}ynamics of {F}acial {E}xpression {E}xtracted {A}utomatically from {V}ideo},
  Author                   = {Littlewort, G. and Bartlett, M.S. and Fasel, I. and Susskind, J. and Movellan, J.},
  Booktitle                = {Image and Vision Computing},
  Year                     = {2006},

  Abstract                 = { {W}e present a systematic comparison of machine learning methods applied to the problem of fully automatic recognition of facial expressions, including {A}da{B}oost, support vector machines, and linear discriminant analysis. {E}ach video-frame is first scanned in real-time to detect approximately upright-frontal faces. {T}he faces found are scaled into image patches of equal size, convolved with a bank of {G}abor energy filters, and then passed to a recognition engine that codes facial expressions into 7 dimensions in real time: neutral, anger, disgust, fear, joy, sadness, surprise. {W}e report results on a series of experiments comparing spatial frequency ranges, feature selection techniques, and recognition engines. {B}est results were obtained by selecting a subset of {G}abor filters using {A}da{B}oost and then training {S}upport {V}ector {M}achines on the outputs of the filters selected by {A}da{B}oost. {T}he generalization performance to new subjects for a 7-way forced choice was 93% or more correct on two publicly available datasets, the best performance reported so far on these datasets. {S}urprisingly, registration of internal facial features was not necessary, even though the face detector does not provide precisely registered images. {T}he outputs of the classifier change smoothly as a function of time and thus can be used for unobtrusive motion capture. {W}e developed an end-to-end system that provides facial expression codes at 24 frames per second and animates a computer generated character. {I}n real-time this expression mirror operates down to resolutions of 16 pixels from eye to eye. {W}e also applied the system to fully automated facial action coding.},
  Doi                      = {10.1109/CVPR.2004.53}
}

@InProceedings{Bartlett_FG11,
  Title                    = {{C}omputer {E}xpression {R}ecognition {T}oolbox},
  Author                   = {Littlewort, G. and Whitehill, J and Wu, T. and Fasel, I. and Frank,M. and Movellan, J. and Bartlett, M.},
  Booktitle                = {IEEE International Conference on Automatic Face and Gesture Recognition},
  Year                     = {2011},
  Month                    = {May},

  Abstract                 = {{W}e present a live demo of the {C}omputer {E}xpression {R}ecognition {T}oolbox ({CERT}) developed at {U}niversity of {C}alifornia, {S}an {D}iego. {CERT} measures facial expressions in real-time, and codes them with respect to expressions of basic emotion, as well as over 20 facial actions from the {F}acial {A}ction {C}oding {S}ystem ({E}kman amp; {F}riesen, 1978). {H}ead pose (yaw, pitch, and roll) is also detected using an algorithm presented at this conference ({W}hitehill amp; {M}ovellan, 2008). {A} sample output is shown in {F}igure 1.},
  Doi                      = {10.1109/AFGR.2008.4813406},
  Keywords                 = {computer expression recognition toolbox;emotion recognition;facial action coding system;facial expression;pose estimation;emotion recognition;face recognition;image coding;pose estimation;}
}

@InProceedings{Littlewort_CERT_FG2011,
  Title                    = {The computer expression recognition toolbox ({CERT})},
  Author                   = {Littlewort, Gwen and Whitehill, J. and Tingfan Wu and Fasel, Ian and Frank, M. and Movellan, J. and Bartlett, M.},
  Booktitle                = {IEEE International Conference on Automatic Face Gesture Recognition and Workshops},
  Year                     = {2011},
  Month                    = {March},
  Pages                    = {298-305},

  Doi                      = {10.1109/FG.2011.5771414},
  Keywords                 = {emotion recognition;face recognition;image coding;software tools;3D orientation;CERT;FACS;automatic real-time facial expression recognition;computer expression recognition toolbox;dual core laptop;extended Cohn-Kanade;facial action unit coding system;facial expression dataset;software tool;two-alternative forced choice task;Accuracy;Detectors;Encoding;Face;Face recognition;Facial features;Gold}
}

@InProceedings{Littlewort_FERA11,
  Title                    = {{T}he {M}otion in {E}motion – {A} {CERT} {B}ased {A}pproach to the {FERA} {E}motion {C}hallenge},
  Author                   = {Gwen Littlewort and Jacob Whitehill and Ting-Fan Wu and Nicholas Butko and Paul Ruvolo and Javier Movellan and Marian Bartlett},
  Booktitle                = {Proc. FG Workshop on FERA Challenge},
  Year                     = {2011},

  Owner                    = {Songfan},
  Timestamp                = {2011.04.29}
}

@Article{Liu_PAMI11,
  Title                    = {{SIFT} {F}low: Dense Correspondence across Scenes and Its Applications},
  Author                   = {Ce Liu and Yuen, J. and Torralba, A.},
  Journal                  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  Year                     = {2011},
  Number                   = {5},
  Pages                    = {978-994},
  Volume                   = {33},

  Doi                      = {10.1109/TPAMI.2010.147},
  ISSN                     = {0162-8828},
  Keywords                 = {computer vision;transforms;SIFT flow;computer vision;face recognition;image alignment;image analysis;image corpus;image information;image registration;image synthesis;motion synthesis;object transfer;optical flow;scale invariance feature transform;Belief propagation;Complexity theory;Databases;Object recognition;Optical imaging;Pixel;Visualization;SIFT flow;Scene alignment;alignment-based large database framework;belief propagation;coarse to fine;dense scene correspondence;face recognition;motion prediction for a single image;motion synthesis via object transfer.;satellite image registration;Algorithms;Humans;Image Processing, Computer-Assisted;Motion;Pattern Recognition, Automated;Video Recording}
}

@Article{SIFT,
  Title                    = {Distinctive Image Features from Scale-Invariant Keypoints},
  Author                   = {Lowe, David G.},
  Journal                  = {International Journal of Computer Vision},
  Year                     = {2004},
  Number                   = {2},
  Pages                    = {91-110},
  Volume                   = {60},

  Doi                      = {10.1023/B:VISI.0000029664.99615.94},
  ISSN                     = {0920-5691},
  Keywords                 = {invariant features; object recognition; scale invariance; image matching}
}

@InProceedings{Lucey_FG06,
  Title                    = {\uppercase{AAM} {D}erived {F}ace {R}epresentations for {R}obust {F}acial {A}ction {R}ecognition},
  Author                   = {Lucey, S. and Matthews, I. and Changbo Hu and Ambadar, Z. and de la Torre, F. and Cohn, J.},
  Booktitle                = {Proc. FG},
  Year                     = {2006},

  Doi                      = {10.1109/FGR.2006.17},
  Keywords                 = {active appearance model;face representations;normalization methods;robust facial action recognition;face recognition;image representation;}
}

@Article{Martinez_PAMI13,
  Title                    = {Local Evidence Aggregation for Regression-Based Facial Point Detection},
  Author                   = {Martinez, B. and Valstar, M.F. and Binefa, X. and Pantic, M.},
  Journal                  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  Year                     = {2013},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {1149-1163},
  Volume                   = {35},

  Doi                      = {10.1109/TPAMI.2012.205},
  ISSN                     = {0162-8828},
  Keywords                 = {face recognition;object detection;regression analysis;shape recognition;exhaustive-search approach;face shape model;facial point detection;local evidence aggregation;near-frontal face image;probabilistic graphical model;regression-based approach;sequential approximation;target location detection;Face;Feature extraction;Prediction algorithms;Shape;Support vector machines;Training;Vectors;Facial point detection;object detection;probabilistic graphical networks;support vector regression;Biometric Identification;Databases, Factual;Face;Humans;Image Processing, Computer-Assisted;Regression Analysis;Support Vector Machines}
}

@Article{Matthews_IJCV03,
  Title                    = {{A}ctive {A}ppearance {M}odels {R}evisited},
  Author                   = {Iain Matthews and Simon Baker},
  Journal                  = {IJCV},
  Year                     = {2003}
}

@Article{McDuff_TAC14,
  Title                    = {Predicting Ad Liking and Purchase Intent: Large-Scale Analysis of Facial Responses to Ads},
  Author                   = {McDuff, D. and El Kaliouby, R. and Cohn, J.F. and Picard, R.W.},
  Journal                  = {IEEE Transactions on Affective Computing},
  Year                     = {2015},
  Number                   = {3},
  Pages                    = {223-235},
  Volume                   = {6}
}

@InProceedings{Meng_FERA11,
  Title                    = {{E}motion {R}ecognition by {T}wo {V}iew \uppercase{SVM} 2\uppercase{K} {C}lassifier on {D}ynamic {F}acial {E}xpression {F}eatures},
  Author                   = {Hongying Meng and Bernardino Romera-Paredes and Nadia Bianchi-Berthouze},
  Booktitle                = {Proc. FG Workshop on FERA Challenge},
  Year                     = {2011},

  Owner                    = {Songfan},
  Timestamp                = {2011.05.01}
}

@InCollection{Meng05,
  Title                    = {{S}upport {V}ector {M}achine to {S}ynthesise {K}ernels},
  Author                   = {Meng, Hongying and Shawe-Taylor, John and Szedmak, Sandor and Farquhar, Jason},
  Booktitle                = {Deterministic and Statistical Methods in Machine Learning},
  Publisher                = {Springer Berlin / Heidelberg},
  Year                     = {2005},
  Editor                   = {Winkler, Joab and Niranjan, Mahesan and Lawrence, Neil},

  Affiliation              = {School of Electronics and Computer Science, University of Southampton, Southampton, SO17 1BJ UK}
}

@InProceedings{Negri06,
  Title                    = {An Oriented-Contour Point Based Voting Algorithm for Vehicle Type Classification},
  Author                   = {Pablo Negri and Xavier Clady and Maurice Milgram and Raphael Poulenard},
  Booktitle                = {Proc. International Conference on Pattern Recognition},
  Year                     = {2006}
}

@Article{Ojala_PAMI02,
  Title                    = {Multiresolution gray-scale and rotation invariant texture classification with local binary patterns},
  Author                   = {Ojala, T. and Pietik\"ainen, M. and Maenpaa, T.},
  Journal                  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  Year                     = {2002},

  Month                    = {Jul},
  Number                   = {7},
  Pages                    = {971-987},
  Volume                   = {24},

  Doi                      = {10.1109/TPAMI.2002.1017623},
  ISSN                     = {0162-8828},
  Keywords                 = {image classification;image texture;invariance;nonparametric statistics;angular space;computational simplicity;gray-scale variations;local binary patterns;local image texture;multiresolution analysis;multiresolution gray-scale texture classification;nonparametric discrimination;occurrence histogram;prototype distributions;rotation invariant texture classification;sample distributions;spatial resolution;uniform patterns;Gray-scale;Histograms;Image recognition;Image texture;Multiresolution analysis;Pattern recognition;Prototypes;Quantization;Robustness;Spatial resolution}
}

@InProceedings{LPQ,
  Title                    = {Blur insensitive texture classification using local phase quantization},
  Author                   = {Ville Ojansivu and Janne Heikkil\"a},
  Booktitle                = {International Conference on Image and Signal Processing},
  Year                     = {2008},
  Pages                    = {236--243}
}

@InProceedings{LPQ-TOP,
  Title                    = {Volume Local Phase Quantization for Blur-Insensitive Dynamic Texture Classification},
  Author                   = {P\"aiv\"arinta, J. and Rahtu, E. and Heikkil\"a, J.},
  Booktitle                = {Proc. Scandinavian Conference on Image Analysis (SCIA)},
  Year                     = {2011},

  Owner                    = {chidi},
  Timestamp                = {2015.05.13}
}

@Article{Pantic_SMCB06,
  Title                    = {{D}ynamics of {F}acial {E}xpression: {R}ecognition of {F}acial {A}ctions and {T}heir {T}emporal {S}egments from {F}ace {P}rofile {I}mage {S}equences},
  Author                   = {Pantic, M. and Patras, I.},
  Journal                  = {IEEE Trans. SMC-B},
  Year                     = {2006},

  Abstract                 = {{A}utomatic analysis of human facial expression is a challenging problem with many applications. {M}ost of the existing automated systems for facial expression analysis attempt to recognize a few prototypic emotional expressions, such as anger and happiness. {I}nstead of representing another approach to machine analysis of prototypic facial expressions of emotion, the method presented in this paper attempts to handle a large range of human facial behavior by recognizing facial muscle actions that produce expressions. {V}irtually all of the existing vision systems for facial muscle action detection deal only with frontal-view face images and cannot handle temporal dynamics of facial actions. {I}n this paper, we present a system for automatic recognition of facial action units ({AU}s) and their temporal models from long, profile-view face image sequences. {W}e exploit particle filtering to track 15 facial points in an input face-profile sequence, and we introduce facial-action-dynamics recognition from continuous video input using temporal rules. {T}he algorithm performs both automatic segmentation of an input video into facial expressions pictured and recognition of temporal segments (i.e., onset, apex, offset) of 27 {AU}s occurring alone or in a combination in the input face-profile video. {A} recognition rate of 87% is achieved.},
  ISSN                     = {1083-4419},
  Keywords                 = {automatic facial action unit recognition;automatic human facial expression analysis;automatic video segmentation;computer vision system;emotional expression recognition;face profile image sequences;facial muscle action recognition;facial-action-dynamics recognition;human facial behavior;particle filtering;temporal model;temporal segment recognition;computer vision;emotion recognition;face recognition;image segmentation;image sequences;particle filtering (numerical methods);Algorithms;Artificial Intelligence;Cluster Analysis;Face;Facial Expression;Humans;Image Enhancement;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Movement;Pattern Recognition, Automated;Photography;Reproducibility of Results;Sensitivity and Specificity;Subtraction Technique;Time Factors;Video Recording;}
}

@Article{Pantic_SMCB04,
  Title                    = {{F}acial {A}ction {R}ecognition for {F}acial {E}xpression {A}nalysis from {S}tatic {F}ace {I}mages},
  Author                   = {Pantic, M. and Rothkrantz, L.J.M.},
  Journal                  = {IEEE Trans. SMC-B},
  Year                     = {2004},

  Abstract                 = {{A}utomatic recognition of facial gestures (i.e., facial muscle activity) is rapidly becoming an area of intense interest in the research field of machine vision. {I}n this paper, we present an automated system that we developed to recognize facial gestures in static, frontal- and/or profile-view color face images. {A} multidetector approach to facial feature localization is utilized to spatially sample the profile contour and the contours of the facial components such as the eyes and the mouth. {F}rom the extracted contours of the facial features, we extract ten profile-contour fiducial points and 19 fiducial points of the contours of the facial components. {B}ased on these, 32 individual facial muscle actions ({AU}s) occurring alone or in combination are recognized using rule-based reasoning. {W}ith each scored {AU}, the utilized algorithm associates a factor denoting the certainty with which the pertinent {AU} has been scored. {A} recognition rate of 86% is achieved.},
  ISSN                     = {1083-4419},
  Keywords                 = {facial action recognition;facial component contour sampling;facial expression analysis;facial feature localization;facial gesture recognition;facial muscle action units;image processing;profile contour sampling;profile-contour fiducial points;rule-based reasoning;spatial reasoning;static face images;face recognition;feature extraction;gesture recognition;image colour analysis;knowledge based systems;spatial reasoning;uncertainty handling;Algorithms;Artificial Intelligence;Face;Facial Expression;Humans;Image Interpretation, Computer-Assisted;Pattern Recognition, Automated;Photography;Posture;Reproducibility of Results;Sensitivity and Specificity;}
}

@Article{Pantic_PAMI00,
  Title                    = {{A}utomatic {A}nalysis of {F}acial {E}xpressions: {T}he {S}tate of the {A}rt},
  Author                   = {Pantic, M. and Rothkrantz, L.J.M.},
  Journal                  = {IEEE Trans. PAMI},
  Year                     = {2000},

  Doi                      = {10.1109/34.895976},
  ISSN                     = {0162-8828},
  Keywords                 = {automatic analysis;automatic facial expression analyzer;emotion categories;facial expressions;human visual system;human-like interaction;image segment detection;face recognition;feature extraction;image classification;image sequences;neural nets;}
}

@InProceedings{Pantic_ICME05,
  Title                    = {{W}eb-based {D}atabase for {F}acial {E}xpression {A}nalysis},
  Author                   = {Pantic, M. and Valstar, M. and Rademaker, R. and Maat, L.},
  Booktitle                = {Proc. IEEE Int’l Conf. Multimedia and Expo.},
  Year                     = {2005},
  Pages                    = {317-321},

  Abstract                 = { {I}n the last decade, the research topic of automatic analysis of facial expressions has become a central topic in machine vision research. {N}onetheless, there is a glaring lack of a comprehensive, readily accessible reference set of face images that could be used as a basis for benchmarks for efforts in the field. {T}his lack of easily accessible, suitable, common testing resource forms the major impediment to comparing and extending the issues concerned with automatic facial expression analysis. {I}n this paper, we discuss a number of issues that make the problem of creating a benchmark facial expression database difficult. {W}e then present the {MMI} facial expression database, which includes more than 1500 samples of both static images and image sequences of faces in frontal and in profile view displaying various expressions of emotion, single and multiple facial muscle activation. {I}t has been built as a {W}eb-based direct-manipulation application, allowing easy access and easy search of the available images. {T}his database represents the most comprehensive reference set of images for studies on facial expression analysis to date.},
  Doi                      = {10.1109/ICME.2005.1521424},
  Keywords                 = { MMI facial expression database; Web-based direct-manipulation application; image representation; image sequence; man-machine interaction; multiple facial muscle activation; static image; Internet; distributed databases; emotion recognition; face recognition; image representation; image sequences; visual databases;}
}

@InProceedings{Pearce11,
  Title                    = {Automatic make and model recognition from frontal images of cars},
  Author                   = {Pearce, G. and Pears, N.},
  Booktitle                = {IEEE International Conference on Advanced Video and Signal-Based Surveillance},
  Year                     = {2011},
  Month                    = {Aug},
  Pages                    = {373-378},

  Doi                      = {10.1109/AVSS.2011.6027353},
  Keywords                 = {automobiles;image classification;video surveillance;Harris corner strengths;Naive Bayes classifier;automatic make and model recognition;cars;classification approaches;feature detection;frontal images;hierarchical fashion;k nearest neighbour classifier;Detectors;Feature extraction;Image edge detection;Licenses;Testing;Training;Vehicles}
}

@Article{sklearn,
  Title                    = {Scikit-learn: Machine Learning in {P}ython},
  Author                   = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2011}
}

@Article{Peng_PAMI12,
  Title                    = {{RASL}: Robust Alignment by Sparse and Low-Rank Decomposition for Linearly Correlated Images},
  Author                   = { Yigang Peng and A. Ganesh and J. Wright and Wenli Xu and Yi Ma},
  Journal                  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  Year                     = {2012},
  Number                   = {11},
  Pages                    = {2233-2246},
  Volume                   = {34},

  Address                  = {Los Alamitos, CA, USA},
  Doi                      = {http://doi.ieeecomputersociety.org/10.1109/TPAMI.2011.282},
  ISSN                     = {0162-8828},
  Publisher                = {IEEE Computer Society}
}

@InProceedings{Petrovic04,
  Title                    = {Analysis of Features for Rigid Structure Vehicle Type Recognition},
  Author                   = {V. S. Petrovi\'c and T. F. Cootes},
  Booktitle                = {British Machine Vision Conference},
  Year                     = {2004},
  Pages                    = {587--596}
}

@Article{Pham_06,
  Title                    = {{R}obust {F}usion of {I}rregularly {S}ampled {D}ata {U}sing {A}daptive {N}ormalized {C}onvolution},
  Author                   = {Pham, Tuan Q. and van Vliet, Lucas J. and Schutte, Klamer},
  Journal                  = {EURASIP Journal on Applied Signal Processing},
  Year                     = {2006}
}

@Book{Russell97,
  Title                    = {{T}he {P}sychology of {F}acial {E}xpression},
  Author                   = {James A. Russell and Jos\'e Miguel Fern\'andez-Dols},
  Publisher                = {Cambridge University Press},
  Year                     = {1997},

  Owner                    = {songfan},
  Timestamp                = {2011.04.28}
}

@InProceedings{Saragih_ICCV09,
  Title                    = {{F}ace {A}lignment through {S}ubspace {C}onstrained {M}ean-{S}hifts},
  Author                   = {Saragih, J.M. and Lucey, S. and Cohn, J.F.},
  Booktitle                = {Proc. ICCV},
  Year                     = {2009},

  Doi                      = {10.1109/ICCV.2009.5459377},
  ISSN                     = {1550-5499},
  Keywords                 = {computer vision community;deformable model fitting;distribution replacement;face alignment;local detector;model landmark distribution;principled optimization strategy;smoothed estimate hierarchy;subspace constrained mean shifts;face recognition;optimisation;}
}

@Article{Scharstein_IJCV02,
  Title                    = {{A} {T}axonomy and {E}valuation of {D}ense {T}wo-frame {S}tereo {C}orrespondence {A}lgorithms},
  Author                   = {Daniel Scharstein and Richard Szeliski},
  Journal                  = {IJCV},
  Year                     = {2002}
}

@InProceedings{Senechal_FERA11,
  Title                    = {{A}ccumulated {M}otion {I}mages for {F}acial {E}xpression {R}ecognition in {V}ideos},
  Author                   = {Ruchir Srivastava and Sujoy Roy and Shuicheng Yan and Terence Sim},
  Booktitle                = {Proc. FG Workshop on FERA Challenge},
  Year                     = {2011},

  Owner                    = {Songfan},
  Timestamp                = {2011.05.01}
}

@InProceedings{Sun_CVPR08,
  Title                    = {Image super-resolution using gradient profile prior},
  Author                   = {Jian Sun and Zongben Xu and Heung-Yeung Shum},
  Booktitle                = {IEEE Conference on Computer Vision and Pattern Recognition},
  Year                     = {2008},
  Month                    = {June},
  Pages                    = {1-8},

  Doi                      = {10.1109/CVPR.2008.4587659},
  ISSN                     = {1063-6919},
  Keywords                 = {gradient methods;image resolution;natural scenes;generic image prior;gradient profile prior;hi-resolution image;image gradients;image super-resolution;low-resolution image;natural images;parametric prior;Asia;Frequency;Image reconstruction;Image resolution;Interpolation;Learning systems;Parametric statistics;Pixel;Shape;Statistical distributions}
}

@Article{Szeliski06,
  Title                    = {Image Alignment and Stitching: A Tutorial},
  Author                   = {Richard Szeliski},
  Journal                  = {Foundations and Trends in Computer Graphics and Vision},
  Year                     = {2006},
  Number                   = {1},
  Pages                    = {1-104},
  Volume                   = {2}
}

@InProceedings{Tariq_FERA11,
  Title                    = {{E}motion {R}ecognition from an {E}nsemble of {F}eatures},
  Author                   = {Usman Tariq and Kai-Hsiang Lin and Zhen Li and Xi Zhou and Zhaowen Wang and Vuong Le and Thomas S. Huang and Xutao Lv and Tony X. Han},
  Booktitle                = {Proc. FG Workshop on FERA Challenge},
  Year                     = {2011},

  Owner                    = {Songfan},
  Timestamp                = {2011.04.29}
}

@Article{Thakoor13,
  Title                    = {Structural Signatures for Passenger Vehicle Classification in Video},
  Author                   = {Thakoor, N.S. and Bhanu, B.},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2013},

  Month                    = {Dec},
  Number                   = {4},
  Pages                    = {1796-1805},
  Volume                   = {14},

  Doi                      = {10.1109/TITS.2013.2269137},
  ISSN                     = {1524-9050},
  Keywords                 = {image motion analysis;object recognition;vehicles;passenger vehicle classification;pattern recognition;road surface;structural signatures;vehicle side profile information;vehicle surfaces;video data set;Feature extraction;Image motion analysis;Object recognition;Pattern recognition;Vehicles;Image motion analysis;object recognition;vehicles}
}

@InProceedings{Thakoor05,
  Title                    = {Automatic video object shape extraction and its classification with camera in motion},
  Author                   = {Thakoor, N. and Jean Gao},
  Booktitle                = {IEEE International Conference on Image Processing},
  Year                     = {2005},
  Month                    = {Sept},
  Pages                    = {III-437-40},
  Volume                   = {3},

  Doi                      = {10.1109/ICIP.2005.1530422},
  Keywords                 = {cameras;feature extraction;hidden Markov models;image classification;object recognition;probability;HMM;ML description;automatic video object shape classification;automatic video object shape extraction;camera motion estimation;change detection approach;curvature features extraction;discrimination phase;forward region boundary;frame difference;frame difference information;generalized probabilistic descent method;hidden Markov model;moving camera;optical flow;shape classifier;uniform intensity regions;weighted likelihood discriminant;Cameras;Data mining;Feature extraction;Hidden Markov models;Image motion analysis;Maximum likelihood detection;Maximum likelihood estimation;Motion estimation;Robustness;Shape}
}

@Article{Tian_PAMI01,
  Title                    = {{R}ecognizing {A}ction {U}nits for {F}acial {E}xpression {A}nalysis},
  Author                   = {Tian, Y.-I. and Kanade, T. and Cohn, J.F.},
  Journal                  = {IEEE Trans. PAMI},
  Year                     = {2001},

  Abstract                 = {{M}ost automatic expression analysis systems attempt to recognize a small set of prototypic expressions, such as happiness, anger, surprise, and fear. {S}uch prototypic expressions, however, occur rather infrequently. {H}uman emotions and intentions are more often communicated by changes in one or a few discrete facial features. {I}n this paper, we develop an automatic face analysis ({AFA}) system to analyze facial expressions based on both permanent facial features (brows, eyes, mouth) and transient facial features (deepening of facial furrows) in a nearly frontal-view face image sequence. {T}he {AFA} system recognizes fine-grained changes in facial expression into action units ({AU}) of the {F}acial {A}ction {C}oding {S}ystem ({FACS}), instead of a few prototypic expressions. {M}ultistate face and facial component models are proposed for tracking and modeling the various facial features, including lips, eyes, brows, cheeks, and furrows. {D}uring tracking, detailed parametric descriptions of the facial features are extracted. {W}ith these parameters as the inputs, a group of action units (neutral expression, six upper face {AU} and 10 lower face {AU}) are recognized whether they occur alone or in combinations. {T}he system has achieved average recognition rates of 96.4 percent (95.4 percent if neutral expressions are excluded) for upper face {AU} and 96.7 percent (95.6 percent with neutral expressions excluded) for lower face {AU}. {T}he generalizability of the system has been tested by using independent image databases collected and {FACS}-coded for ground-truth by different research teams},
  ISSN                     = {0162-8828},
  Keywords                 = {AU;FACS;Facial Action Coding System;action unit recognition;anger;automatic facial expression analysis;cheeks;discrete facial features;eyebrows;eyes;facial furrows;fear;frontal-view face image sequence;happiness;image databases;lips;mouth;multistate face models;neutral expression;parametric descriptions;surprise;tracking;face recognition;feature extraction;image sequences;tracking;}
}

@Article{Tong_PAMI10,
  Title                    = {{A} {U}nified {P}robabilistic {F}ramework for {S}pontaneous {F}acial {A}ction {M}odeling and {U}nderstanding},
  Author                   = {Yan Tong and Jixu Chen and Qiang Ji},
  Journal                  = {IEEE Trans. PAMI},
  Year                     = {2010},

  Abstract                 = {{F}acial expression is a natural and powerful means of human communication. {R}ecognizing spontaneous facial actions, however, is very challenging due to subtle facial deformation, frequent head movements, and ambiguous and uncertain facial motion measurements. {B}ecause of these challenges, current research in facial expression recognition is limited to posed expressions and often in frontal view. {A} spontaneous facial expression is characterized by rigid head movements and nonrigid facial muscular movements. {M}ore importantly, it is the coherent and consistent spatiotemporal interactions among rigid and nonrigid facial motions that produce a meaningful facial expression. {R}ecognizing this fact, we introduce a unified probabilistic facial action model based on the dynamic {B}ayesian network ({DBN}) to simultaneously and coherently represent rigid and nonrigid facial motions, their spatiotemporal dependencies, and their image measurements. {A}dvanced machine learning methods are introduced to learn the model based on both training data and subjective prior knowledge. {G}iven the model and the measurements of facial motions, facial action recognition is accomplished through probabilistic inference by systematically integrating visual measurements with the facial action model. {E}xperiments show that compared to the state-of-the-art techniques, the proposed system yields significant improvements in recognizing both rigid and nonrigid facial motions, especially for spontaneous facial expressions.},
  ISSN                     = {0162-8828},
  Keywords                 = {dynamic Bayesian network;facial action recognition;facial deformation;facial expression;facial motion measurement;facial muscular movement;head movement;human communication;image measurement;machine learning;probabilistic facial action model;probabilistic inference;rigid facial motion;spatiotemporal interaction;spontaneous facial action modeling;spontaneous facial action understanding;visual measurement;belief networks;face recognition;image motion analysis;inference mechanisms;learning (artificial intelligence);Algorithms;Artificial Intelligence;Bayes Theorem;Biometric Identification;Databases, Factual;Face;Humans;Models, Statistical;}
}

@InProceedings{Uenohara95,
  Title                    = {Real-Time Vision Based Object Registration for Image Overlay},
  Author                   = {Michihiro Uenohara and Takeo Kanade},
  Booktitle                = {1995 Conference on Computer Vision,Virtual Reality and Robotics in Medicine},
  Year                     = {1995},
  Month                    = {April}
}

@InProceedings{FERA15,
  Title                    = {FERA 2015 - second Facial Expression Recognition and Analysis challenge},
  Author                   = {Valstar, M.F. and Almaev, T. and Girard, J.M. and McKeown, G. and Mehu, M. and Lijun Yin and Pantic, M. and Cohn, J.F.},
  Booktitle                = {IEEE International Conference and Workshops on Automatic Face and Gesture Recognition},
  Year                     = {2015}
}

@InProceedings{Valstar_FERA11,
  Title                    = {{T}he {F}irst {F}acial {E}xpression {R}ecognition and {A}nalysis {C}hallenge},
  Author                   = {Valstar, M.F. and Jiang, B. and M\'{e}hu, M. and Pantic, M. and Scherer, K},
  Booktitle                = {IEEE International Conference on Automatic Face Gesture Recognition and Workshops},
  Year                     = {2011},
  Pages                    = {921-926},

  Owner                    = {songfan},
  Timestamp                = {2011.03.01}
}

@Article{Valstar12,
  Title                    = {Meta-Analysis of the First Facial Expression Recognition Challenge},
  Author                   = {Valstar, M.F. and Mehu, M. and Bihan Jiang and Pantic, M. and Scherer, K.},
  Journal                  = {IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics},
  Year                     = {2012},

  Month                    = {Aug},
  Number                   = {4},
  Pages                    = {966-979},
  Volume                   = {42},

  Doi                      = {10.1109/TSMCB.2012.2200675},
  ISSN                     = {1083-4419},
  Keywords                 = {emotion recognition;face recognition;gesture recognition;AU;automatic facial expression recognition;challenge data;computer science;discrete emotion categories;discrete emotion states;evaluation protocol;facial action coding system action unit;facial expression databases;facial expressive imagery;first facial expression recognition challenge;gesture recognition;meta analysis;Databases;Emotion recognition;Face;Face recognition;Gold;Protocols;Training;Challenges;discrete emotion recognition;facial action coding system (FACS) analysis;facial expression analysis}
}

@Article{Valstar_SMCB12,
  Title                    = {Fully Automatic Recognition of the Temporal Phases of Facial Actions},
  Author                   = {Valstar, M.F. and Pantic, M.},
  Journal                  = {IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics},
  Year                     = {2012},

  Month                    = {Feb},
  Number                   = {1},
  Pages                    = {28-43},
  Volume                   = {42},

  Doi                      = {10.1109/TSMCB.2011.2163710},
  ISSN                     = {1083-4419},
  Keywords                 = {face recognition;hidden Markov models;image classification;image sequences;particle filtering (numerical methods);support vector machines;Gabor-feature-based boosted classifiers;GentleBoost;action units;decision making;facial behavior;facial fiducial point localization;facial muscle action temporal phase automatic recognition;facial point detector;factorized likelihoods;hidden Markov models;image sequence;particle filtering;support vector machines;temporal activation models;Detectors;Emotion recognition;Face;Face recognition;Gold;Hidden Markov models;Image sequences;Facial expression analysis;GentleBoost;particle filtering;spatiotemporal facial behavior analysis;support vector machine (SVM);Artificial Intelligence;Facial Expression;Humans;Image Interpretation, Computer-Assisted;Pattern Recognition, Automated;Photography;Subtraction Technique;Video Recording}
}

@InProceedings{Valstar_HCI07,
  Title                    = {{C}ombined {S}upport {V}ector {M}achines and {H}idden {M}arkov {M}odels for {M}odeling {F}acial {A}ction {T}emporal {D}ynamics},
  Author                   = {M. F. Valstar and M. Pantic},
  Booktitle                = {Proc. CVPR workshop on Human Computer Interaction},
  Year                     = {2007}
}

@Article{Vandewalle06,
  Title                    = {{A} {F}requency {D}omain {A}pproach to {R}egistration of {A}liased {I}mages with {A}pplication to {S}uper-{R}esolution},
  Author                   = {Patrick Vandewalle and Sabine S\"usstrunk and Martin Vetterli},
  Journal                  = {EURASIP Journal on Applied Signal Processing},
  Year                     = {2006}
}

@Misc{vlfeat,
  Title                    = {{VLFeat}: An Open and Portable Library of Computer Vision Algorithms},

  Author                   = {A. Vedaldi and B. Fulkerson},
  HowPublished             = {\url{http://www.vlfeat.org/}},
  Year                     = {2008},

  Owner                    = {Songfan},
  Timestamp                = {2014.05.17}
}

@Article{Viola_IJCV04,
  Title                    = {Robust Real-time Face Detection},
  Author                   = {Paul Viola and Michael Jones},
  Journal                  = {International Journal of Computer Vision},
  Year                     = {2004},
  Number                   = {2},
  Pages                    = {137-154},
  Volume                   = {57}
}

@InProceedings{Wagner2009,
  Title                    = {Towards a practical face recognition system: Robust registration and illumination by sparse representation},
  Author                   = {Wagner, A. and Wright, J. and Ganesh, A. and Zihan Zhou and Yi Ma},
  Booktitle                = {IEEE Conference on Computer Vision and Pattern Recognition},
  Year                     = {2009},
  Month                    = {June},
  Pages                    = {597-604},

  ISSN                     = {1063-6919}
}

@Article{Wang_TIP94,
  Title                    = {{R}epresenting {M}oving {I}mages with {L}ayers},
  Author                   = {Wang, J.Y.A. and Adelson, E.H.},
  Journal                  = {IEEE Trans. IP},
  Year                     = {1994},

  Doi                      = {10.1109/83.334981},
  ISSN                     = {1057-7149},
  Keywords                 = {alpha map;image coding;image representation;image sequences decomposition;intensity map;layered representation;motion analysis;motion estimation;motion segmentation;moving images;overlapping layers;transparency;velocity maps;video sequences;image coding;image segmentation;image sequences;motion estimation;video signals;}
}

@InProceedings{Wang_FG98,
  Title                    = {{E}xpression {R}ecognition from {T}ime-sequential {F}acial {I}mages by {U}se of {E}xpression {C}hange {M}odel},
  Author                   = {Mei Wang and Iwai, Y. and Yachida, M.},
  Booktitle                = {Proc. FG},
  Year                     = {1998},

  Abstract                 = {{I}n order to construct a better human interface, recognition of facial expressions by means of computer is an important technology. {A}n approach is proposed to recognize the degree of facial expression change from time-sequential images. {T}he facial features in an input image sequence are tracked by using labeled graph matching with weighted links. {T}o represent the relationship between the motion of features and change of expression, we construct expression change models by using {B}-spline curves. {B}y making a comparison between the trajectory of features and the expression change models, the facial expression in the input image sequence can be recognized. {N}ot only the category, but also the degree of facial expression change can be determined. {F}urthermore, the obtained expressional information is then fed back to guide the tracking in the next frame},
  Keywords                 = {B-spline curves;expression change model;expression recognition;expressional information;facial expression change;facial features;feature trajectory;human interface;input image sequence;labeled graph matching;time-sequential facial images;tracking;weighted links;face recognition;image sequences;splines (mathematics);user interfaces;}
}

@InProceedings{Wu_CVPRW10,
  Title                    = {{F}acial {E}xpression {R}ecognition {U}sing {G}abor {M}otion {E}nergy {F}ilters},
  Author                   = {Tingfan Wu and Bartlett, M.S. and Movellan, J.R.},
  Booktitle                = {Proc. CVPRW},
  Year                     = {2010},

  Doi                      = {10.1109/CVPRW.2010.5543267},
  Keywords                 = {Cohn-Kanade expression dataset;GME;Gabor motion energy filters;biologically inspired representation;computer vision;dynamic facial expressions;face expression analysis;face recognition;facial expression recognition;primary visual cortex;spatial Gabor energy filters;spatio-temporal Gabor filters;video sequences;visual signal;Gabor filters;computer vision;face recognition;image motion analysis;image representation;image sequences;spatial filters;video signal processing;}
}

@InProceedings{Wu_FERA11,
  Title                    = {{A}ction {U}nit {R}ecognition {T}ransfer across {D}atasets},
  Author                   = {Tingfan Wu and Butko, N.J. and Ruvolo, P. and Whitehill, J. and Bartlett, M.S. and Movellan, J.R.},
  Booktitle                = {Proc. FG Workshop on FERA Challenge},
  Year                     = {2011},

  Doi                      = {10.1109/FG.2011.5771369},
  Keywords                 = {CERT;FERA dataset;action unit recognition;baseline method;computer expression recognition toolbox;idiosyncratic;spontaneous facial expression;transfer across dataset;emotion recognition;face recognition;}
}

@Article{Wu_TIP06,
  Title                    = {Improvement of Color Video Demosaicking in Temporal Domain},
  Author                   = {Xiaolin Wu and Zhang, D.},
  Journal                  = {IEEE Trans. Image Process.},
  Year                     = {2006},

  Month                    = {Oct},
  Number                   = {10},
  Pages                    = {3138-3151},
  Volume                   = {15},

  ISSN                     = {1057-7149}
}

@InProceedings{Xiong13,
  Title                    = {Supervised Descent Method and Its Applications to Face Alignment},
  Author                   = {Xuehan Xiong and De la Torre, F.},
  Booktitle                = {Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on},
  Year                     = {2013},
  Month                    = {June},
  Pages                    = {532-539},

  Doi                      = {10.1109/CVPR.2013.75},
  ISSN                     = {1063-6919},
  Keywords                 = {computer vision;face recognition;feature extraction;gradient methods;learning (artificial intelligence);least squares approximations;nonlinear programming;2nd order descent methods;NLS function;SDM;computer vision problems;face alignment;facial feature detection;general smooth function;learned descent directions;nonlinear least square function;nonlinear optimization method;numerical approximations;supervised descent method;Face;Jacobian matrices;Newton method;Shape;Training;Training data;Vectors;face alignment;facial feature tracking;non-linear least squares;supervised descent method}
}

@InProceedings{Xue_CVPR13,
  Title                    = {Learning without Human Scores for Blind Image Quality Assessment},
  Author                   = {Wufeng Xue and Lei Zhang and Xuanqin Mou},
  Booktitle                = {IEEE Conference on Computer Vision and Pattern Recognition},
  Year                     = {2013},
  Month                    = {June},
  Pages                    = {995-1002},

  Doi                      = {10.1109/CVPR.2013.133},
  ISSN                     = {1063-6919},
  Keywords                 = {image processing;learning (artificial intelligence);pattern clustering;regression analysis;BIQA methods;QAC method;centroids;general purpose blind image quality assessment;human scored images;human subjective scores;image local features;image local quality map;image processing;machine learning;overlapped patches;percentile pooling strategy;perceptual quality score;quality-aware clustering method;regression analysis;training samples;Databases;Feature extraction;Image coding;Image quality;Linearity;Training;Transform coding;bind image quality assessment;clustering;qualiyt aware}
}

@Article{Yacoob_PAMI96,
  Title                    = {{R}ecognizing {H}uman {F}acial {E}xpressions from {L}ong {I}mage {S}equences {U}sing {O}ptical {F}low},
  Author                   = {Yacoob, Y. and Davis, L.S.},
  Journal                  = {IEEE Trans. PAMI},
  Year                     = {1996},

  Doi                      = {10.1109/34.506414},
  ISSN                     = {0162-8828},
  Keywords                 = {facial dynamics;human facial expression recognition;image sequences;motion analysis;optical flow;symbolic representation;face recognition;image recognition;image representation;image sequences;motion estimation;}
}

@InProceedings{Yang_CVPR07,
  Title                    = {{B}oosting {C}oded {D}ynamic {F}eatures for {F}acial {A}ction {U}nits and {F}acial {E}xpression {R}ecognition},
  Author                   = {Peng Yang and Qingshan Liu and Metaxas, D.N.},
  Booktitle                = {Proc. CVPR},
  Year                     = {2007},

  Abstract                 = {{I}t is well known that how to extract dynamical features is a key issue for video based face analysis. {I}n this paper, we present a novel approach of facial action units ({AU}) and expression recognition based on coded dynamical features. {I}n order to capture the dynamical characteristics of facial events, we design the dynamical haar-like features to represent the temporal variations of facial events. {I}nspired by the binary pattern coding, we further encode the dynamic haar-like features into binary pattern features, which are useful to construct weak classifiers for boosting learning. {F}inally the {A}daboost is performed to learn a set of discriminating coded dynamic features for facial active units and expression recognition. {E}xperiments on the {CMU} expression database and our own facial {AU} database show its encouraging performance.},
  Doi                      = {10.1109/CVPR.2007.383059},
  Keywords                 = {binary pattern coding;coded dynamic features;facial action units;facial expression recognition;video based face analysis;binary codes;emotion recognition;face recognition;video signal processing;}
}

@InProceedings{Yang_FG13,
  Title                    = {Improving action units recognition using dense flow-based face registration in video},
  Author                   = {Songfan Yang and Le An and Bhanu, B. and Thakoor, N.},
  Booktitle                = {IEEE International Conference on Automatic Face and Gesture Recognition},
  Year                     = {2013},
  Month                    = {April},
  Pages                    = {1-8},

  Doi                      = {10.1109/FG.2013.6553790},
  Keywords                 = {affine transforms;face recognition;image motion analysis;image registration;image sequences;muscle;pose estimation;video streaming;SIFT method;SOFAIT;action unit recognition;affine transformation;appearance change;arbitrary face transformation;automatic video-based face registration architecture;canonical pose;dense SIFT-flow-based affine warping problem;dense flow-based face registration;dense optical flow;face alignment;facial expression recognition;facial muscle dynamics;generic reference face;head pose;nonrigid facial appearance;nonrigid muscle motion;optical flow affine image transform;optical-flow-based affine warping problem;person independent face model;real-time algorithm;real-world streaming video;realistic streaming data;spontaneous expression;temporal smoothness;Communities;Estimation;Optical imaging;Optical propagation;Visualization}
}

@Article{Yang_SMCB12,
  Title                    = {Understanding Discrete Facial Expressions in Video Using an Emotion Avatar Image},
  Author                   = {Songfan Yang and Bhanu, B.},
  Journal                  = {IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics},
  Year                     = {2012},

  Month                    = {Aug},
  Number                   = {4},
  Pages                    = {980-992},
  Volume                   = {42},

  Doi                      = {10.1109/TSMCB.2012.2192269},
  ISSN                     = {1083-4419},
  Keywords                 = {avatars;emotion recognition;face recognition;image classification;support vector machines;video signal processing;EAI representation;GEMEP-FERA;aggregate dynamic information;appearance-based information;associated reference image;avatar reference;discrete facial expression analysis;emotion avatar image;emotion inference;geometry-based information;image-based representation;linear support vector machine classifier;local binary patterns;local phase quantization;video frames;video-based facial expression recognition techniques;Avatars;Face;Face recognition;Feature extraction;Geometry;Image recognition;Avatar reference;Scale-invariant feature transform (SIFT) flow;emotion avatar image (EAI);face registration;person-independent emotion recognition}
}

@InProceedings{Yang_FERA11,
  Title                    = {{F}acial {E}xpression {R}ecognition {U}sing {E}motion {A}vatar {I}mage},
  Author                   = {Songfan Yang and Bir Bhanu},
  Booktitle                = {Proc. FG Workshop on FERA Challenge},
  Year                     = {2011},

  Owner                    = {songfan},
  Timestamp                = {2011.05.23}
}

@Article{Yu_PRL06,
  Title                    = {{E}volutionary {F}eature {S}ynthesis for {F}acial {E}xpression {R}ecognition},
  Author                   = {Jiangang Yu and Bir Bhanu},
  Journal                  = {Pattern Recognition Letters},
  Year                     = {2006},

  Keywords                 = {genetic algorithms, genetic programming, Feature learning, Gabor filters}
}

@InProceedings{Zafar09,
  Title                    = {Localised contourlet features in vehicle make and model recognition},
  Author                   = {Iffat Zafar and Eran A. Edirisinghe and B. Serpil Acar},
  Booktitle                = {Image Processing: Machine Vision Applications II, Proc. of SPIE-IS\&T Electronic Imaging},
  Year                     = {2009}
}

@InProceedings{LGBP,
  Title                    = {Local Gabor Binary Pattern Histogram Sequence (LGBPHS): A Novel Non-Statistical Model for Face Representation and Recognition},
  Author                   = {Zhang, Wenchao and Shan, Shiguang and Gao, Wen and Chen, Xilin and Zhang, Hongming},
  Booktitle                = {IEEE International Conference on Computer Vision},
  Year                     = {2005}
}

@Article{BP4D,
  Title                    = {BP4D-Spontaneous: a high-resolution spontaneous 3D dynamic facial expression database },
  Author                   = {Xing Zhang and Lijun Yin and Jeffrey F. Cohn and Shaun Canavan and Michael Reale and Andy Horowitz and Peng Liu and Jeffrey M. Girard},
  Journal                  = {Image and Vision Computing },
  Year                     = {2014},
  Number                   = {10},
  Pages                    = {692 - 706},
  Volume                   = {32},

  ISSN                     = {0262-8856}
}

@InProceedings{Zhang_ACCV10,
  Title                    = {{TILT}: {T}ransform {I}nvariant {L}ow-rank {T}extures},
  Author                   = {Z. Zhang and X. Liang and A. Ganesh and Y. Ma},
  Booktitle                = {Proc. ACCV},
  Year                     = {2010}
}

@Article{Zhao_PAMI07,
  Title                    = {Dynamic Texture Recognition Using Local Binary Patterns with an Application to Facial Expressions},
  Author                   = {Guoying Zhao and Pietik\"ainen, M.},
  Journal                  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  Year                     = {2007},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {915-928},
  Volume                   = {29},

  Doi                      = {10.1109/TPAMI.2007.1110},
  ISSN                     = {0162-8828},
  Keywords                 = {face recognition;image texture;dynamic texture recognition;facial expressions;facial image analysis;volume local binary patterns;Face recognition;Gray-scale;Image motion analysis;Image recognition;Image texture analysis;Motion analysis;Pattern analysis;Pattern recognition;Robustness;Spatial databases;Temporal texture;facial expression;facial image analysis;local binary pattern.;motion;Algorithms;Artificial Intelligence;Biometry;Computer Security;Face;Facial Expression;Humans;Image Enhancement;Image Interpretation, Computer-Assisted;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity;Signal Processing, Computer-Assisted;Video Recording}
}

@InProceedings{Zhou_CVPR10,
  Title                    = {{U}nsupervised {D}iscovery of {F}acial {E}vents},
  Author                   = {Feng Zhou and De la Torre, F. and Cohn, J.F.},
  Booktitle                = {Proc. CVPR},
  Year                     = {2010},

  Abstract                 = {{A}utomatic facial image analysis has been a long standing research problem in computer vision. {A} key component in facial image analysis, largely conditioning the success of subsequent algorithms (e.g. facial expression recognition), is to define a vocabulary of possible dynamic facial events. {T}o date, that vocabulary has come from the anatomically-based {F}acial {A}ction {C}oding {S}ystem ({FACS}) or more subjective approaches (i.e. emotion-specified expressions). {T}he aim of this paper is to discover facial events directly from video of naturally occurring facial behavior, without recourse to {FACS} or other labeling schemes. {T}o discover facial events, we propose a temporal clustering algorithm, {A}ligned {C}luster {A}nalysis ({ACA}), and a multi-subject correspondence algorithm for matching expressions. {W}e use a variety of video sources: posed facial behavior ({C}ohn-{K}anade database), unscripted facial behavior ({RU}-{FACS} database) and some video in infants. {A}ccuracy of (unsupervised) {ACA} approached that of a supervised version, achieved moderate intersystem agreement with {FACS}, and proved informative as a visualization/summarization tool.},
  Doi                      = {10.1109/CVPR.2010.5539966},
  ISSN                     = {1063-6919},
  Keywords                 = {aligned cluster analysis;automatic facial image analysis;computer vision;facial action coding system;facial events;facial expression recognition;labeling schemes;multi-subject correspondence algorithm;unsupervised discovery;computer vision;emotion recognition;face recognition;image matching;video coding;}
}

@InProceedings{Zhu_ICCV09,
  Title                    = {{U}nsupervised {F}ace {A}lignment by {R}obust {N}onrigid {M}apping},
  Author                   = {Jianke Zhu and Luc Van Gool and Hoi, S.C.H.},
  Booktitle                = {Proc. ICCV},
  Year                     = {2009},

  ISSN                     = {1550-5499},
  Keywords                 = {Lucas-Kanade image registration;affine transformations;robust nonrigid mapping;robust optimization scheme;unsupervised facial image alignment;face recognition;feature extraction;image registration;transforms;}
}

@Article{Zhu_PAMI09,
  Title                    = {{A} {F}ast 2{D} {S}hape {R}ecovery {A}pproach by {F}using {F}eatures and {A}ppearance},
  Author                   = {Zhu, J. and Lyu, M.R. and Huang, T.S.},
  Journal                  = {IEEE Trans. PAMI},
  Year                     = {2009},

  ISSN                     = {0162-8828},
  Keywords                 = {2D shape recovery;Lucas-Kanade algorithm;appearance fusion;computer vision;feature fusion;feature-based nonrigid surface detection;finite Newton optimization scheme;least squares problem;unconstrained quadratic optimization problem;computer graphics;image fusion;least squares approximations;object detection;quadratic programming;Algorithms;Artificial Intelligence;Image Enhancement;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Models, Biological;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity;Subtraction Technique;}
}

@InProceedings{Zhu_CVPR12,
  Title                    = {Face detection, pose estimation, and landmark localization in the wild},
  Author                   = {Xiangxin Zhu and Ramanan, D.},
  Booktitle                = {IEEE Conference on Computer Vision and Pattern Recognition},
  Year                     = {2012},
  Month                    = {June},
  Pages                    = {2879-2886},

  Doi                      = {10.1109/CVPR.2012.6248014},
  ISSN                     = {1063-6919},
  Keywords                 = {computer vision;face recognition;object detection;pose estimation;trees (mathematics);Google Picasa;computer vision;face detection;face.com;global elastic deformation;in-the-wild annotated dataset;landmark estimation;landmark localization;pose estimation;topological changes;tree-structured models;Computational modeling;Detectors;Estimation;Face;Face detection;Google;Shape}
}

@Article{Zitova_IVC03,
  Title                    = {{I}mage {R}egistration {M}ethods: {A} {S}urvey},
  Author                   = {Barbara Zitov\'a and Jan Flusser},
  Journal                  = {Image and Vision Computing},
  Year                     = {2003}
}

@InProceedings{Zomet_CVPR01,
  Title                    = {{R}obust {S}uper-resolution},
  Author                   = {Zomet, A. and Rav-Acha, A. and Peleg, S.},
  Booktitle                = {Proc. CVPR},
  Year                     = {2001}
}

@Manual{FERA11,
  Title                    = {{FERA}2011: {F}acial {E}xpression {R}ecognition and {A}nalysis {C}hallenge},
  Note                     = {http://sspnet.eu/fera2011/},

  Owner                    = {songfan},
  Timestamp                = {2011.02.28}
}


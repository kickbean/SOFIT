% This file was created with JabRef 2.6.
% Encoding: Cp1252

@ARTICLE{Ambadar05,
  author = {Zara Ambadar and J. Schooler and Jeffrey Cohn},
  title = {{D}eciphering the {E}nigmatic {F}ace: the {I}mportance of {F}acial
	{D}ynamics to {I}nterpreting {S}ubtle {F}acial {E}xpressions},
  journal = {Psychological Science},
  year = {2005}
}

@INPROCEEDINGS{Baker_ICCV07,
  author = {Baker, S. and Roth, S. and Scharstein, D. and Black, M.J. and Lewis,
	J.P. and Szeliski, R.},
  title = {A Database and Evaluation Methodology for Optical Flow},
  booktitle = {IEEE International Conference on Computer Vision},
  year = {2007},
  pages = {1-8},
  month = {Oct}
}

@INPROCEEDINGS{Tadas_FERA15,
  author = {Baltrusaitis, T. and Mahmoud, M. and Robinson, P.},
  title = {Cross-dataset learning and person-specific normalisation for automatic
	Action Unit detection},
  booktitle = {IEEE International Conference and Workshops on Automatic Face and
	Gesture Recognition},
  year = {2015}
}

@INPROCEEDINGS{Tadas_FERA11,
  author = {Tadas Baltrusaitis and Daniel McDuff and Ntombikayise Banda and Marwa
	Mahmoud and Rana el Kaliouby and Peter Robinson and Rosalind Picard},
  title = {{R}eal-time {I}nference of {M}ental {S}tates from {F}acial {E}xpressions
	and {U}pper {B}ody {G}estures},
  booktitle = {Proc. FG Workshop on FERA Challenge},
  year = {2011},
  owner = {Songfan},
  timestamp = {2011.05.01}
}

@INCOLLECTION{Tadas_ECCV14,
  author = {Baltru{\v{s}}aitis, Tadas and Robinson, Peter and Morency, Louis-Philippe},
  title = {Continuous Conditional Neural Fields for Structured Regression},
  booktitle = {European Conference on Computer Vision},
  year = {2014}
}

@INPROCEEDINGS{Bartlett_FG06,
  author = {Bartlett, M.S. and Littlewort, G. and Frank, M. and Lainscsek, C.
	and Fasel, I. and Movellan, J.},
  title = {{F}ully {A}utomatic {F}acial {A}ction {R}ecognition in {S}pontaneous
	{B}ehavior},
  booktitle = {Proc. FG},
  year = {2006},
  keywords = {AdaBoost classifiers;facial action coding system;fully automatic facial
	action recognition;spontaneous behavior;support vector machines;emotion
	recognition;face recognition;support vector machines;}
}

@ARTICLE{Bassili79,
  author = {Bassili, John N.},
  title = {{E}motion {R}ecognition: {T}he {R}ole of {F}acial {M}ovement and
	the {R}elative {I}mportance of {U}pper and {L}ower {A}reas of the
	{F}ace},
  journal = {Personality and Social Psychology},
  year = {1979},
  owner = {songfan},
  timestamp = {2011.02.28}
}

@ARTICLE{Boker09,
  author = {Steven M. Boker and Jeffrey F. Cohn and Iain Matthews and Timothy
	R. Brick},
  title = {{E}ffects of {D}amping {H}ead {M}ovement and {F}acial {E}xpression
	in {D}yadic {C}onversation {U}sing {R}eal-time {F}acial {E}xpression
	{T}racking and {S}ynthesized {A}vatars},
  journal = {Philosophical Transactions B of the Royal Society},
  year = {2009}
}

@ARTICLE{opencv,
  author = {Bradski, G.},
  title = {{The OpenCV Library}},
  journal = {Dr. Dobb's Journal of Software Tools},
  year = {2000},
  citeulike-article-id = {2236121},
  keywords = {bibtex-import},
  posted-at = {2008-01-15 19:21:54},
  priority = {4}
}

@ARTICLE{Candes_JACM11,
  author = {E. Cand\`{e}s and X. Li and Y. Ma and J. Wright},
  title = {{R}obust {P}rincipal {C}omponent {A}nalysis?},
  journal = {Journal of the {ACM}},
  year = {2011}
}

@ARTICLE{Caspi_PAMI02,
  author = {Caspi, Y. and Irani, M.},
  title = {Spatio-temporal alignment of sequences},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {2002},
  volume = {24},
  pages = {1409-1424},
  number = {11},
  month = {Nov},
  issn = {0162-8828},
  keywords = {image matching;image motion analysis;image sequences;video cameras;dynamic
	scene;image alignment;image frames;image motion analysis;intercamera
	external parameters;scene illumination;sequence-to-sequence alignment;spatial
	cues;spatio-temporal sequence alignment;temporal cues;video cameras;video
	sequences;Cameras;Helium;Image analysis;Image resolution;Infrared
	sensors;Layout;Lighting;Pixel;Spatial resolution;Video sequences}
}

@ARTICLE{SVMlib,
  author = {Chang, Chih-Chung and Lin, Chih-Jen},
  title = {{LIBSVM}: A library for support vector machines},
  journal = {ACM Transactions on Intelligent Systems and Technology},
  year = {2011},
  volume = {2},
  pages = {1-27},
  issue = {3}
}

@INPROCEEDINGS{Chew_FERA11,
  author = {Sien Wei Chew and Patrick J. Lucey and Simon Lucey and Jason Saragih
	and Jeffrey Cohn and Sridha Sridharan},
  title = {{P}erson-independent {F}acial {E}xpression {D}etection {U}sing {C}onstrained
	{L}ocal {M}odels},
  booktitle = {Proc. FG Workshop on FERA Challenge},
  year = {2011},
  abstract = {{I}n automatic facial expression detection, very accurate registration
	is desired which can be achieved via a deformable model approach
	where a dense mesh of 60-70 points on the face is used, such as an
	active appearance model ({AAM}). {H}owever, for applications where
	manually labeling frames is prohibitive, {AAM}s do not work well
	as they do not generalize well to unseen subjects. {A}s such, a more
	coarse approach is taken for person-independent facial expression
	detection, where just a couple of key features (such as face and
	eyes) are tracked using a {V}iola-{J}ones type approach. {T}he tracked
	image is normally post-processed to encode for shift and illumination
	invariance using a linear bank of filters. {R}ecently, it was shown
	that this preprocessing step is of no benefit when close to ideal
	registration has been obtained. {I}n this paper, we present a system
	based on the {C}onstrained {L}ocal {M}odel ({CLM}) which is a generic
	or person-independent face alignment algorithm which gains high accuracy.
	{W}e show these results against the {LBP} feature extraction on the
	{CK}+ and {GEMEP} datasets.},
  keywords = {facial expression recognition, constrained local models, local binary
	patterns, fera2011 challenge}
}

@ARTICLE{Cootes_PAMI01,
  author = {Cootes, T.F. and Edwards, G.J. and Taylor, C.J.},
  title = {{A}ctive {A}ppearance {M}odels},
  journal = {IEEE Trans. PAMI},
  year = {2001},
  abstract = {{W}e describe a new method of matching statistical models of appearance
	to images. {A} set of model parameters control modes of shape and
	gray-level variation learned from a training set. {W}e construct
	an efficient iterative matching algorithm by learning the relationship
	between perturbations in the model parameters and the induced image
	errors},
  issn = {0162-8828},
  keywords = {active appearance models;deformable template;gray-level variation;iterative
	method;learning;model matching;optimisation;shape matching;statistical
	models;texture matching;image matching;image texture;iterative methods;learning
	(artificial intelligence);optimisation;statistical analysis;}
}

@INPROCEEDINGS{Cox_CVPR08,
  author = {Cox, M. and Sridharan, S. and Lucey, S. and Cohn, J.},
  title = {{L}east {S}quares {C}ongealing for {U}nsupervised {A}lignment of
	{I}mages},
  booktitle = {Proc. CVPR},
  year = {2008},
  issn = {1063-6919},
  keywords = {image alignment;least squares congealing;unsupervised learning;warp
	parameter update estimation;image processing;least squares approximations;unsupervised
	learning;}
}

@INPROCEEDINGS{Cruz_AVEC11,
  author = {Albert Cruz and Bir Bhanu and Songfan Yang},
  title = {{A} {P}sychologically-{I}nspired {M}atch-{S}core {F}usion {M}odel
	for {V}ideo-{B}ased {F}acial {E}xpression {R}ecognition},
  booktitle = {Proc. ACII workshop on AVEC},
  year = {2011}
}

@INPROCEEDINGS{Dahmane_FERA11,
  author = {Mohamed Dahmane and Jean Meunier},
  title = {{E}motion {R}ecognition using {D}ynamic {G}rid-based {H}o{G} {F}eatures},
  booktitle = {Proc. FG Workshop on FERA Challenge},
  year = {2011},
  owner = {Songfan},
  timestamp = {2011.05.01}
}

@INPROCEEDINGS{Dalal_CVPR05,
  author = {Dalal, N. and Triggs, B.},
  title = {{H}istograms of {O}riented {G}radients for {H}uman {D}etection},
  booktitle = {Proc. CVPR},
  year = {2005}
}

@INPROCEEDINGS{intraface,
  author = {De la Torre, F. and Wen-Sheng Chu and Xuehan Xiong and Vicente, F.
	and Xiaoyu Ding and Cohn, J.},
  title = {{I}ntraFace},
  booktitle = {IEEE International Conference and Workshops on Automatic Face and
	Gesture Recognition},
  year = {2015}
}

@INPROCEEDINGS{Dhall_FERA11,
  author = {Abhinav Dhall and Akshay Asthana and Roland Goecke and Tom Gedeon},
  title = {{E}motion {R}ecognition {U}sing \uppercase{PHOG} and \uppercase{LPQ}
	features},
  booktitle = {Proc. FG Workshop on FERA Challenge},
  year = {2011},
  owner = {Songfan},
  timestamp = {2011.05.01}
}

@ARTICLE{Donato_PAMI99,
  author = {Donato, G. and Bartlett, M.S. and Hager, J.C. and Ekman, P. and Sejnowski,
	T.J.},
  title = {{C}lassifying {F}acial {A}ctions},
  journal = {IEEE Trans. PAMI},
  year = {1999},
  abstract = {{T}he facial action coding system ({FAGS}) is an objective method
	for quantifying facial movement in terms of component actions. {T}his
	paper explores and compares techniques for automatically recognizing
	facial actions in sequences of images. {T}hese techniques include:
	analysis of facial motion through estimation of optical flow; holistic
	spatial analysis, such as principal component analysis, independent
	component analysis, local feature analysis, and linear discriminant
	analysis; and methods based on the outputs of local filters, such
	as {G}abor wavelet representations and local principal components.
	{P}erformance of these systems is compared to naive and expert human
	subjects. {B}est performances were obtained using the {G}abor wavelet
	representation and the independent component representation, both
	of which achieved 96 percent accuracy for classifying 12 facial actions
	of the upper and lower face. {T}he results provide converging evidence
	for the importance of using local filters, high spatial frequencies,
	and statistical independence for classifying facial actions},
  issn = {0162-8828},
  keywords = {Gabor wavelet;computer vision;facial action coding system;facial expression
	recognition;image sequences;independent component analysis;linear
	discriminant analysis;local feature analysis;motion estimation;optical
	flow;principal component analysis;computer vision;face recognition;image
	sequences;motion estimation;principal component analysis;wavelet
	transforms;}
}

@ARTICLE{Beltfast,
  author = {Ellen Douglas-Cowie and Nick Campbell and Roddy Cowie and Peter Roach},
  title = {{E}motional {S}peech: {T}owards {A} {N}ew {G}eneration of {D}atabases},
  journal = {Speech Communication},
  year = {2003}
}

@INPROCEEDINGS{Douglas-Cowie00,
  author = {Ellen Douglas-Cowie and Roddy Cowie and Marc Schröder},
  title = {{A} {N}ew {E}motion {D}atabase: {C}onsiderations, {S}ources and {S}cope},
  booktitle = {Proc. the ISCA Workshop on Speech and Emotion},
  year = {2000}
}

@BOOK{Ekman78,
  title = {{F}acial {A}ction {C}oding {S}ystem: {A} {T}echnique for the {M}easurement
	of {F}acial {M}ovement},
  publisher = {Consulting Psychologists Press},
  year = {1978},
  author = {Ekman, P. and Friesen, W.},
  journal = {Consulting Psychologists Press},
  owner = {Songfan},
  timestamp = {2012.02.20}
}

@BOOK{Ekman2005,
  title = {What the Face Reveals: Basic and Applied Studies of Spontaneous Expression
	Using the Facial Action Coding System (FACS)},
  publisher = {Oxford University Press},
  year = {2005},
  author = {Paul Ekman and Erika Rosenberg},
  owner = {Songfan},
  timestamp = {2014.04.04}
}

@INPROCEEDINGS{Kaliouby_SMC04,
  author = {El Kaliouby, R. and Robinson, P.},
  title = {{M}ind {R}eading {M}achines: {A}utomated {I}nference of {C}ognitive
	{M}ental {S}tates from {V}ideo},
  booktitle = {Proc. SMC},
  year = {2004},
  abstract = {{M}ind reading encompasses our ability to attribute mental states
	to others, and is essential for operating in a complex social environment.
	{T}he goal in building mind reading machines is to enable computer
	technologies to understand and react to people's emotions and mental
	states. {T}his paper describes a system, for the automated inference
	of cognitive mental states from observed facial expressions and head
	gestures in video. {T}he system is based on a multilevel dynamic
	{B}ayesian network classifier which models cognitive mental states
	as a number of interacting facial and head displays. {E}xperimental
	results yield an average recognition rate of 87.4% for 6 mental states
	groups: agreement, concentrating, disagreement, interested, thinking
	and unsure. {R}eal time performance, unobtrusiveness and lack of
	preprocessing make our system particularly suitable for user-independent
	human computer interaction},
  issn = {1062-922X},
  keywords = {automated inference;cognitive mental states;complex social environment;head
	gestures;mind reading machines;multilevel dynamic Bayesian network
	classifier;observed facial expressions;user-independent human computer
	interaction;Bayes methods;belief networks;cognition;emotion recognition;human
	computer interaction;inference mechanisms;}
}

@ARTICLE{Essa_PAMI97,
  author = {Essa, I.A. and Pentland, A.P.},
  title = {{C}oding, {A}nalysis, {I}nterpretation, and {R}ecognition of {F}acial
	{E}xpressions},
  journal = {IEEE Trans. PAMI},
  year = {1997},
  abstract = {{W}e describe a computer vision system for observing facial motion
	by using an optimal estimation optical flow method coupled with geometric,
	physical and motion-based dynamic models describing the facial structure.
	{O}ur method produces a reliable parametric representation of the
	face's independent muscle action groups, as well as an accurate estimate
	of facial motion. {P}revious efforts at analysis of facial expression
	have been based on the facial action coding system ({FACS}), a representation
	developed in order to allow human psychologists to code expression
	from static pictures. {T}o avoid use of this heuristic coding scheme,
	we have used our computer vision system to probabilistically characterize
	facial motion and muscle activation in an experimental population,
	thus deriving a new, more accurate, representation of human facial
	expressions that we call {FACS}+. {F}inally, we show how this method
	can be used for coding, analysis, interpretation, and recognition
	of facial expressions},
  issn = {0162-8828},
  keywords = {FACS;FACS+;computer vision system;facial action coding system;facial
	expression recognition;facial motion;facial structure;image analysis;image
	coding;image interpretation;independent muscle action groups;muscle
	activation;optimal estimation optical flow method;probabilistic characterization;face
	recognition;image coding;image sequences;motion estimation;}
}

@ARTICLE{LIBLINEAR,
  author = {Rong-En Fan and Kai-Wei Chang and Cho-Jui Hsieh and Xiang-Rui Wang
	and Chih-Jen Lin},
  title = {{LIBLINEAR}: {A} {L}ibrary for {L}arge {L}inear {C}lassification},
  journal = {JMLR},
  year = {2008}
}

@ARTICLE{Fang_PR14,
  author = {Hui Fang and Neil Mac Parthaláin and Andrew J. Aubrey and Gary K.L.
	Tam and Rita Borgo and Paul L. Rosin and Philip W. Grant and David
	Marshall and Min Chen},
  title = {Facial expression recognition in dynamic sequences: An integrated
	approach },
  journal = {Pattern Recognition },
  year = {2014},
  volume = {47},
  pages = {1271 - 1281},
  number = {3}
}

@ARTICLE{Freeman_IJCV00,
  author = {Freeman, William T. and Pasztor, Egon C. and Carmichael, Owen T.},
  title = {{L}earning {L}ow-{L}evel {V}ision},
  journal = {IJCV},
  year = {2000},
  issue = {1}
}

@INPROCEEDINGS{Gehrig_CVPRW11,
  author = {Gehrig, T. and Ekenel, H.K.},
  title = {{A} {C}ommon {F}ramework for {R}eal-time {E}motion {R}ecognition
	and {F}acial {A}ction {U}nit {D}etection},
  booktitle = {Proc. CVPR Workshops},
  year = {2011},
  issn = {2160-7508},
  keywords = {FG 2011 facial expression analysis;FG 2011 facial expression recognition;discrete
	cosine transform;facial action unit detection;local appearance-based
	face representation approach;real-time emotion recognition;support
	vector machine classifiers;discrete cosine transforms;emotion recognition;face
	recognition;image classification;support vector machines;}
}

@ARTICLE{Han_PAMI06,
  author = {Han, J. and Bhanu, B.},
  title = {{I}ndividual {R}ecognition {U}sing {G}ait {E}nergy {I}mage},
  journal = {IEEE Trans. PAMI},
  year = {2006},
  abstract = {{I}n this paper, we propose a new spatio-temporal gait representation,
	called {G}ait {E}nergy {I}mage ({GEI}), to characterize human walking
	properties for individual recognition by gait. {T}o address the problem
	of the lack of training templates, we also propose a novel approach
	for human recognition by combining statistical gait features from
	real and synthetic templates. {W}e directly compute the real templates
	from training silhouette sequences, while we generate the synthetic
	templates from training sequences by simulating silhouette distortion.
	{W}e use a statistical approach for learning effective features from
	real and synthetic templates. {W}e compare the proposed {GEI}-based
	gait recognition approach with other gait recognition approaches
	on {USF} {H}uman{ID} {D}atabase. {E}xperimental results show that
	the proposed {GEI} is an effective and efficient gait representation
	for individual recognition, and the proposed approach achieves highly
	competitive performance with respect to the published gait recognition
	approaches},
  doi = {10.1109/TPAMI.2006.38},
  issn = {0162-8828},
  keywords = {distortion analysis;feature fusion;gait energy image;human recognition;human
	walking properties;individual recognition;spatio-temporal gait representation;statistical
	gait features;gait analysis;image recognition;}
}

@INPROCEEDINGS{Hu_IVC06,
  author = {Changbo Hu and Ya Chang and Feris, R. and Turk, M.},
  title = {{M}anifold {B}ased {A}nalysis of {F}acial {E}xpression},
  booktitle = {Image and Vision Computing},
  year = {2006}
}

@INPROCEEDINGS{Huang_ICCV07,
  author = {Huang, G.B. and Jain, V. and Learned-Miller, E.},
  title = {{U}nsupervised {J}oint {A}lignment of {C}omplex {I}mages},
  booktitle = {Proc. ICCV},
  year = {2007},
  issn = {1550-5499},
  keywords = {canonical pose recognition;class-specialized learning algorithm;face
	detector;face recognition;image recognition algorithm;unsupervised
	joint alignment;face recognition;pose estimation;unsupervised learning;}
}

@BOOK{Huber81,
  title = {{R}obust {S}tatistics},
  publisher = {John Wiley \& Sons, Inc.},
  year = {1981},
  author = {Huber, P. J.},
  address = {Hoboken, NJ},
  owner = {Songfan},
  timestamp = {2012.02.22}
}

@ARTICLE{Irani91,
  author = {Michal Irani and Shmuel Peleg},
  title = {Improving resolution by image registration },
  journal = {Graphical Models and Image Processing },
  year = {1991},
  volume = {53},
  pages = {231 - 239},
  number = {3}
}

@ARTICLE{Josephine2003,
  author = {Josephine L.C.M. Woltman Elpers, Michel Wedel, Rik G.M. Pieters},
  title = {Why Do Consumers Stop Viewing Television Commercials? Two Experiments
	on the Influence of Moment-to-Moment Entertainment and Information
	Value},
  journal = {Journal of Marketing Research},
  year = {2003},
  owner = {Songfan},
  timestamp = {2014.04.14}
}

@INPROCEEDINGS{close_loop_icra_05,
  author = {Kaess, M. and Dellaert, F.},
  title = {A Markov Chain Monte Carlo Approach to Closing the Loop in {SLAM}},
  booktitle = {IEEE International Conference on Robotics and Automation},
  year = {2005},
  pages = {643-648},
  month = {April}
}

@INPROCEEDINGS{Kanade_FG00,
  author = {Kanade, T. and Cohn, J.F. and YingLi Tian},
  title = {Comprehensive database for facial expression analysis},
  booktitle = {IEEE International Conference on Automatic Face and Gesture Recognition},
  year = {2000},
  pages = {46-53}
}

@INPROCEEDINGS{Keren_CVPR88,
  author = {Keren, D. and Peleg, S. and Brada, R.},
  title = {Image sequence enhancement using sub-pixel displacements},
  booktitle = {Computer Society Conference on Computer Vision and Pattern Recognition},
  year = {1988},
  pages = {742-746},
  month = {Jun}
}

@ARTICLE{Kotsia_IP07,
  author = {Kotsia, I. and Pitas, I.},
  title = {{F}acial {E}xpression {R}ecognition in {I}mage {S}equences {U}sing
	{G}eometric {D}eformation {F}eatures and {S}upport {V}ector {M}achines},
  journal = {IEEE Trans. IP},
  year = {2007},
  issn = {1057-7149},
  keywords = {Candide grid nodes;SVM;facial action units;facial expression;facial
	expression recognition;geometric deformation features;grid-tracking;image
	sequences;support vector machines;video frames;face recognition;image
	sequences;support vector machines;Algorithms;Artificial Intelligence;Face;Facial
	Expression;Humans;Image Enhancement;Image Interpretation, Computer-Assisted;Information
	Storage and Retrieval;Pattern Recognition, Automated;Subtraction
	Technique;Video Recording;}
}

@ARTICLE{LearnedMiller_PAMI06,
  author = {Learned-Miller, E.G.},
  title = {Data driven image models through continuous joint alignment},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {2006},
  volume = {28},
  pages = {236-250},
  number = {2},
  month = {Feb}
}

@ARTICLE{Li_PR15,
  author = {Yongqiang Li and S. Mohammad Mavadati and Mohammad H. Mahoor and
	Yongping Zhao and Qiang Ji},
  title = {Measuring the intensity of spontaneous facial action units with dynamic
	Bayesian network },
  journal = {Pattern Recognition },
  year = {2015},
  volume = {48},
  pages = {3417 - 3427},
  number = {11}
}

@INPROCEEDINGS{Littlewort_IVC06,
  author = {Littlewort, G. and Bartlett, M.S. and Fasel, I. and Susskind, J.
	and Movellan, J.},
  title = {{D}ynamics of {F}acial {E}xpression {E}xtracted {A}utomatically from
	{V}ideo},
  booktitle = {Image and Vision Computing},
  year = {2006}
}

@INPROCEEDINGS{Bartlett_FG11,
  author = {Littlewort, G. and Whitehill, J and Wu, T. and Fasel, I. and Frank,M.
	and Movellan, J. and Bartlett, M.},
  title = {{C}omputer {E}xpression {R}ecognition {T}oolbox},
  booktitle = {IEEE International Conference on Automatic Face and Gesture Recognition},
  year = {2011},
  month = {May},
  abstract = {{W}e present a live demo of the {C}omputer {E}xpression {R}ecognition
	{T}oolbox ({CERT}) developed at {U}niversity of {C}alifornia, {S}an
	{D}iego. {CERT} measures facial expressions in real-time, and codes
	them with respect to expressions of basic emotion, as well as over
	20 facial actions from the {F}acial {A}ction {C}oding {S}ystem ({E}kman
	amp; {F}riesen, 1978). {H}ead pose (yaw, pitch, and roll) is also
	detected using an algorithm presented at this conference ({W}hitehill
	amp; {M}ovellan, 2008). {A} sample output is shown in {F}igure 1.},
  doi = {10.1109/AFGR.2008.4813406},
  keywords = {computer expression recognition toolbox;emotion recognition;facial
	action coding system;facial expression;pose estimation;emotion recognition;face
	recognition;image coding;pose estimation;}
}

@INPROCEEDINGS{Littlewort_CERT_FG2011,
  author = {Littlewort, Gwen and Whitehill, J. and Tingfan Wu and Fasel, Ian
	and Frank, M. and Movellan, J. and Bartlett, M.},
  title = {The computer expression recognition toolbox ({CERT})},
  booktitle = {IEEE International Conference on Automatic Face Gesture Recognition
	and Workshops},
  year = {2011},
  pages = {298-305},
  month = {March},
  keywords = {emotion recognition;face recognition;image coding;software tools;3D
	orientation;CERT;FACS;automatic real-time facial expression recognition;computer
	expression recognition toolbox;dual core laptop;extended Cohn-Kanade;facial
	action unit coding system;facial expression dataset;software tool;two-alternative
	forced choice task;Accuracy;Detectors;Encoding;Face;Face recognition;Facial
	features;Gold}
}

@INPROCEEDINGS{Littlewort_FERA11,
  author = {Gwen Littlewort and Jacob Whitehill and Ting-Fan Wu and Nicholas
	Butko and Paul Ruvolo and Javier Movellan and Marian Bartlett},
  title = {{T}he {M}otion in {E}motion – {A} {CERT} {B}ased {A}pproach to the
	{FERA} {E}motion {C}hallenge},
  booktitle = {Proc. FG Workshop on FERA Challenge},
  year = {2011},
  owner = {Songfan},
  timestamp = {2011.04.29}
}

@ARTICLE{Liu_PAMI11,
  author = {Ce Liu and Yuen, J. and Torralba, A.},
  title = {{SIFT} {F}low: Dense Correspondence across Scenes and Its Applications},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {2011},
  volume = {33},
  pages = {978-994},
  number = {5}
}

@ARTICLE{SIFT,
  author = {Lowe, David G.},
  title = {Distinctive Image Features from Scale-Invariant Keypoints},
  journal = {International Journal of Computer Vision},
  year = {2004},
  volume = {60},
  pages = {91-110},
  number = {2}
}

@INPROCEEDINGS{Lucey_FG06,
  author = {Lucey, S. and Matthews, I. and Changbo Hu and Ambadar, Z. and de
	la Torre, F. and Cohn, J.},
  title = {\uppercase{AAM} {D}erived {F}ace {R}epresentations for {R}obust {F}acial
	{A}ction {R}ecognition},
  booktitle = {Proc. FG},
  year = {2006},
  doi = {10.1109/FGR.2006.17},
  keywords = {active appearance model;face representations;normalization methods;robust
	facial action recognition;face recognition;image representation;}
}

@ARTICLE{Martinez_PAMI13,
  author = {Martinez, B. and Valstar, M.F. and Binefa, X. and Pantic, M.},
  title = {Local Evidence Aggregation for Regression-Based Facial Point Detection},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {2013},
  volume = {35},
  pages = {1149-1163},
  number = {5},
  month = {May}
}

@ARTICLE{Matthews_IJCV03,
  author = {Iain Matthews and Simon Baker},
  title = {{A}ctive {A}ppearance {M}odels {R}evisited},
  journal = {IJCV},
  year = {2003}
}

@ARTICLE{McDuff_TAC14,
  author = {McDuff, D. and El Kaliouby, R. and Cohn, J.F. and Picard, R.W.},
  title = {Predicting Ad Liking and Purchase Intent: Large-Scale Analysis of
	Facial Responses to Ads},
  journal = {IEEE Transactions on Affective Computing},
  year = {2015},
  volume = {6},
  pages = {223-235},
  number = {3}
}

@INPROCEEDINGS{Meng_FERA11,
  author = {Hongying Meng and Bernardino Romera-Paredes and Nadia Bianchi-Berthouze},
  title = {{E}motion {R}ecognition by {T}wo {V}iew \uppercase{SVM} 2\uppercase{K}
	{C}lassifier on {D}ynamic {F}acial {E}xpression {F}eatures},
  booktitle = {Proc. FG Workshop on FERA Challenge},
  year = {2011},
  owner = {Songfan},
  timestamp = {2011.05.01}
}

@INCOLLECTION{Meng05,
  author = {Meng, Hongying and Shawe-Taylor, John and Szedmak, Sandor and Farquhar,
	Jason},
  title = {{S}upport {V}ector {M}achine to {S}ynthesise {K}ernels},
  booktitle = {Deterministic and Statistical Methods in Machine Learning},
  publisher = {Springer Berlin / Heidelberg},
  year = {2005},
  editor = {Winkler, Joab and Niranjan, Mahesan and Lawrence, Neil},
  affiliation = {School of Electronics and Computer Science, University of Southampton,
	Southampton, SO17 1BJ UK}
}

@INPROCEEDINGS{Negri06,
  author = {Pablo Negri and Xavier Clady and Maurice Milgram and Raphael Poulenard},
  title = {An Oriented-Contour Point Based Voting Algorithm for Vehicle Type
	Classification},
  booktitle = {Proc. International Conference on Pattern Recognition},
  year = {2006}
}

@ARTICLE{Ojala_PAMI02,
  author = {Ojala, T. and Pietik\"ainen, M. and Maenpaa, T.},
  title = {Multiresolution gray-scale and rotation invariant texture classification
	with local binary patterns},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {2002},
  volume = {24},
  pages = {971-987},
  number = {7},
  month = {Jul}
}

@INPROCEEDINGS{LPQ,
  author = {Ville Ojansivu and Janne Heikkil\"a},
  title = {Blur insensitive texture classification using local phase quantization},
  booktitle = {International Conference on Image and Signal Processing},
  year = {2008},
  pages = {236--243}
}

@INPROCEEDINGS{LPQ-TOP,
  author = {P\"aiv\"arinta, J. and Rahtu, E. and Heikkil\"a, J.},
  title = {Volume Local Phase Quantization for Blur-Insensitive Dynamic Texture
	Classification},
  booktitle = {Proc. Scandinavian Conference on Image Analysis (SCIA)},
  year = {2011},
  owner = {chidi},
  timestamp = {2015.05.13}
}

@ARTICLE{Pantic_SMCB06,
  author = {Pantic, M. and Patras, I.},
  title = {{D}ynamics of {F}acial {E}xpression: {R}ecognition of {F}acial {A}ctions
	and {T}heir {T}emporal {S}egments from {F}ace {P}rofile {I}mage {S}equences},
  journal = {IEEE Trans. SMC-B},
  year = {2006}
}

@ARTICLE{Pantic_SMCB04,
  author = {Pantic, M. and Rothkrantz, L.J.M.},
  title = {{F}acial {A}ction {R}ecognition for {F}acial {E}xpression {A}nalysis
	from {S}tatic {F}ace {I}mages},
  journal = {IEEE Trans. SMC-B},
  year = {2004}
}

@ARTICLE{Pantic_PAMI00,
  author = {Pantic, M. and Rothkrantz, L.J.M.},
  title = {{A}utomatic {A}nalysis of {F}acial {E}xpressions: {T}he {S}tate of
	the {A}rt},
  journal = {IEEE Trans. PAMI},
  year = {2000}
}

@INPROCEEDINGS{Pantic_ICME05,
  author = {Pantic, M. and Valstar, M. and Rademaker, R. and Maat, L.},
  title = {{W}eb-based {D}atabase for {F}acial {E}xpression {A}nalysis},
  booktitle = {Proc. IEEE Int’l Conf. Multimedia and Expo.},
  year = {2005},
  pages = {317-321},
  abstract = { {I}n the last decade, the research topic of automatic analysis of
	facial expressions has become a central topic in machine vision research.
	{N}onetheless, there is a glaring lack of a comprehensive, readily
	accessible reference set of face images that could be used as a basis
	for benchmarks for efforts in the field. {T}his lack of easily accessible,
	suitable, common testing resource forms the major impediment to comparing
	and extending the issues concerned with automatic facial expression
	analysis. {I}n this paper, we discuss a number of issues that make
	the problem of creating a benchmark facial expression database difficult.
	{W}e then present the {MMI} facial expression database, which includes
	more than 1500 samples of both static images and image sequences
	of faces in frontal and in profile view displaying various expressions
	of emotion, single and multiple facial muscle activation. {I}t has
	been built as a {W}eb-based direct-manipulation application, allowing
	easy access and easy search of the available images. {T}his database
	represents the most comprehensive reference set of images for studies
	on facial expression analysis to date.},
  doi = {10.1109/ICME.2005.1521424},
  keywords = { MMI facial expression database; Web-based direct-manipulation application;
	image representation; image sequence; man-machine interaction; multiple
	facial muscle activation; static image; Internet; distributed databases;
	emotion recognition; face recognition; image representation; image
	sequences; visual databases;}
}

@INPROCEEDINGS{Pearce11,
  author = {Pearce, G. and Pears, N.},
  title = {Automatic make and model recognition from frontal images of cars},
  booktitle = {IEEE International Conference on Advanced Video and Signal-Based
	Surveillance},
  year = {2011},
  pages = {373-378},
  month = {Aug}
}

@ARTICLE{sklearn,
  author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and
	Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and
	Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau,
	D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  title = {Scikit-learn: Machine Learning in {P}ython},
  journal = {Journal of Machine Learning Research},
  year = {2011}
}

@ARTICLE{Peng_PAMI12,
  author = { Yigang Peng and A. Ganesh and J. Wright and Wenli Xu and Yi Ma},
  title = {{RASL}: Robust Alignment by Sparse and Low-Rank Decomposition for
	Linearly Correlated Images},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {2012},
  volume = {34},
  pages = {2233-2246},
  number = {11}
}

@INPROCEEDINGS{Petrovic04,
  author = {V. S. Petrovi\'c and T. F. Cootes},
  title = {Analysis of Features for Rigid Structure Vehicle Type Recognition},
  booktitle = {British Machine Vision Conference},
  year = {2004},
  pages = {587--596}
}

@ARTICLE{Pham_06,
  author = {Pham, Tuan Q. and van Vliet, Lucas J. and Schutte, Klamer},
  title = {{R}obust {F}usion of {I}rregularly {S}ampled {D}ata {U}sing {A}daptive
	{N}ormalized {C}onvolution},
  journal = {EURASIP Journal on Applied Signal Processing},
  year = {2006}
}

@BOOK{Russell97,
  title = {{T}he {P}sychology of {F}acial {E}xpression},
  publisher = {Cambridge University Press},
  year = {1997},
  author = {James A. Russell and Jos\'e Miguel Fern\'andez-Dols},
  owner = {songfan},
  timestamp = {2011.04.28}
}

@INPROCEEDINGS{Saragih_ICCV09,
  author = {Saragih, J.M. and Lucey, S. and Cohn, J.F.},
  title = {{F}ace {A}lignment through {S}ubspace {C}onstrained {M}ean-{S}hifts},
  booktitle = {Proc. ICCV},
  year = {2009},
  doi = {10.1109/ICCV.2009.5459377},
  issn = {1550-5499},
  keywords = {computer vision community;deformable model fitting;distribution replacement;face
	alignment;local detector;model landmark distribution;principled optimization
	strategy;smoothed estimate hierarchy;subspace constrained mean shifts;face
	recognition;optimisation;}
}

@ARTICLE{Scharstein_IJCV02,
  author = {Daniel Scharstein and Richard Szeliski},
  title = {{A} {T}axonomy and {E}valuation of {D}ense {T}wo-frame {S}tereo {C}orrespondence
	{A}lgorithms},
  journal = {IJCV},
  year = {2002}
}

@INPROCEEDINGS{Senechal_FERA11,
  author = {Ruchir Srivastava and Sujoy Roy and Shuicheng Yan and Terence Sim},
  title = {{A}ccumulated {M}otion {I}mages for {F}acial {E}xpression {R}ecognition
	in {V}ideos},
  booktitle = {Proc. FG Workshop on FERA Challenge},
  year = {2011},
  owner = {Songfan},
  timestamp = {2011.05.01}
}

@INPROCEEDINGS{Sun_CVPR08,
  author = {Jian Sun and Zongben Xu and Heung-Yeung Shum},
  title = {Image super-resolution using gradient profile prior},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
  year = {2008},
  pages = {1-8},
  month = {June}
}

@ARTICLE{Szeliski06,
  author = {Richard Szeliski},
  title = {Image Alignment and Stitching: A Tutorial},
  journal = {Foundations and Trends in Computer Graphics and Vision},
  year = {2006},
  volume = {2},
  pages = {1-104},
  number = {1}
}

@INPROCEEDINGS{Tariq_FERA11,
  author = {Usman Tariq and Kai-Hsiang Lin and Zhen Li and Xi Zhou and Zhaowen
	Wang and Vuong Le and Thomas S. Huang and Xutao Lv and Tony X. Han},
  title = {{E}motion {R}ecognition from an {E}nsemble of {F}eatures},
  booktitle = {Proc. FG Workshop on FERA Challenge},
  year = {2011},
  owner = {Songfan},
  timestamp = {2011.04.29}
}

@ARTICLE{Thakoor13,
  author = {Thakoor, N.S. and Bhanu, B.},
  title = {Structural Signatures for Passenger Vehicle Classification in Video},
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  year = {2013},
  volume = {14},
  pages = {1796-1805},
  number = {4},
  month = {Dec},
  doi = {10.1109/TITS.2013.2269137},
  issn = {1524-9050},
  keywords = {image motion analysis;object recognition;vehicles;passenger vehicle
	classification;pattern recognition;road surface;structural signatures;vehicle
	side profile information;vehicle surfaces;video data set;Feature
	extraction;Image motion analysis;Object recognition;Pattern recognition;Vehicles;Image
	motion analysis;object recognition;vehicles}
}

@INPROCEEDINGS{Thakoor05,
  author = {Thakoor, N. and Jean Gao},
  title = {Automatic video object shape extraction and its classification with
	camera in motion},
  booktitle = {IEEE International Conference on Image Processing},
  year = {2005},
  volume = {3},
  pages = {III-437-40},
  month = {Sept},
  doi = {10.1109/ICIP.2005.1530422},
  keywords = {cameras;feature extraction;hidden Markov models;image classification;object
	recognition;probability;HMM;ML description;automatic video object
	shape classification;automatic video object shape extraction;camera
	motion estimation;change detection approach;curvature features extraction;discrimination
	phase;forward region boundary;frame difference;frame difference information;generalized
	probabilistic descent method;hidden Markov model;moving camera;optical
	flow;shape classifier;uniform intensity regions;weighted likelihood
	discriminant;Cameras;Data mining;Feature extraction;Hidden Markov
	models;Image motion analysis;Maximum likelihood detection;Maximum
	likelihood estimation;Motion estimation;Robustness;Shape}
}

@ARTICLE{Tian_PAMI01,
  author = {Tian, Y.-I. and Kanade, T. and Cohn, J.F.},
  title = {{R}ecognizing {A}ction {U}nits for {F}acial {E}xpression {A}nalysis},
  journal = {IEEE Trans. PAMI},
  year = {2001},
  abstract = {{M}ost automatic expression analysis systems attempt to recognize
	a small set of prototypic expressions, such as happiness, anger,
	surprise, and fear. {S}uch prototypic expressions, however, occur
	rather infrequently. {H}uman emotions and intentions are more often
	communicated by changes in one or a few discrete facial features.
	{I}n this paper, we develop an automatic face analysis ({AFA}) system
	to analyze facial expressions based on both permanent facial features
	(brows, eyes, mouth) and transient facial features (deepening of
	facial furrows) in a nearly frontal-view face image sequence. {T}he
	{AFA} system recognizes fine-grained changes in facial expression
	into action units ({AU}) of the {F}acial {A}ction {C}oding {S}ystem
	({FACS}), instead of a few prototypic expressions. {M}ultistate face
	and facial component models are proposed for tracking and modeling
	the various facial features, including lips, eyes, brows, cheeks,
	and furrows. {D}uring tracking, detailed parametric descriptions
	of the facial features are extracted. {W}ith these parameters as
	the inputs, a group of action units (neutral expression, six upper
	face {AU} and 10 lower face {AU}) are recognized whether they occur
	alone or in combinations. {T}he system has achieved average recognition
	rates of 96.4 percent (95.4 percent if neutral expressions are excluded)
	for upper face {AU} and 96.7 percent (95.6 percent with neutral expressions
	excluded) for lower face {AU}. {T}he generalizability of the system
	has been tested by using independent image databases collected and
	{FACS}-coded for ground-truth by different research teams},
  issn = {0162-8828},
  keywords = {AU;FACS;Facial Action Coding System;action unit recognition;anger;automatic
	facial expression analysis;cheeks;discrete facial features;eyebrows;eyes;facial
	furrows;fear;frontal-view face image sequence;happiness;image databases;lips;mouth;multistate
	face models;neutral expression;parametric descriptions;surprise;tracking;face
	recognition;feature extraction;image sequences;tracking;}
}

@ARTICLE{Tong_PAMI10,
  author = {Yan Tong and Jixu Chen and Qiang Ji},
  title = {{A} {U}nified {P}robabilistic {F}ramework for {S}pontaneous {F}acial
	{A}ction {M}odeling and {U}nderstanding},
  journal = {IEEE Trans. PAMI},
  year = {2010},
  abstract = {{F}acial expression is a natural and powerful means of human communication.
	{R}ecognizing spontaneous facial actions, however, is very challenging
	due to subtle facial deformation, frequent head movements, and ambiguous
	and uncertain facial motion measurements. {B}ecause of these challenges,
	current research in facial expression recognition is limited to posed
	expressions and often in frontal view. {A} spontaneous facial expression
	is characterized by rigid head movements and nonrigid facial muscular
	movements. {M}ore importantly, it is the coherent and consistent
	spatiotemporal interactions among rigid and nonrigid facial motions
	that produce a meaningful facial expression. {R}ecognizing this fact,
	we introduce a unified probabilistic facial action model based on
	the dynamic {B}ayesian network ({DBN}) to simultaneously and coherently
	represent rigid and nonrigid facial motions, their spatiotemporal
	dependencies, and their image measurements. {A}dvanced machine learning
	methods are introduced to learn the model based on both training
	data and subjective prior knowledge. {G}iven the model and the measurements
	of facial motions, facial action recognition is accomplished through
	probabilistic inference by systematically integrating visual measurements
	with the facial action model. {E}xperiments show that compared to
	the state-of-the-art techniques, the proposed system yields significant
	improvements in recognizing both rigid and nonrigid facial motions,
	especially for spontaneous facial expressions.},
  issn = {0162-8828},
  keywords = {dynamic Bayesian network;facial action recognition;facial deformation;facial
	expression;facial motion measurement;facial muscular movement;head
	movement;human communication;image measurement;machine learning;probabilistic
	facial action model;probabilistic inference;rigid facial motion;spatiotemporal
	interaction;spontaneous facial action modeling;spontaneous facial
	action understanding;visual measurement;belief networks;face recognition;image
	motion analysis;inference mechanisms;learning (artificial intelligence);Algorithms;Artificial
	Intelligence;Bayes Theorem;Biometric Identification;Databases, Factual;Face;Humans;Models,
	Statistical;}
}

@INPROCEEDINGS{Uenohara95,
  author = {Michihiro Uenohara and Takeo Kanade},
  title = {Real-Time Vision Based Object Registration for Image Overlay},
  booktitle = {1995 Conference on Computer Vision,Virtual Reality and Robotics in
	Medicine},
  year = {1995},
  month = {April}
}

@INPROCEEDINGS{FERA15,
  author = {Valstar, M.F. and Almaev, T. and Girard, J.M. and McKeown, G. and
	Mehu, M. and Lijun Yin and Pantic, M. and Cohn, J.F.},
  title = {FERA 2015 - second Facial Expression Recognition and Analysis challenge},
  booktitle = {IEEE International Conference and Workshops on Automatic Face and
	Gesture Recognition},
  year = {2015}
}

@INPROCEEDINGS{Valstar_FERA11,
  author = {Valstar, M.F. and Jiang, B. and M\'{e}hu, M. and Pantic, M. and Scherer,
	K},
  title = {{T}he {F}irst {F}acial {E}xpression {R}ecognition and {A}nalysis
	{C}hallenge},
  booktitle = {IEEE International Conference on Automatic Face Gesture Recognition
	and Workshops},
  year = {2011},
  pages = {921-926},
  owner = {songfan},
  timestamp = {2011.03.01}
}

@ARTICLE{Valstar12,
  author = {Valstar, M.F. and Mehu, M. and Bihan Jiang and Pantic, M. and Scherer,
	K.},
  title = {Meta-Analysis of the First Facial Expression Recognition Challenge},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics},
  year = {2012},
  volume = {42},
  pages = {966-979},
  number = {4},
  month = {Aug}
}

@ARTICLE{Valstar_SMCB12,
  author = {Valstar, M.F. and Pantic, M.},
  title = {Fully Automatic Recognition of the Temporal Phases of Facial Actions},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics},
  year = {2012},
  volume = {42},
  pages = {28-43},
  number = {1},
  month = {Feb},
  doi = {10.1109/TSMCB.2011.2163710},
  issn = {1083-4419},
  keywords = {face recognition;hidden Markov models;image classification;image sequences;particle
	filtering (numerical methods);support vector machines;Gabor-feature-based
	boosted classifiers;GentleBoost;action units;decision making;facial
	behavior;facial fiducial point localization;facial muscle action
	temporal phase automatic recognition;facial point detector;factorized
	likelihoods;hidden Markov models;image sequence;particle filtering;support
	vector machines;temporal activation models;Detectors;Emotion recognition;Face;Face
	recognition;Gold;Hidden Markov models;Image sequences;Facial expression
	analysis;GentleBoost;particle filtering;spatiotemporal facial behavior
	analysis;support vector machine (SVM);Artificial Intelligence;Facial
	Expression;Humans;Image Interpretation, Computer-Assisted;Pattern
	Recognition, Automated;Photography;Subtraction Technique;Video Recording}
}

@INPROCEEDINGS{Valstar_HCI07,
  author = {M. F. Valstar and M. Pantic},
  title = {{C}ombined {S}upport {V}ector {M}achines and {H}idden {M}arkov {M}odels
	for {M}odeling {F}acial {A}ction {T}emporal {D}ynamics},
  booktitle = {Proc. CVPR workshop on Human Computer Interaction},
  year = {2007}
}

@ARTICLE{Vandewalle06,
  author = {Patrick Vandewalle and Sabine S\"usstrunk and Martin Vetterli},
  title = {{A} {F}requency {D}omain {A}pproach to {R}egistration of {A}liased
	{I}mages with {A}pplication to {S}uper-{R}esolution},
  journal = {EURASIP Journal on Applied Signal Processing},
  year = {2006}
}

@MISC{vlfeat,
  author = {A. Vedaldi and B. Fulkerson},
  title = {{VLFeat}: An Open and Portable Library of Computer Vision Algorithms},
  howpublished = {\url{http://www.vlfeat.org/}},
  year = {2008},
  owner = {Songfan},
  timestamp = {2014.05.17}
}

@ARTICLE{Viola_IJCV04,
  author = {Paul Viola and Michael Jones},
  title = {Robust Real-time Face Detection},
  journal = {International Journal of Computer Vision},
  year = {2004},
  volume = {57},
  pages = {137-154},
  number = {2}
}

@INPROCEEDINGS{Wagner2009,
  author = {Wagner, A. and Wright, J. and Ganesh, A. and Zihan Zhou and Yi Ma},
  title = {Towards a practical face recognition system: Robust registration
	and illumination by sparse representation},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
  year = {2009},
  pages = {597-604},
  month = {June},
  issn = {1063-6919}
}

@ARTICLE{Wang_TIP94,
  author = {Wang, J.Y.A. and Adelson, E.H.},
  title = {{R}epresenting {M}oving {I}mages with {L}ayers},
  journal = {IEEE Trans. IP},
  year = {1994},
  doi = {10.1109/83.334981},
  issn = {1057-7149},
  keywords = {alpha map;image coding;image representation;image sequences decomposition;intensity
	map;layered representation;motion analysis;motion estimation;motion
	segmentation;moving images;overlapping layers;transparency;velocity
	maps;video sequences;image coding;image segmentation;image sequences;motion
	estimation;video signals;}
}

@INPROCEEDINGS{Wang_FG98,
  author = {Mei Wang and Iwai, Y. and Yachida, M.},
  title = {{E}xpression {R}ecognition from {T}ime-sequential {F}acial {I}mages
	by {U}se of {E}xpression {C}hange {M}odel},
  booktitle = {Proc. FG},
  year = {1998},
  abstract = {{I}n order to construct a better human interface, recognition of facial
	expressions by means of computer is an important technology. {A}n
	approach is proposed to recognize the degree of facial expression
	change from time-sequential images. {T}he facial features in an input
	image sequence are tracked by using labeled graph matching with weighted
	links. {T}o represent the relationship between the motion of features
	and change of expression, we construct expression change models by
	using {B}-spline curves. {B}y making a comparison between the trajectory
	of features and the expression change models, the facial expression
	in the input image sequence can be recognized. {N}ot only the category,
	but also the degree of facial expression change can be determined.
	{F}urthermore, the obtained expressional information is then fed
	back to guide the tracking in the next frame},
  keywords = {B-spline curves;expression change model;expression recognition;expressional
	information;facial expression change;facial features;feature trajectory;human
	interface;input image sequence;labeled graph matching;time-sequential
	facial images;tracking;weighted links;face recognition;image sequences;splines
	(mathematics);user interfaces;}
}

@INPROCEEDINGS{Wu_CVPRW10,
  author = {Tingfan Wu and Bartlett, M.S. and Movellan, J.R.},
  title = {{F}acial {E}xpression {R}ecognition {U}sing {G}abor {M}otion {E}nergy
	{F}ilters},
  booktitle = {Proc. CVPRW},
  year = {2010},
  doi = {10.1109/CVPRW.2010.5543267},
  keywords = {Cohn-Kanade expression dataset;GME;Gabor motion energy filters;biologically
	inspired representation;computer vision;dynamic facial expressions;face
	expression analysis;face recognition;facial expression recognition;primary
	visual cortex;spatial Gabor energy filters;spatio-temporal Gabor
	filters;video sequences;visual signal;Gabor filters;computer vision;face
	recognition;image motion analysis;image representation;image sequences;spatial
	filters;video signal processing;}
}

@INPROCEEDINGS{Wu_FERA11,
  author = {Tingfan Wu and Butko, N.J. and Ruvolo, P. and Whitehill, J. and Bartlett,
	M.S. and Movellan, J.R.},
  title = {{A}ction {U}nit {R}ecognition {T}ransfer across {D}atasets},
  booktitle = {Proc. FG Workshop on FERA Challenge},
  year = {2011},
  doi = {10.1109/FG.2011.5771369},
  keywords = {CERT;FERA dataset;action unit recognition;baseline method;computer
	expression recognition toolbox;idiosyncratic;spontaneous facial expression;transfer
	across dataset;emotion recognition;face recognition;}
}

@ARTICLE{Wu_TIP06,
  author = {Xiaolin Wu and Zhang, D.},
  title = {Improvement of Color Video Demosaicking in Temporal Domain},
  journal = {IEEE Trans. Image Process.},
  year = {2006},
  volume = {15},
  pages = {3138-3151},
  number = {10},
  month = {Oct},
  issn = {1057-7149}
}

@INPROCEEDINGS{Xiong13,
  author = {Xuehan Xiong and De la Torre, F.},
  title = {Supervised Descent Method and Its Applications to Face Alignment},
  booktitle = {Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference
	on},
  year = {2013},
  pages = {532-539},
  month = {June}
}

@INPROCEEDINGS{Xue_CVPR13,
  author = {Wufeng Xue and Lei Zhang and Xuanqin Mou},
  title = {Learning without Human Scores for Blind Image Quality Assessment},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
  year = {2013},
  pages = {995-1002},
  month = {June}
}

@ARTICLE{Yacoob_PAMI96,
  author = {Yacoob, Y. and Davis, L.S.},
  title = {{R}ecognizing {H}uman {F}acial {E}xpressions from {L}ong {I}mage
	{S}equences {U}sing {O}ptical {F}low},
  journal = {IEEE Trans. PAMI},
  year = {1996},
  doi = {10.1109/34.506414},
  issn = {0162-8828},
  keywords = {facial dynamics;human facial expression recognition;image sequences;motion
	analysis;optical flow;symbolic representation;face recognition;image
	recognition;image representation;image sequences;motion estimation;}
}

@INPROCEEDINGS{Yang_CVPR07,
  author = {Peng Yang and Qingshan Liu and Metaxas, D.N.},
  title = {{B}oosting {C}oded {D}ynamic {F}eatures for {F}acial {A}ction {U}nits
	and {F}acial {E}xpression {R}ecognition},
  booktitle = {Proc. CVPR},
  year = {2007},
  abstract = {{I}t is well known that how to extract dynamical features is a key
	issue for video based face analysis. {I}n this paper, we present
	a novel approach of facial action units ({AU}) and expression recognition
	based on coded dynamical features. {I}n order to capture the dynamical
	characteristics of facial events, we design the dynamical haar-like
	features to represent the temporal variations of facial events. {I}nspired
	by the binary pattern coding, we further encode the dynamic haar-like
	features into binary pattern features, which are useful to construct
	weak classifiers for boosting learning. {F}inally the {A}daboost
	is performed to learn a set of discriminating coded dynamic features
	for facial active units and expression recognition. {E}xperiments
	on the {CMU} expression database and our own facial {AU} database
	show its encouraging performance.},
  doi = {10.1109/CVPR.2007.383059},
  keywords = {binary pattern coding;coded dynamic features;facial action units;facial
	expression recognition;video based face analysis;binary codes;emotion
	recognition;face recognition;video signal processing;}
}

@INPROCEEDINGS{Yang_FG13,
  author = {Songfan Yang and Le An and Bhanu, B. and Thakoor, N.},
  title = {Improving action units recognition using dense flow-based face registration
	in video},
  booktitle = {IEEE International Conference on Automatic Face and Gesture Recognition},
  year = {2013},
  pages = {1-8},
  month = {April},
  doi = {10.1109/FG.2013.6553790},
  keywords = {affine transforms;face recognition;image motion analysis;image registration;image
	sequences;muscle;pose estimation;video streaming;SIFT method;SOFAIT;action
	unit recognition;affine transformation;appearance change;arbitrary
	face transformation;automatic video-based face registration architecture;canonical
	pose;dense SIFT-flow-based affine warping problem;dense flow-based
	face registration;dense optical flow;face alignment;facial expression
	recognition;facial muscle dynamics;generic reference face;head pose;nonrigid
	facial appearance;nonrigid muscle motion;optical flow affine image
	transform;optical-flow-based affine warping problem;person independent
	face model;real-time algorithm;real-world streaming video;realistic
	streaming data;spontaneous expression;temporal smoothness;Communities;Estimation;Optical
	imaging;Optical propagation;Visualization}
}

@ARTICLE{Yang_SMCB12,
  author = {Songfan Yang and Bhanu, B.},
  title = {Understanding Discrete Facial Expressions in Video Using an Emotion
	Avatar Image},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics},
  year = {2012},
  volume = {42},
  pages = {980-992},
  number = {4},
  month = {Aug}
}

@INPROCEEDINGS{Yang_FERA11,
  author = {Songfan Yang and Bir Bhanu},
  title = {{F}acial {E}xpression {R}ecognition {U}sing {E}motion {A}vatar {I}mage},
  booktitle = {Proc. FG Workshop on FERA Challenge},
  year = {2011},
  owner = {songfan},
  timestamp = {2011.05.23}
}

@ARTICLE{Yu_PRL06,
  author = {Jiangang Yu and Bir Bhanu},
  title = {{E}volutionary {F}eature {S}ynthesis for {F}acial {E}xpression {R}ecognition},
  journal = {Pattern Recognition Letters},
  year = {2006},
  keywords = {genetic algorithms, genetic programming, Feature learning, Gabor filters}
}

@INPROCEEDINGS{Zafar09,
  author = {Iffat Zafar and Eran A. Edirisinghe and B. Serpil Acar},
  title = {Localised contourlet features in vehicle make and model recognition},
  booktitle = {Image Processing: Machine Vision Applications II, Proc. of SPIE-IS\&T
	Electronic Imaging},
  year = {2009}
}

@INPROCEEDINGS{LGBP,
  author = {Zhang, Wenchao and Shan, Shiguang and Gao, Wen and Chen, Xilin and
	Zhang, Hongming},
  title = {Local Gabor Binary Pattern Histogram Sequence (LGBPHS): A Novel Non-Statistical
	Model for Face Representation and Recognition},
  booktitle = {IEEE International Conference on Computer Vision},
  year = {2005}
}

@ARTICLE{BP4D,
  author = {Xing Zhang and Lijun Yin and Jeffrey F. Cohn and Shaun Canavan and
	Michael Reale and Andy Horowitz and Peng Liu and Jeffrey M. Girard},
  title = {BP4D-Spontaneous: a high-resolution spontaneous 3D dynamic facial
	expression database },
  journal = {Image and Vision Computing },
  year = {2014},
  volume = {32},
  pages = {692 - 706},
  number = {10},
  issn = {0262-8856}
}

@INPROCEEDINGS{Zhang_ACCV10,
  author = {Z. Zhang and X. Liang and A. Ganesh and Y. Ma},
  title = {{TILT}: {T}ransform {I}nvariant {L}ow-rank {T}extures},
  booktitle = {Proc. ACCV},
  year = {2010}
}

@ARTICLE{Zhao_PAMI07,
  author = {Guoying Zhao and Pietik\"ainen, M.},
  title = {Dynamic Texture Recognition Using Local Binary Patterns with an Application
	to Facial Expressions},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {2007},
  volume = {29},
  pages = {915-928},
  number = {6},
  month = {June}
}

@INPROCEEDINGS{Zhou_CVPR10,
  author = {Feng Zhou and De la Torre, F. and Cohn, J.F.},
  title = {{U}nsupervised {D}iscovery of {F}acial {E}vents},
  booktitle = {Proc. CVPR},
  year = {2010},
  abstract = {{A}utomatic facial image analysis has been a long standing research
	problem in computer vision. {A} key component in facial image analysis,
	largely conditioning the success of subsequent algorithms (e.g. facial
	expression recognition), is to define a vocabulary of possible dynamic
	facial events. {T}o date, that vocabulary has come from the anatomically-based
	{F}acial {A}ction {C}oding {S}ystem ({FACS}) or more subjective approaches
	(i.e. emotion-specified expressions). {T}he aim of this paper is
	to discover facial events directly from video of naturally occurring
	facial behavior, without recourse to {FACS} or other labeling schemes.
	{T}o discover facial events, we propose a temporal clustering algorithm,
	{A}ligned {C}luster {A}nalysis ({ACA}), and a multi-subject correspondence
	algorithm for matching expressions. {W}e use a variety of video sources:
	posed facial behavior ({C}ohn-{K}anade database), unscripted facial
	behavior ({RU}-{FACS} database) and some video in infants. {A}ccuracy
	of (unsupervised) {ACA} approached that of a supervised version,
	achieved moderate intersystem agreement with {FACS}, and proved informative
	as a visualization/summarization tool.},
  doi = {10.1109/CVPR.2010.5539966},
  issn = {1063-6919},
  keywords = {aligned cluster analysis;automatic facial image analysis;computer
	vision;facial action coding system;facial events;facial expression
	recognition;labeling schemes;multi-subject correspondence algorithm;unsupervised
	discovery;computer vision;emotion recognition;face recognition;image
	matching;video coding;}
}

@INPROCEEDINGS{Zhu_ICCV09,
  author = {Jianke Zhu and Luc Van Gool and Hoi, S.C.H.},
  title = {{U}nsupervised {F}ace {A}lignment by {R}obust {N}onrigid {M}apping},
  booktitle = {Proc. ICCV},
  year = {2009},
  issn = {1550-5499},
  keywords = {Lucas-Kanade image registration;affine transformations;robust nonrigid
	mapping;robust optimization scheme;unsupervised facial image alignment;face
	recognition;feature extraction;image registration;transforms;}
}

@ARTICLE{Zhu_PAMI09,
  author = {Zhu, J. and Lyu, M.R. and Huang, T.S.},
  title = {{A} {F}ast 2{D} {S}hape {R}ecovery {A}pproach by {F}using {F}eatures
	and {A}ppearance},
  journal = {IEEE Trans. PAMI},
  year = {2009},
  issn = {0162-8828},
  keywords = {2D shape recovery;Lucas-Kanade algorithm;appearance fusion;computer
	vision;feature fusion;feature-based nonrigid surface detection;finite
	Newton optimization scheme;least squares problem;unconstrained quadratic
	optimization problem;computer graphics;image fusion;least squares
	approximations;object detection;quadratic programming;Algorithms;Artificial
	Intelligence;Image Enhancement;Image Interpretation, Computer-Assisted;Information
	Storage and Retrieval;Models, Biological;Pattern Recognition, Automated;Reproducibility
	of Results;Sensitivity and Specificity;Subtraction Technique;}
}

@INPROCEEDINGS{Zhu_CVPR12,
  author = {Xiangxin Zhu and Ramanan, D.},
  title = {Face detection, pose estimation, and landmark localization in the
	wild},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
  year = {2012},
  pages = {2879-2886},
  month = {June},
  doi = {10.1109/CVPR.2012.6248014},
  issn = {1063-6919},
  keywords = {computer vision;face recognition;object detection;pose estimation;trees
	(mathematics);Google Picasa;computer vision;face detection;face.com;global
	elastic deformation;in-the-wild annotated dataset;landmark estimation;landmark
	localization;pose estimation;topological changes;tree-structured
	models;Computational modeling;Detectors;Estimation;Face;Face detection;Google;Shape}
}

@ARTICLE{Zitova_IVC03,
  author = {Barbara Zitov\'a and Jan Flusser},
  title = {{I}mage {R}egistration {M}ethods: {A} {S}urvey},
  journal = {Image and Vision Computing},
  year = {2003}
}

@INPROCEEDINGS{Zomet_CVPR01,
  author = {Zomet, A. and Rav-Acha, A. and Peleg, S.},
  title = {{R}obust {S}uper-resolution},
  booktitle = {Proc. CVPR},
  year = {2001}
}

@MANUAL{FERA11,
  title = {{FERA}2011: {F}acial {E}xpression {R}ecognition and {A}nalysis {C}hallenge},
  note = {http://sspnet.eu/fera2011/},
  owner = {songfan},
  timestamp = {2011.02.28}
}

